metadata:
  version: 1.0.0
  timestamp: 2025-07-13 18:49:02.224944
  description: YAML snapshot of /Users/richardhightower/src/art_hug_08
  generator: yaml-project
  generator_version: 0.1.0
  author:
  tags:
  custom:
config:
  supported_extensions:
    .py: python
    .sh: bash
    .java: java
    .js: javascript
    .jsx: javascript
    .ts: typescript
    .tsx: typescript
    .html: html
    .css: css
    .md: markdown
    .yml: yaml
    .yaml: yaml
    .json: json
    .txt: text
    .go: go
    .rs: rust
    .rb: ruby
    .php: php
    .c: c
    .cpp: cpp
    .h: c
    .hpp: cpp
    .cs: csharp
    .toml: toml
    .xml: xml
    .sql: sql
    .kt: kotlin
    .swift: swift
    .dart: dart
    .r: r
    .scala: scala
    .pl: perl
    .lua: lua
    .ini: ini
    .cfg: ini
    .properties: properties
    .ipynb: ipynb
  forbidden_dirs:
    - __pycache__
    - node_modules
    - dist
    - cdk.out
    - env
    - venv
    - .venv
    - .idea
    - build
    - .git
    - .svn
    - .hg
    - .DS_Store
    - .vs
    - .vscode
    - target
    - bin
    - obj
    - out
    - Debug
    - Release
    - tmp
    - .tox
    - .pytest_cache
    - __MACOSX
    - .mypy_cache
    - tests
  include_pattern:
  exclude_pattern:
  outfile: /Users/richardhightower/src/art_hug_08/project.yaml
  log_level: INFO
  max_file_size: 204800
  config_dir: .yamlproject
  chunking_enabled: false
  chunk_size: 1048576
  temp_dir:
  backup_enabled: false
  backup_dir:
  metadata_fields:
    - extension
    - size_bytes
    - language
  yaml_format:
    indent: 2
    width: 120
tests:
content:
  files:
    pyproject.toml:
      content: |
        [tool.poetry]
        name = "huggingface-workflows"
        version = "0.1.0"
        description = "Custom Pipelines and Data Workflows - From User to Architect"
        authors = ["Your Name <you@example.com>"]
        readme = "README.md"
        packages = [{include = "src"}]

        [tool.poetry.dependencies]
        python = "^3.12"
        transformers = "^4.40.0"
        torch = "^2.2.0"
        torchvision = "^0.17.0"
        torchaudio = "^2.2.0"
        datasets = "^2.14.0"
        diffusers = "^0.31.0"
        accelerate = "^0.25.0"
        bitsandbytes = "^0.41.0"
        peft = "^0.8.0"
        pillow = "^10.2.0"
        soundfile = "^0.12.1"
        matplotlib = "^3.8.0"
        seaborn = "^0.13.0"
        # sentencepiece = "^0.1.99"  # Optional - install separately if needed
        protobuf = "^4.25.0"  # Add protobuf for tokenizers
        python-dotenv = "^1.0.0"
        pandas = "^2.1.0"
        numpy = "^1.26.0"
        requests = "^2.31.0"
        tqdm = "^4.66.0"
        onnx = "^1.15.0"
        optimum = "^1.16.0"
        onnxruntime = "^1.16.0"
        psutil = "^5.9.0"
        gputil = "^1.4.0"

        [tool.poetry.group.dev.dependencies]
        pytest = "^8.0.0"
        black = "^24.0.0"
        ruff = "^0.6.0"
        jupyter = "^1.0.0"
        ipykernel = "^6.29.0"
        ipywidgets = "^8.1.0"
        memory-profiler = "^0.61.0"

        [build-system]
        requires = ["poetry-core"]
        build-backend = "poetry.core.masonry.api"

        [tool.black]
        line-length = 88
        target-version = ['py312']

        [tool.ruff]
        line-length = 88
        target-version = "py312"

        [tool.ruff.lint]
        select = ["E", "F", "I", "N", "UP", "B", "C4", "SIM"]
      metadata:
        extension: .toml
        size_bytes: 1327
        language: toml
    README.md:
      content: |
        # Customizing Pipelines and Data Workflows: Advanced Models and Efficient Processing

        This project contains working examples for Chapter 8 of the Hugging Face Transformers book, demonstrating how to transform from pipeline user to workflow architect.

        ## Overview

        Learn how to implement and understand:

        - Pipeline anatomy and customization
        - Component swapping and composition
        - Efficient data handling with ðŸ¤— Datasets
        - Streaming for massive datasets
        - Model optimization (quantization, batching, edge deployment)
        - Synthetic data generation with LLMs and diffusion models
        - Production-ready workflows with cost optimization

        ## Prerequisites

        - Python 3.12 (managed via pyenv)
        - Poetry for dependency management
        - Go Task for build automation
        - macOS (Apple Silicon), Linux, or Windows
        - CUDA GPU (optional for NVIDIA users)
        - MPS support for Apple Silicon
        - (Optional) Hugging Face account for accessing gated models

        ## Installation

        1. Clone the repository:
        ```bash
        git clone git@github.com:RichardHightower/art_hug_08.git
        cd art_hug_08
        ```

        2. Run the setup task (this handles Python environment and dependencies):
        ```bash
        task setup
        ```

        That's it! The setup task will:
        - Install Python 3.12.9 if needed
        - Set up Poetry environment
        - Install all dependencies (including workarounds for macOS)
        - Configure the environment

        3. (Optional) Copy and configure environment variables:
        ```bash
        cp .env.example .env
        # Edit .env with your configuration (API keys, etc.)
        ```

        ## Quick Start

        After setup, test the environment:
        ```bash
        poetry run python test_environment.py
        ```

        Run the main demonstration:
        ```bash
        task run
        ```

        Or use Poetry to run specific modules:
        ```bash
        # Custom pipeline examples
        poetry run python src/custom_pipelines.py

        # Efficient data handling demonstrations
        poetry run python src/efficient_data_handling.py

        # Optimization benchmarks
        poetry run python src/optimization_demo.py

        # Production workflow example
        poetry run python src/production_workflows.py
        ```

        ### Tutorial Notebook

        For an interactive learning experience with all Chapter 8 examples:
        ```bash
        poetry run jupyter notebook notebooks/tutorial.ipynb
        ```

        ## Project Structure

        ```
        art_hug_08/
        â”œâ”€â”€ src/
        â”‚   â”œâ”€â”€ custom_pipelines.py      # Pipeline customization examples
        â”‚   â”œâ”€â”€ efficient_data_handling.py # Datasets library demonstrations
        â”‚   â”œâ”€â”€ optimization_demo.py     # Model optimization techniques
        â”‚   â”œâ”€â”€ synthetic_data.py        # Data generation with LLMs
        â”‚   â”œâ”€â”€ production_workflows.py  # End-to-end retail example
        â”‚   â”œâ”€â”€ edge_deployment.py       # ONNX export and edge deployment
        â”‚   â”œâ”€â”€ peft_lora.py            # PEFT/LoRA fine-tuning examples
        â”‚   â”œâ”€â”€ flash_attention.py       # Flash Attention demonstrations
        â”‚   â”œâ”€â”€ advanced_quantization.py # INT4/INT8 quantization
        â”‚   â”œâ”€â”€ diffusion_generation.py  # Stable Diffusion for images
        â”‚   â”œâ”€â”€ config.py               # Configuration management
        â”‚   â”œâ”€â”€ utils.py                # Helper functions
        â”‚   â””â”€â”€ main.py                 # Main demo runner
        â”œâ”€â”€ notebooks/
        â”‚   â”œâ”€â”€ tutorial.ipynb          # Complete Chapter 8 tutorial
        â”‚   â”œâ”€â”€ pipeline_exploration.ipynb
        â”‚   â””â”€â”€ optimization_benchmarks.ipynb
        â”œâ”€â”€ docs/
        â”‚   â”œâ”€â”€ art_08.md               # Original chapter
        â”‚   â””â”€â”€ art_08i.md              # Improved chapter with grammar fixes
        â”œâ”€â”€ tests/
        â”‚   â””â”€â”€ test_basic.py           # Unit tests
        â””â”€â”€ examples/
            â””â”€â”€ retail_workflow.py      # Real-world retail example
        ```

        ## Key Features

        ### 1. Custom Pipeline Creation
        - Subclass and extend standard pipelines
        - Chain multiple models together
        - Add business logic and preprocessing

        ### 2. Efficient Data Processing
        - Stream datasets without memory limits
        - Parallel transformations with `map()`
        - Smart batching for 10x speedup

        ### 3. Model Optimization
        - INT8/INT4 quantization for 75% cost reduction
        - Edge deployment strategies
        - PEFT/LoRA for efficient fine-tuning

        ### 4. Synthetic Data Generation
        - LLM-based text generation
        - SDXL image creation
        - Quality validation pipelines

        ## Available Tasks

        ```bash
        task --list        # Show all available tasks
        task setup         # Set up the development environment
        task run           # Run the main demonstration
        task test          # Run tests
        task format        # Format code with black
        task lint          # Run linting checks
        ```

        ## Known Issues & Solutions

        1. **sentencepiece on macOS**: The setup automatically handles this by installing via pip
        2. **bitsandbytes on macOS**: Limited functionality (no INT8 quantization) - this is expected
        3. **GPU Support**: 
           - NVIDIA GPUs: Full CUDA support
           - Apple Silicon: MPS (Metal) support
           - CPU: Fallback for all systems

        ## Learning Path

        1. **Start with the tutorial notebook**: `notebooks/tutorial.ipynb` - Interactive examples with explanations
        2. Explore `custom_pipelines.py` to understand pipeline anatomy
        3. Study `efficient_data_handling.py` for handling large-scale data
        4. Run `optimization_demo.py` to see performance improvements
        5. Experiment with `synthetic_data.py` for data augmentation
        6. Review `production_workflows.py` for real-world patterns
        7. Check out the advanced modules:
           - `peft_lora.py` - Parameter-efficient fine-tuning
           - `flash_attention.py` - GPU optimization techniques
           - `advanced_quantization.py` - INT4/INT8 quantization
           - `diffusion_generation.py` - Image generation
           - `edge_deployment.py` - ONNX export

        ## Performance Benchmarks

        | Technique | Before | After | Improvement |
        |-----------|--------|-------|-------------|
        | Batching | 50ms/item | 6.25ms/item | 8x faster |
        | INT8 Quantization | 400MB | 100MB | 75% smaller |
        | Streaming | 100GB RAM | 200MB RAM | 500x less |
        | PEFT Fine-tuning | 13GB | 40MB | 99.7% fewer params |

        ## Resources

        - [Hugging Face Pipelines Documentation](https://huggingface.co/docs/transformers/main_classes/pipelines)
        - [Datasets Library Guide](https://huggingface.co/docs/datasets)
        - [Quantization Tutorial](https://huggingface.co/docs/transformers/quantization)
        - [PEFT Documentation](https://huggingface.co/docs/peft)

        ## Documentation

        - **Original Chapter**: `docs/art_08.md` - Chapter 8 from the book
        - **Improved Chapter**: `docs/art_08i.md` - Enhanced version with grammar improvements
        - **Tutorial Notebook**: `notebooks/tutorial.ipynb` - Hands-on examples with explanations
        - **Claude Guide**: `CLAUDE.md` - Instructions for future Claude Code instances
        - **Setup Notes**: `SETUP_NOTES.md` - Detailed setup documentation

        ## Contributing

        1. Fork the repository
        2. Create a feature branch (`git checkout -b feature/amazing-feature`)
        3. Make your changes
        4. Run tests and linting:
           ```bash
           task test
           task lint
           task format
           ```
        5. Commit your changes (`git commit -m 'Add amazing feature'`)
        6. Push to the branch (`git push origin feature/amazing-feature`)
        7. Open a Pull Request

        ## Repository

        GitHub: [https://github.com/RichardHightower/art_hug_08](https://github.com/RichardHightower/art_hug_08)

        ## License

        This project is licensed under the MIT License - see the LICENSE file for details.

        ## Acknowledgments

        - Chapter 8 examples from "The Art of Hugging Face Transformers"
        - Hugging Face team for the amazing transformers library
        - Community contributors and testers
      metadata:
        extension: .md
        size_bytes: 7372
        language: markdown
    SETUP_NOTES.md:
      content: |-
        # Setup Notes

        ## Environment Setup Complete âœ…

        The project environment has been successfully set up with the following components:

        ### Installed Packages
        - âœ… **transformers** - Core Hugging Face library
        - âœ… **torch** - PyTorch 2.2.2 with MPS (Apple Silicon) support
        - âœ… **datasets** - Hugging Face datasets library
        - âœ… **diffusers** - Updated to 0.31.0 for compatibility
        - âœ… **peft** - Parameter-efficient fine-tuning
        - âœ… **sentencepiece** - Tokenizer support
        - âœ… **accelerate** - Training acceleration
        - âš ï¸ **bitsandbytes** - Installed but limited on macOS (CUDA-only features)

        ### Known Issues & Solutions

        1. **sentencepiece build failure**: Resolved by installing via pip instead of poetry
        2. **diffusers compatibility**: Fixed by updating to version 0.31.0
        3. **bitsandbytes on macOS**: Limited functionality (no 8-bit quantization) - this is expected

        ### Running the Tutorial Notebook

        To run the tutorial notebook:

        ```bash
        # Activate the environment
        poetry shell

        # Or run directly with poetry
        poetry run jupyter notebook notebooks/tutorial.ipynb
        ```

        ### Testing the Environment

        Run the test script to verify everything is working:

        ```bash
        poetry run python test_environment.py
        ```

        ### Device Support
        - **macOS (Apple Silicon)**: Uses MPS device for GPU acceleration
        - **Linux/Windows with NVIDIA GPU**: Would use CUDA
        - **CPU**: Fallback option for all platforms

        ## Next Steps

        1. Run the tutorial notebook to explore all Chapter 8 examples
        2. Check out the demonstration scripts in `src/`
        3. Read the improved documentation in `docs/art_08i.md`
      metadata:
        extension: .md
        size_bytes: 1583
        language: markdown
    Taskfile.yml:
      content: |
        version: '3'

        vars:
          PYTHON_VERSION: 3.12.9

        tasks:
          default:
            desc: "Show available tasks"
            cmds:
              - task --list

          setup:
            desc: "Set up the Python environment and install dependencies"
            cmds:
              - pyenv install -s {{.PYTHON_VERSION}}
              - pyenv local {{.PYTHON_VERSION}}
              - poetry config virtualenvs.in-project true
              - poetry install
              - poetry run pip install sentencepiece  # Handle macOS build issue
              - mkdir -p data/images data/audio output synthetic cache
              - cp .env.example .env || true
              - 'echo "Setup complete! Run tests with: task test-env"'

          test-env:
            desc: "Test that the environment is set up correctly"
            cmds:
              - poetry run python test_environment.py

          quick-start:
            desc: "Run a quick demo to verify everything works"
            cmds:
              - poetry run python test_demo.py

          run:
            desc: "Run the main workflow demonstration"
            cmds:
              - poetry run python -m src.main

          pipelines:
            desc: "Run custom pipeline examples"
            cmds:
              - poetry run python -m src.custom_pipelines

          data:
            desc: "Run data workflow demonstrations"
            cmds:
              - poetry run python -m src.data_workflows

          optimize:
            desc: "Run optimization benchmarks"
            cmds:
              - poetry run python -m src.optimization

          synthetic:
            desc: "Run synthetic data generation"
            cmds:
              - poetry run python -m src.synthetic_data

          retail:
            desc: "Run retail workflow example"
            cmds:
              - poetry run python examples/retail_workflow.py

          edge:
            desc: "Run edge deployment demonstration"
            cmds:
              - poetry run python -m src.edge_deployment

          peft:
            desc: "Run PEFT/LoRA demonstration"
            cmds:
              - poetry run python -m src.peft_lora

          flash:
            desc: "Run Flash Attention demonstration"
            cmds:
              - poetry run python -m src.flash_attention

          quantization:
            desc: "Run advanced quantization demonstration"
            cmds:
              - poetry run python -m src.advanced_quantization

          diffusion:
            desc: "Run diffusion generation demonstration"
            cmds:
              - poetry run python -m src.diffusion_generation

          production:
            desc: "Run production workflow demonstration"
            cmds:
              - poetry run python -m src.production_workflows

          test:
            desc: "Run all tests"
            cmds:
              - poetry run pytest tests/ -v

          test-coverage:
            desc: "Run tests with coverage"
            cmds:
              - poetry run pytest tests/ --cov=src --cov-report=html

          format:
            desc: "Format code with black"
            cmds:
              - poetry run black src/ tests/

          lint:
            desc: "Lint code with ruff"
            cmds:
              - poetry run ruff check src/ tests/

          clean:
            desc: "Clean up temporary files"
            cmds:
              - find . -type d -name "__pycache__" -exec rm -rf {} +
              - find . -type f -name "*.pyc" -delete
              - rm -rf .pytest_cache/
              - rm -rf htmlcov/
              - rm -rf .coverage

          notebook:
            desc: "Start Jupyter notebook server"
            cmds:
              - poetry run jupyter notebook

          benchmark:
            desc: "Run performance benchmarks"
            cmds:
              - poetry run python -m src.optimization --benchmark

          profile:
            desc: "Profile memory usage"
            cmds:
              - poetry run mprof run python -m src.data_workflows
              - poetry run mprof plot
      metadata:
        extension: .yml
        size_bytes: 3247
        language: yaml
    test_demo.py:
      content: |-
        #!/usr/bin/env python
        """Test basic demo functionality."""

        print("Testing basic pipeline functionality...")

        from transformers import pipeline

        # Create a simple pipeline
        clf = pipeline('sentiment-analysis')

        # Test it
        text = "I love this new Hugging Face library!"
        result = clf(text)

        print(f"\nText: {text}")
        print(f"Result: {result}")

        print("\nâœ… Basic functionality works!")
      metadata:
        extension: .py
        size_bytes: 381
        language: python
    CLAUDE.md:
      content: |-
        # CLAUDE.md

        This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

        ## Project Overview

        This is a comprehensive Python project demonstrating **custom pipelines and data workflows** using HuggingFace Transformers. The project serves as Chapter 8 examples from the HuggingFace Transformers book, showcasing how to transform from a pipeline user to a workflow architect.

        **Primary Goal**: Demonstrate advanced model optimization, custom pipeline creation, efficient data handling, and production-ready workflows for transformer models.

        ## Technology Stack

        ### Core Technologies
        - **Language**: Python 3.12
        - **Framework**: HuggingFace Transformers, PyTorch
        - **Package Manager**: Poetry
        - **Task Runner**: Go Task (Taskfile.yml)
        - **Environment Management**: pyenv + Poetry

        ### Key Dependencies
        - **transformers** (^4.40.0) - Core HuggingFace library
        - **torch** (^2.2.0) + torchvision + torchaudio - PyTorch ecosystem
        - **datasets** (^2.14.0) - Efficient data handling
        - **diffusers** (^0.25.0) - Diffusion models for synthetic data
        - **accelerate** (^0.25.0) - Model acceleration
        - **bitsandbytes** (^0.41.0) - Quantization
        - **peft** (^0.8.0) - Parameter-efficient fine-tuning
        - **optimum** (^1.16.0) - Model optimization

        ### Development Tools
        - **pytest** - Testing framework
        - **black** - Code formatting
        - **ruff** - Linting
        - **jupyter** - Interactive notebooks
        - **memory-profiler** - Performance monitoring

        ## Project Structure

        ```
        /Users/richardhightower/src/art_hug_08/
        â”œâ”€â”€ src/                           # Main source code
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ config.py                  # Configuration management
        â”‚   â”œâ”€â”€ main.py                    # Entry point with CLI
        â”‚   â”œâ”€â”€ custom_pipelines.py        # Custom pipeline implementations
        â”‚   â”œâ”€â”€ data_workflows.py          # Efficient data handling demos
        â”‚   â”œâ”€â”€ optimization.py            # Model optimization techniques
        â”‚   â”œâ”€â”€ synthetic_data.py          # Synthetic data generation
        â”‚   â”œâ”€â”€ production_workflows.py    # End-to-end production examples
        â”‚   â””â”€â”€ utils.py                   # Helper utilities
        â”œâ”€â”€ tests/
        â”‚   â””â”€â”€ test_workflows.py          # Unit tests
        â”œâ”€â”€ notebooks/                     # Jupyter notebooks
        â”‚   â”œâ”€â”€ pipeline_exploration.ipynb
        â”‚   â””â”€â”€ optimization_benchmarks.ipynb
        â”œâ”€â”€ examples/
        â”‚   â””â”€â”€ retail_workflow.py         # Real-world retail example
        â”œâ”€â”€ .venv/                         # Virtual environment
        â”œâ”€â”€ .yamlproject/                  # YAML project configuration
        â”œâ”€â”€ .claude/                       # Claude configuration
        â”œâ”€â”€ pyproject.toml                 # Poetry configuration
        â”œâ”€â”€ Taskfile.yml                   # Task definitions
        â”œâ”€â”€ .env.example                   # Environment template
        â”œâ”€â”€ .gitignore                     # Git ignore rules
        â”œâ”€â”€ .python-version                # Python version (3.12.9)
        â””â”€â”€ README.md                      # Project documentation
        ```

        ## Development Setup

        ### Prerequisites
        ```bash
        # Ensure pyenv and Poetry are installed
        pyenv install 3.12.9
        pyenv local 3.12.9

        # Install Go Task (optional but recommended)
        # brew install go-task  # macOS
        # snap install task     # Linux
        ```

        ### Installation & Setup
        ```bash
        # 1. Install dependencies
        task setup
        # Or manually:
        poetry install
        cp .env.example .env

        # 2. Configure environment
        # Edit .env with your settings:
        # - HUGGINGFACE_TOKEN (for gated models)
        # - DEFAULT_DEVICE (cuda/cpu/mps)
        # - Model and data paths
        ```

        ## Essential Commands

        ### Setup and Core Development
        ```bash
        task setup         # Install dependencies and setup environment
        task run           # Run all demonstrations
        task test          # Run all tests
        task format        # Format code with black (88 char line length)
        task lint          # Lint code with ruff
        ```

        ### Module-Specific Workflows
        ```bash
        task pipelines     # Custom pipeline examples
        task data          # Data workflow demonstrations  
        task optimize      # Model optimization benchmarks
        task synthetic     # Synthetic data generation
        task retail        # Retail workflow example
        ```

        ### Additional Development Tools
        ```bash
        task test-coverage # Run tests with coverage report
        task notebook      # Start Jupyter server
        task benchmark     # Run performance benchmarks
        task profile       # Profile memory usage
        task clean         # Clean temporary files
        ```

        ### Manual Execution
        ```bash
        # Run specific modules directly
        poetry run python -m src.main --demo pipelines
        poetry run python -m src.custom_pipelines
        poetry run python -m src.optimization --benchmark
        ```

        ## Architecture & Key Components

        ### 1. Custom Pipeline Architecture (`src/custom_pipelines.py`)
        - **CustomSentimentPipeline**: Extends standard pipelines with preprocessing and business logic
        - **SentimentNERPipeline**: Composite pipeline combining sentiment + NER
        - Demonstrates pipeline inheritance, preprocessing hooks, and result post-processing

        ### 2. Data Workflows (`src/data_workflows.py`)
        - Efficient dataset loading with HuggingFace Datasets
        - Batch processing with `map()` for 8x performance improvement
        - Streaming for handling massive datasets (6GBâ†’200MB memory)
        - Filtering, column selection, and custom dataset creation

        ### 3. Model Optimization (`src/optimization.py`)
        - **Quantization**: INT8/FP16 for 75% memory reduction
        - **Batching**: Up to 8x throughput improvement
        - **Dynamic quantization**: CPU-optimized inference
        - Comprehensive benchmarking utilities

        ### 4. Synthetic Data Generation (`src/synthetic_data.py`)
        - LLM-based text generation with GPT-2
        - Quality validation and filtering pipelines
        - Data augmentation for imbalanced datasets
        - Validation scoring (length, vocabulary, diversity)

        ### 5. Production Workflows (`src/production_workflows.py`)
        - **RetailReviewWorkflow**: Complete end-to-end example
        - Business logic integration (priority scoring, categorization)
        - Real-time processing simulation
        - Insight generation and reporting

        ## Configuration System

        ### Environment Variables (`.env`)
        ```bash
        # Core Settings
        HUGGINGFACE_TOKEN=your_token_here
        DEFAULT_DEVICE=cuda                # cuda/cpu/mps
        BATCH_SIZE=32
        MAX_LENGTH=512

        # Data Paths
        DATA_PATH=./data
        CACHE_DIR=./cache
        SYNTHETIC_DATA_PATH=./synthetic
        NUM_WORKERS=4

        # Optimization
        ENABLE_QUANTIZATION=true
        QUANTIZATION_BITS=8
        ENABLE_FLASH_ATTENTION=true

        # Validation
        VALIDATION_THRESHOLD=0.85
        ```

        ### Default Models (`src/config.py`)
        - Sentiment: `distilbert-base-uncased-finetuned-sst-2-english`
        - NER: `dbmdz/bert-large-cased-finetuned-conll03-english`
        - Classification: `facebook/bart-large-mnli`
        - Generation: `mistralai/Mistral-7B-Instruct-v0.2`

        ## Testing Strategy

        ### Unit Tests (`tests/test_workflows.py`)
        - Custom pipeline functionality
        - Data preprocessing validation
        - Synthetic data quality checks
        - Utility function testing
        - Import verification

        ### Running Tests
        ```bash
        task test           # Basic test run
        task test-coverage  # With coverage report
        poetry run pytest tests/ -v --cov=src
        ```

        ## Performance Benchmarks

        | Optimization Technique | Performance Gain | Memory Reduction | Use Case |
        |----------------------|------------------|------------------|----------|
        | Batching (32 samples) | 8x faster | - | GPU servers |
        | INT8 Quantization | 2-3x faster | 75% smaller | CPU/Edge deployment |
        | FP16 Precision | 1.5-2x faster | 50% smaller | Modern GPUs |
        | DistilBERT (pruning) | 3-4x faster | 60% smaller | General purpose |
        | Streaming datasets | - | 500x less RAM | Large datasets |

        ## Code Quality Standards

        ### Formatting & Linting
        - **Black**: Line length 88, Python 3.12 target
        - **Ruff**: Comprehensive linting (E, F, I, N, UP, B, C4, SIM rules)
        - **Type hints**: Encouraged but not enforced
        - **Docstrings**: Required for public functions

        ### Code Organization
        - Modular design with clear separation of concerns
        - Configuration centralized in `src/config.py`
        - Utilities shared via `src/utils.py`
        - Examples demonstrate real-world usage patterns

        ## Common Development Workflows

        ### Adding a New Pipeline
        1. Create pipeline class in `src/custom_pipelines.py`
        2. Extend from `transformers.Pipeline`
        3. Override `preprocess()` and `postprocess()` methods
        4. Add demonstration in `demonstrate_custom_pipelines()`
        5. Add tests in `tests/test_workflows.py`

        ### Performance Optimization
        1. Benchmark baseline performance with `src/optimization.py`
        2. Apply optimization technique (quantization, batching, etc.)
        3. Measure improvement using benchmark utilities
        4. Document results in performance tables

        ### Data Workflow Development
        1. Use HuggingFace Datasets for data loading
        2. Implement efficient preprocessing with `map(batched=True)`
        3. Add streaming support for large datasets
        4. Include memory profiling and benchmarks

        ## Troubleshooting

        ### Common Issues

        #### Installation Problems
        ```bash
        # Poetry dependency conflicts
        poetry lock --no-update
        poetry install

        # CUDA/PyTorch compatibility
        pip install torch --index-url https://download.pytorch.org/whl/cu121
        ```

        #### Memory Issues
        ```bash
        # Enable memory profiling
        task profile

        # Use smaller batch sizes
        export BATCH_SIZE=8

        # Enable streaming for large datasets
        # Use dataset.streaming=True in code
        ```

        #### Model Loading Errors
        ```bash
        # Set HuggingFace token for gated models
        export HUGGINGFACE_TOKEN=your_token

        # Clear cache if corrupted
        rm -rf ~/.cache/huggingface/

        # Use CPU if GPU issues
        export DEFAULT_DEVICE=cpu
        ```

        ## AI Assistant Guidelines

        ### When Working on This Project

        1. **Understand the Architecture**: This is a comprehensive ML workflow project with multiple demonstration modules
        2. **Follow the Module Pattern**: Each workflow type has its own module (pipelines, data, optimization, synthetic)
        3. **Use Configuration**: All settings should go through `src/config.py`
        4. **Add Tests**: Any new functionality should include unit tests
        5. **Performance Focus**: This project emphasizes optimization and efficiency
        6. **Documentation**: Update this CLAUDE.md file for significant changes

        ### Preferred Development Patterns
        - Use Task runner for common operations (`task test`, `task format`)
        - Leverage Poetry for dependency management
        - Follow the existing code organization in `src/`
        - Add benchmarking for performance-related features
        - Include real-world examples in `examples/`

        ### Code Style
        - Use Black formatting (88 char line length)
        - Follow Ruff linting rules
        - Add type hints where helpful
        - Include docstrings for public functions
        - Use descriptive variable names

        ## Additional Resources

        ### Documentation Links
        - [HuggingFace Transformers](https://huggingface.co/docs/transformers)
        - [HuggingFace Datasets](https://huggingface.co/docs/datasets)
        - [PyTorch Performance](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
        - [Model Optimization](https://huggingface.co/docs/transformers/optimization)

        ### Learning Path
        1. Start with `src/main.py` to understand the overall structure
        2. Explore `src/custom_pipelines.py` for pipeline customization
        3. Study `src/data_workflows.py` for efficient data handling
        4. Examine `src/optimization.py` for performance techniques
        5. Review `src/production_workflows.py` for real-world patterns

        This project represents production-ready patterns for transformer model deployment, emphasizing performance, efficiency, and maintainability.
      metadata:
        extension: .md
        size_bytes: 11429
        language: markdown
    test_environment.py:
      content: |-
        #!/usr/bin/env python
        """Test environment setup."""

        def test_imports():
            """Test all required imports."""
            print("Testing imports...")
            
            try:
                import transformers
                print("âœ“ transformers imported successfully")
            except ImportError as e:
                print(f"âœ— transformers import failed: {e}")
            
            try:
                import torch
                print("âœ“ torch imported successfully")
                print(f"  PyTorch version: {torch.__version__}")
                print(f"  CUDA available: {torch.cuda.is_available()}")
                print(f"  MPS available: {torch.backends.mps.is_available()}")
            except ImportError as e:
                print(f"âœ— torch import failed: {e}")
            
            try:
                import datasets
                print("âœ“ datasets imported successfully")
            except ImportError as e:
                print(f"âœ— datasets import failed: {e}")
            
            try:
                import diffusers
                print("âœ“ diffusers imported successfully")
            except ImportError as e:
                print(f"âœ— diffusers import failed: {e}")
            
            try:
                import peft
                print("âœ“ peft imported successfully")
            except ImportError as e:
                print(f"âœ— peft import failed: {e}")
            
            try:
                import sentencepiece
                print("âœ“ sentencepiece imported successfully")
            except ImportError as e:
                print(f"âœ— sentencepiece import failed: {e}")
            
            try:
                import accelerate
                print("âœ“ accelerate imported successfully")
            except ImportError as e:
                print(f"âœ— accelerate import failed: {e}")
            
            try:
                import bitsandbytes
                print("âœ“ bitsandbytes imported successfully")
            except ImportError as e:
                print(f"âœ— bitsandbytes import failed: {e}")
            
            print("\nAll imports successful!")
            

        def test_basic_pipeline():
            """Test basic pipeline functionality."""
            print("\nTesting basic pipeline...")
            
            from transformers import pipeline
            
            # Create a simple sentiment analysis pipeline
            clf = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')
            
            # Test it
            result = clf("I love Hugging Face!")
            print(f"Pipeline test result: {result}")
            

        if __name__ == "__main__":
            test_imports()
            test_basic_pipeline()
            print("\nâœ… Environment setup complete!")
      metadata:
        extension: .py
        size_bytes: 2301
        language: python
    QUICK_START.md:
      content: |-
        # Quick Start Guide

        Welcome to the HuggingFace Workflows project! This guide will get you up and running in minutes.

        ## 1. Prerequisites

        Make sure you have:
        - Git
        - Task (go-task)
        - pyenv

        ## 2. Clone and Setup

        ```bash
        # Clone the repository
        git clone git@github.com:RichardHightower/art_hug_08.git
        cd art_hug_08

        # Run the complete setup (installs Python, Poetry, and all dependencies)
        task setup
        ```

        ## 3. Verify Installation

        ```bash
        # Test the environment
        task test-env

        # Run a quick demo
        task quick-start
        ```

        ## 4. Explore Examples

        ### Interactive Tutorial (Recommended)
        ```bash
        poetry run jupyter notebook notebooks/tutorial.ipynb
        ```

        ### Individual Demos
        ```bash
        # Custom pipelines
        poetry run python src/custom_pipelines.py

        # Efficient data handling
        poetry run python src/efficient_data_handling.py

        # Optimization techniques
        poetry run python src/optimization_demo.py

        # Production workflow
        poetry run python src/production_workflows.py
        ```

        ## 5. Advanced Examples

        ```bash
        # PEFT/LoRA fine-tuning
        task peft

        # Flash Attention
        task flash

        # Quantization
        task quantization

        # Diffusion models
        task diffusion
        ```

        ## Troubleshooting

        - **Import errors**: Make sure you're using Poetry: `poetry run python ...`
        - **GPU issues**: The code automatically detects and uses available hardware (CUDA/MPS/CPU)
        - **Memory errors**: Reduce batch sizes in the examples

        ## Next Steps

        1. Read the improved documentation: `docs/art_08i.md`
        2. Work through the tutorial notebook
        3. Experiment with the examples
        4. Build your own custom workflows!

        Happy learning! ðŸš€
      metadata:
        extension: .md
        size_bytes: 1572
        language: markdown
    ENVIRONMENT_STATUS.md:
      content: |-
        # Environment Setup Status âœ…

        ## Setup Completed Successfully!

        The Hugging Face workflows environment has been successfully configured and tested.

        ### What was fixed:
        1. **sentencepiece** - Installed via pip to avoid build issues on macOS ARM64
        2. **diffusers** - Updated to v0.31.0 for compatibility with latest huggingface-hub
        3. **Import paths** - Fixed relative imports in all demonstration modules
        4. **Pipeline registration** - Updated for newer transformers API

        ### Verified Components:
        - âœ… Basic pipeline functionality
        - âœ… Sentiment analysis
        - âœ… MPS (Apple Silicon) GPU support
        - âœ… All core libraries imported successfully

        ### Running Examples:

        1. **Test the environment:**
           ```bash
           poetry run python test_environment.py
           ```

        2. **Run simple demo:**
           ```bash
           poetry run python test_demo.py
           ```

        3. **Launch tutorial notebook:**
           ```bash
           poetry run jupyter notebook notebooks/tutorial.ipynb
           ```

        4. **Run individual demos:**
           ```bash
           poetry run python src/custom_pipelines.py
           poetry run python src/efficient_data_handling.py
           poetry run python src/optimization_demo.py
           ```

        ### Notes:
        - The environment uses MPS (Metal Performance Shaders) for GPU acceleration on Apple Silicon
        - bitsandbytes has limited functionality on macOS (no INT8 quantization)
        - All examples have been adapted to work with the current library versions

        The setup is complete and ready for use! ðŸŽ‰
      metadata:
        extension: .md
        size_bytes: 1431
        language: markdown
    .claude/settings.local.json:
      content: |-
        {
          "permissions": {
            "allow": [
              "mcp__filesystem__list_directory",
              "mcp__filesystem__directory_tree",
              "mcp__filesystem__read_file",
              "mcp__filesystem__search_files",
              "mcp__filesystem__write_file",
              "mcp__filesystem__read_multiple_files",
              "Bash(task format)",
              "Bash(task lint)",
              "Bash(poetry run ruff:*)",
              "Bash(poetry install:*)",
              "Bash(pip install:*)",
              "Bash(python:*)",
              "Bash(poetry:*)",
              "Bash(task run)",
              "Bash(task quick-start)"
            ],
            "deny": []
          }
        }
      metadata:
        extension: .json
        size_bytes: 552
        language: json
    docs/art_08.md:
      content: |-
        # Customizing Pipelines and Data Workflows: Advanced Models and Efficient Processing

        ```mermaid
        mindmap
          root((Workflow Mastery))
            Pipeline Anatomy
              Components
              Customization
              Debugging
              Registration
            Custom Workflows
              Preprocessing
              Composition
              Business Logic
              Production Scale
            Efficient Data
              Datasets Library
              Streaming
              Transformation
              Annotation
            Optimization
              Batching
              Quantization
              Deployment
              Edge Computing
            Synthetic Data
              Text Generation
              Image Creation
              Quality Control
              Fairness
        ```

        **Step-by-Step Explanation:**
        - Root node focuses on **Workflow Mastery** - transforming from user to architect
        - Branch covers **Pipeline Anatomy** including components, customization, debugging
        - Branch explores **Custom Workflows** with preprocessing, composition, business logic
        - Branch details **Efficient Data** handling with Datasets library and streaming
        - Branch shows **Optimization** techniques from batching to edge deployment
        - Branch presents **Synthetic Data** generation for augmentation and fairness

        ## Environment Setup

        Before diving into custom pipelines, let's set up a proper development environment:

        ### Poetry Setup (Recommended for Projects)
        ```bash
        # Install poetry if not already installed
        curl -sSL https://install.python-poetry.org | python3 -

        # Create new project
        poetry new huggingface-workflows
        cd huggingface-workflows

        # Add dependencies with flexible versioning
        poetry add "transformers>=4.40.0,<5.0.0" torch torchvision torchaudio
        poetry add "datasets>=2.14.0" diffusers accelerate sentencepiece
        poetry add pillow soundfile bitsandbytes
        poetry add --group dev jupyter ipykernel matplotlib

        # Activate environment
        poetry shell
        ```

        ### Mini-conda Setup (Alternative)
        ```bash
        # Download and install mini-conda from https://docs.conda.io/en/latest/miniconda.html

        # Create environment with Python 3.12.9
        conda create -n huggingface-workflows python=3.12.9
        conda activate huggingface-workflows

        # Install packages
        conda install -c pytorch -c huggingface transformers torch torchvision torchaudio
        conda install -c conda-forge datasets diffusers accelerate pillow soundfile matplotlib
        pip install sentencepiece bitsandbytes
        ```

        ### Traditional pip with pyenv
        ```bash
        # Install pyenv (macOS/Linux)
        curl https://pyenv.run | bash
        # Configure shell (add to ~/.bashrc or ~/.zshrc)
        export PATH="$HOME/.pyenv/bin:$PATH"
        eval "$(pyenv init -)"

        # Install Python 3.12.9 with pyenv
        pyenv install 3.12.9
        pyenv local 3.12.9

        # Create virtual environment
        python -m venv venv
        source venv/bin/activate  # On Windows: venv\Scripts\activate

        # Install packages with flexible versioning
        pip install "transformers>=4.40.0,<5.0.0" torch torchvision torchaudio
        pip install "datasets>=2.14.0" diffusers accelerate sentencepiece
        pip install pillow soundfile bitsandbytes jupyter matplotlib
        ```

        ## Introduction: From Magic Pipelines to Master Chefâ€”Why Custom Data Workflows Matter

        Imagine Hugging Face pipelines as meal kits: quick, convenient, and perfect for a fast start. **Drop in. Run. Done.** But what happens when your customers have allergies? When the recipe doesn't scale to a hundred guests? When you need that secret sauce only you know how to make?

        This chapter transforms you from pipeline user to workflow architect. You'll learn how to peek inside Hugging Face pipelines, swap components, and design data workflows that handle scale, complexity, and real business needs.

        Let's see just how easy pipelines make thingsâ€”and where their limits begin.

        ### Quick Start: Hugging Face Pipeline (2025 Best Practice)

        ```python
        # Modern quick-start with explicit model and device
        from transformers import pipeline

        # Specify model checkpoint and device for reproducibility
        clf = pipeline(
            'sentiment-analysis',
            model='distilbert-base-uncased-finetuned-sst-2-english',
            device=0  # 0 for CUDA GPU, -1 for CPU, 'mps' for Apple Silicon
        )

        # Run prediction on text
        result = clf('I love Hugging Face!')
        print(result)
        # Output: [{'label': 'POSITIVE', 'score': 0.9998}]

        # Check model card: https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english
        ```

        **Magic!** This single command downloads a pre-trained model, loads the tokenizer, and formats your data. Instant resultsâ€”no deep setup required. By specifying model and device, you ensure reproducibility.

        But out-of-the-box pipelines crumble when you need:
        - Custom data cleaning (HTML, emojis, multilingual text)
        - Chained models (sentiment + entity recognition)  
        - Speed optimization (batching, device placement)
        - Business logic (filtering, compliance checks)
        - Scale (streaming, batch processing)

        Ever felt like a chef stuck with meal kits when you need to cater a wedding?

        Here's a real scenario: Your retail chain processes customer reviews from multiple platforms. Standard pipelines work for demos but fail when you need to:
        - Clean data from Twitter, Amazon, and internal systems
        - Add product metadata
        - Process 10,000 reviews per minute
        - Log for compliance
        - Stream from S3 buckets

        **Without custom workflows? Bottlenecks. Errors. Missed SLAs.**

        ### Custom Preprocessing Before Inference

        ```python
        def custom_preprocess(text):
            # Normalize text for consistent predictions
            import string
            text = text.lower()
            return text.translate(str.maketrans('', '', string.punctuation))

        texts = ["Wow! Amazing product!!!", "I don't like this..."]

        # Clean then predict
        cleaned = [custom_preprocess(t) for t in texts]
        results = clf(cleaned, batch_size=16)  # Batch for speed!
        print(results)
        ```

        **Step-by-step:**
        1. Define preprocessing (lowercase, strip punctuation)
        2. Clean inputs before pipeline
        3. Use `batch_size` for 5x faster inference
        4. Get reliable predictions on normalized data

        For production, embed preprocessing directly:

        ### Advanced: Pipeline Subclassing

        ```python
        from transformers import Pipeline

        class CustomSentimentPipeline(Pipeline):
            def preprocess(self, inputs):
                # Strip HTML, normalize text
                text = inputs.lower()
                import string
                text = text.translate(str.maketrans('', '', string.punctuation))
                return super().preprocess(text)
            
            def postprocess(self, outputs):
                # Add confidence thresholds
                results = super().postprocess(outputs)
                for r in results:
                    r['confident'] = r['score'] > 0.95
                return results
        ```

        ### Streaming Large-Scale Data

        ```python
        from datasets import load_dataset

        # Stream massive datasets without memory issues
        dataset = load_dataset('csv', data_files='reviews.csv', 
                              split='train', streaming=True)

        batch_size = 32
        batch = []
        for example in dataset:
            batch.append(custom_preprocess(example['text']))
            if len(batch) == batch_size:
                results = clf(batch, batch_size=batch_size)
                # Process results (save, log, etc.)
                batch = []
        ```

        **Key Takeaways:**
        - Pipelines = fast start, but limited for production
        - Always specify model + device for reproducibility
        - Custom workflows handle real business needs
        - Batch processing can 10x your throughput

        Ready to peek under the hood? Let's explore pipeline anatomy.

        ## From Pipeline to Custom Components

        ```mermaid
        classDiagram
            class Pipeline {
                +model: PreTrainedModel
                +tokenizer: PreTrainedTokenizer  
                +processor: Processor
                +framework: str
                +device: torch.device
                +preprocess(inputs)
                +_forward(model_inputs)
                +postprocess(outputs)
                +__call__(inputs)
            }
            
            class Tokenizer {
                +vocab_size: int
                +model_max_length: int
                +encode(text)
                +decode(ids)
                +batch_encode_plus(texts)
            }
            
            class Model {
                +config: PretrainedConfig
                +num_parameters()
                +forward(input_ids)
                +to(device)
                +eval()
            }
            
            class Processor {
                +feature_extractor
                +tokenizer
                +__call__(inputs)
                +batch_decode(outputs)
            }
            
            Pipeline --> Tokenizer : uses
            Pipeline --> Model : uses
            Pipeline --> Processor : optional
        ```

        ### Pipeline Components: Under the Hood

        Think of pipelines as assembly lines. **Raw input â†’ Predictions.** Three workers make it happen:

        - **Tokenizer:** The translator. Converts "Hello world" â†’ [101, 7592, 2088, 102]
        - **Model:** The brain. Neural network processing tokens â†’ predictions
        - **Processor:** The prep cook. Resizes images, extracts audio features (multimodal tasks)

        Let's inspect:

        ```python
        from transformers import pipeline

        clf = pipeline('text-classification')
        print('Model:', clf.model)
        print('Tokenizer:', clf.tokenizer)  
        print('Processor:', getattr(clf, 'processor', None))
        print('Framework:', clf.framework)  # pytorch or tensorflow
        ```

        **Why inspect?** When predictions look wrong, check if model and tokenizer match. Transformers now warns about mismatches!

        ### Customizing Pipelines: Modern Approach

        Real projects need more than vanilla pipelines. As of Transformers 4.40+, customize via:

        1. **Swap components** - Use custom models/tokenizers
        2. **Compose pipelines** - Chain multiple tasks
        3. **Register new types** - Create reusable workflows

        Let's combine sentiment analysis + entity recognition:

        ```python
        from transformers import Pipeline, pipeline
        from transformers.pipelines import register_pipeline

        class SentimentNERPipeline(Pipeline):
            def __init__(self, sentiment_pipeline, ner_pipeline, **kwargs):
                self.sentiment_pipeline = sentiment_pipeline
                self.ner_pipeline = ner_pipeline
                super().__init__(
                    model=sentiment_pipeline.model,
                    tokenizer=sentiment_pipeline.tokenizer,
                    **kwargs
                )
            
            def _forward(self, inputs):
                sentiment = self.sentiment_pipeline(inputs)
                entities = self.ner_pipeline(inputs)
                return {"sentiment": sentiment, "entities": entities}

        # Register for reuse
        register_pipeline(
            task="sentiment-ner",
            pipeline_class=SentimentNERPipeline,
            pt_model=True
        )

        # Use it!
        pipe = pipeline("sentiment-ner")
        result = pipe("Apple Inc. makes amazing products!")
        # {'sentiment': [{'label': 'POSITIVE', 'score': 0.99}],
        #  'entities': [{'word': 'Apple Inc.', 'entity': 'ORG'}]}
        ```

        **Pro tip:** Composition > Inheritance. Build complex workflows from simple parts.

        ### Debugging Pipelines

        When things break (they will), make errors visible:

        ```python
        from transformers.utils import logging
        logging.set_verbosity_debug()

        # Now see EVERYTHING
        clf = pipeline('text-classification')
        result = clf('Debug me!')
        ```

        **Common issues:**
        - Model/tokenizer mismatch â†’ Check families match
        - Wrong input format â†’ Pipelines expect strings, lists, or dicts
        - Memory errors â†’ Reduce batch size or max_length
        - Slow inference â†’ Enable Flash Attention (GPU) or batch more

        **Next:** Let's handle data at scale with ðŸ¤— Datasets.

        ## Efficient Data Handling with ðŸ¤— Datasets

        ```mermaid
        flowchart LR
            A[Raw Data Sources] --> B{Load Dataset}
            B -->|Small Data| C[In-Memory Dataset]
            B -->|Large Data| D[Streaming Dataset]
            
            C --> E[Transform with map]
            D --> F[Stream + Transform]
            
            E --> G[Filter Examples]
            F --> G
            
            G --> H[Batch Processing]
            H --> I[Model Inference]
            
            J[Version Control] -.->|lakeFS| C
            J -.->|Track Changes| E
            
            K[Annotation Tools] -->|Argilla| C
            K -->|Quality Labels| G
        ```

        Ever tried loading Wikipedia into pandas? **Memory explosion!** The ðŸ¤— Datasets library handles millions of examples without breaking a sweat.

        ### Loading and Transforming Data

        ```python
        from datasets import load_dataset

        # Load IMDB reviews
        dataset = load_dataset('imdb', split='train')
        print(f"Dataset size: {len(dataset)}")  # 25,000 examples
        print(dataset[0])  # {'text': '...', 'label': 1}

        # Custom data? Easy!
        custom = load_dataset('csv', data_files='reviews.csv')
        ```

        Transform data efficiently:

        ```python
        def preprocess(batch):
            # Process entire batches at once
            batch['text'] = [text.lower() for text in batch['text']]
            batch['length'] = [len(text.split()) for text in batch['text']]
            return batch

        # Transform with parallel processing
        dataset = dataset.map(preprocess, batched=True, num_proc=4)

        # Filter short reviews
        dataset = dataset.filter(lambda x: x['length'] > 20)
        ```

        **Performance boost:** `batched=True` processes 100x faster than one-by-one!

        ### Streaming Massive Datasets

        What about Wikipedia-scale data? **Stream it!**

        ```python
        # Stream without loading everything
        wiki = load_dataset('wikipedia', '20220301.en', 
                           split='train', streaming=True)

        # Process as you go
        for i, article in enumerate(wiki):
            if i >= 1000:  # Process first 1000
                break
            # Your processing here
            process_article(article['text'])
        ```

        **Memory usage:** 200MB instead of 100GB. **Magic? No. Smart engineering.**

        ### Modern Annotation Workflow

        Great models need great labels:

        ```python
        # Best practices for annotation
        from datasets import Dataset

        # 1. Start small - annotate 100 examples
        pilot_data = dataset.select(range(100))

        # 2. Use Argilla for team annotation
        # See Article 12 for Argilla + HF integration

        # 3. Version your annotations
        # dataset.push_to_hub("company/product-reviews-v2")

        # 4. Track changes with lakeFS for compliance
        ```

        **Remember:** Bad labels = Bad models. Invest in quality annotation.

        ## Optimized Inference and Cost Management

        ```mermaid
        flowchart TD
            A[Original Model] --> B{Optimization Technique}
            
            B -->|Quantization| C[INT8/INT4 Model]
            B -->|Pruning| D[Sparse Model]
            B -->|Compilation| E[Optimized Model]
            
            C --> F[Mobile/Edge]
            C --> G[CPU Deployment]
            D --> H[Cloud API]
            E --> I[GPU Server]
            
            J[Batching] --> K[5-10x Throughput]
            L[Flash Attention] --> M[2x GPU Speed]
            
            style C fill:#90EE90
            style K fill:#FFB6C1
            style M fill:#87CEEB
        ```

        Deploying transformers resembles running a busy restaurant kitchen. **Speed matters. Costs matter more.**

        ### Batching for 10x Throughput

        ```python
        # Slow: One by one
        texts = ["Review 1", "Review 2", "Review 3"]
        for text in texts:
            result = clf(text)  # 3 separate calls

        # Fast: Batch processing
        results = clf(texts, 
                     padding=True,      # Align lengths
                     truncation=True,   # Cap at max_length
                     max_length=128)    # Prevent memory spikes
        # 10x faster on GPU!
        ```

        **Real numbers:** Single inference: 50ms. Batch of 32: 200ms. **That's 8x speedup!**

        ### Modern Quantization: Slash Costs Dramatically

        ```python
        from transformers import AutoModelForSequenceClassification

        # Standard model: 400MB
        model = AutoModelForSequenceClassification.from_pretrained(
            "bert-base-uncased"
        )

        # Quantized model: 100MB, 4x faster!
        model_int8 = AutoModelForSequenceClassification.from_pretrained(
            "bert-base-uncased",
            load_in_8bit=True,
            device_map="auto"
        )

        # For LLMs: INT4 quantization
        model_int4 = AutoModelForCausalLM.from_pretrained(
            "meta-llama/Llama-2-7b-hf",
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16
        )
        ```

        **Cost impact:** AWS inference costs drop 75% with INT8. **Same accuracy. Quarter the price.**

        ### Edge Deployment Strategy

        ```python
        # 1. Choose efficient model
        model_name = "microsoft/MiniLM-L6-H256-uncased"  # 6x smaller than BERT

        # 2. Quantize for edge
        import torch
        quantized = torch.quantization.quantize_dynamic(
            model, {torch.nn.Linear}, dtype=torch.qint8
        )

        # 3. Export to ONNX/GGUF
        model.save_pretrained("model_mobile", push_to_hub=False)

        # 4. Benchmark on target device
        # iPhone 14: 15ms/inference
        # Raspberry Pi: 100ms/inference
        ```

        **Real example:** Retail chain deploys MiniLM on 10,000 handheld scanners. Instant product search. No cloud costs.

        ### Advanced: PEFT for Large Models

        ```python
        from peft import LoraConfig, get_peft_model, TaskType

        # Adapt Llama-2 with 0.1% of parameters
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=16,  # LoRA rank
            lora_alpha=32,
            lora_dropout=0.1,
            target_modules=["q_proj", "v_proj"]
        )

        model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
        peft_model = get_peft_model(model, peft_config)

        # Only 40MB of trainable parameters instead of 13GB!
        peft_model.print_trainable_parameters()
        # trainable params: 4,194,304 || all params: 6,738,415,616 || trainable%: 0.06%
        ```

        **Impact:** Fine-tune Llama-2 on a single GPU. Deploy updates as small adapters. **Efficiency unlocked.**

        ## Synthetic Data Generation

        ```mermaid
        flowchart LR
            A[Analyze Dataset] --> B{Data Issues?}
            
            B -->|Class Imbalance| C[Generate Minority Examples]
            B -->|Rare Events| D[Simulate Edge Cases]
            B -->|Privacy Concerns| E[Create Safe Data]
            
            C --> F[LLM Text Generation]
            D --> G[Diffusion Images]
            E --> H[Structured Data GANs]
            
            F --> I[Quality Filters]
            G --> I
            H --> I
            
            I --> J[Validation]
            J --> K[Augmented Dataset]
            
            style F fill:#FFE4B5
            style G fill:#E6E6FA
            style H fill:#F0E68C
        ```

        Ever wished you had more training data? **Synthetic data is your genie.**

        ### Text Generation with Modern LLMs

        ```python
        from transformers import pipeline

        # Latest open LLM
        gen = pipeline(
            'text-generation',
            model='mistralai/Mistral-7B-Instruct-v0.2',
            device_map='auto'
        )

        # Generate product reviews
        prompt = """Generate a realistic negative product review for headphones.
        Include specific details about sound quality and comfort."""

        reviews = gen(
            prompt,
            max_new_tokens=100,
            num_return_sequences=5,
            temperature=0.8  # More variety
        )

        # Quality check
        for review in reviews:
            if is_realistic(review['generated_text']):
                dataset.add_item(review)
        ```

        **Pro tip:** Always validate synthetic data. Bad synthetic data â†’ Bad models.

        ### Image Generation with SDXL

        ```python
        from diffusers import DiffusionPipeline
        import torch

        # Load latest Stable Diffusion
        pipe = DiffusionPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            torch_dtype=torch.float16,
            variant="fp16"
        )
        pipe = pipe.to("cuda")

        # Generate training images
        prompts = [
            "smartphone with cracked screen, product photo",
            "laptop with coffee spill damage, repair documentation",
            "pristine condition vintage watch, auction listing"
        ]

        for prompt in prompts:
            image = pipe(prompt, num_inference_steps=30).images[0]
            # Add to training set with appropriate labels
        ```

        ### Synthetic Data Validation

        ```python
        def validate_synthetic_data(synthetic, real):
            """Ensure synthetic data improves dataset"""
            
            # 1. Statistical similarity
            real_stats = calculate_statistics(real)
            synth_stats = calculate_statistics(synthetic)
            assert similarity(real_stats, synth_stats) > 0.85
            
            # 2. Diversity check
            assert len(set(synthetic)) / len(synthetic) > 0.95
            
            # 3. Quality filters
            synthetic = filter_nsfw(synthetic)
            synthetic = filter_toxic(synthetic)
            
            # 4. Human review sample
            sample = random.sample(synthetic, 100)
            # Send sample for manual QA
            
            return synthetic
        ```

        **Remember:** Synthetic data augments, not replaces, real data.

        ## Production Workflows in Practice

        ### RetailReviewWorkflow: Complete End-to-End Example

        The codebase includes a sophisticated production workflow that demonstrates real-world integration:

        ```python
        class RetailReviewWorkflow:
            """End-to-end production workflow for retail review analysis."""
            
            def __init__(self):
                # Multi-pipeline architecture
                self.sentiment_pipeline = pipeline('sentiment-analysis')
                self.category_pipeline = pipeline('zero-shot-classification',
                    candidate_labels=['product_quality', 'shipping', 'customer_service', 'pricing'])
                
            def process_review_batch(self, reviews: List[Dict]) -> Dict[str, Any]:
                # Business logic integration
                for review in reviews:
                    # Sentiment analysis
                    sentiment = self.sentiment_pipeline(review['text'])
                    
                    # Category classification  
                    categories = self.category_pipeline(review['text'])
                    
                    # Priority scoring based on keywords
                    priority_score = self._calculate_priority(review['text'])
                    
                    # Generate insights
                    if priority_score > 0.8:
                        self._trigger_alert(review)
                
                return self._generate_business_insights(processed_reviews)
        ```

        **Key Features:**
        - Multi-pipeline orchestration (sentiment + classification)
        - Business rule integration (priority scoring)
        - Real-time alert system for urgent issues
        - Automated insight generation for decision-making

        ### Advanced Utilities: Production-Grade Tools

        #### Memory Tracking and Resource Management

        ```python
        @contextmanager
        def track_memory(device: str = "cuda"):
            """Context manager for GPU memory profiling."""
            if device == "cuda" and torch.cuda.is_available():
                torch.cuda.synchronize()
                start_memory = torch.cuda.memory_allocated()
                yield
                torch.cuda.synchronize()
                end_memory = torch.cuda.memory_allocated()
                print(f"Memory used: {format_size(end_memory - start_memory)}")
        ```

        **Cross-platform device detection:**
        ```python
        def get_optimal_device() -> torch.device:
            """Automatically detect best available device."""
            if torch.cuda.is_available():
                return torch.device("cuda")
            elif torch.backends.mps.is_available():  # Apple Silicon
                return torch.device("mps")
            else:
                return torch.device("cpu")
        ```

        #### Model Card Generation System

        Automated documentation for deployed models:

        ```python
        def generate_model_card(model, dataset_info, performance_metrics):
            """Generate comprehensive model documentation."""
            return {
                "model_details": {
                    "architecture": model.config.model_type,
                    "parameters": count_parameters(model),
                    "training_data": dataset_info
                },
                "performance": performance_metrics,
                "limitations": analyze_model_limitations(model),
                "ethical_considerations": generate_bias_report(model, dataset_info)
            }
        ```

        ### Configuration Management Framework

        The project includes a sophisticated configuration system:

        ```python
        class Config:
            """Centralized configuration with environment fallbacks."""
            
            # Device configuration with automatic detection
            DEVICE = get_optimal_device()
            
            # Model configurations with env overrides
            DEFAULT_SENTIMENT_MODEL = os.getenv(
                "SENTIMENT_MODEL", 
                "distilbert-base-uncased-finetuned-sst-2-english"
            )
            
            # Performance settings
            BATCH_SIZE = int(os.getenv("BATCH_SIZE", "32"))
            ENABLE_FLASH_ATTENTION = os.getenv("ENABLE_FLASH_ATTENTION", "true").lower() == "true"
            
            # Directory management with auto-creation
            DATA_PATH = Path(os.getenv("DATA_PATH", "./data"))
            DATA_PATH.mkdir(exist_ok=True)
        ```

        ### Quality Validation Pipeline for Synthetic Data

        Multi-metric validation system ensures high-quality synthetic data:

        ```python
        def validate_synthetic_data(synthetic_samples, real_samples):
            """Comprehensive quality validation pipeline."""
            
            metrics = {
                "length_similarity": calculate_length_distribution_similarity(
                    synthetic_samples, real_samples
                ),
                "vocabulary_overlap": calculate_vocabulary_overlap(
                    synthetic_samples, real_samples
                ),
                "diversity_score": calculate_diversity(synthetic_samples),
                "quality_flags": check_quality_issues(synthetic_samples)
            }
            
            # Filter based on thresholds
            filtered_samples = []
            for sample in synthetic_samples:
                if all([
                    not has_repetition(sample),
                    len(sample.split()) > MIN_LENGTH,
                    not is_truncated(sample),
                    passes_profanity_check(sample)
                ]):
                    filtered_samples.append(sample)
            
            return filtered_samples, metrics
        ```

        ### Comprehensive Benchmarking System

        The codebase includes sophisticated benchmarking utilities:

        ```python
        def benchmark_optimization_techniques(model_name: str):
            """Compare all optimization techniques systematically."""
            
            results = {}
            
            # Baseline
            baseline_model = load_model(model_name)
            results["baseline"] = benchmark_model(baseline_model)
            
            # Quantization techniques
            for technique in ["dynamic_int8", "static_int8", "int4_nf4"]:
                quantized = apply_quantization(baseline_model, technique)
                results[technique] = benchmark_model(quantized)
            
            # Batching optimization
            for batch_size in [1, 8, 32, 64]:
                results[f"batch_{batch_size}"] = benchmark_batching(
                    baseline_model, batch_size
                )
            
            # Generate comparison report
            return generate_benchmark_report(results)
        ```

        **Benchmark metrics captured:**
        - Inference latency (ms)
        - Throughput (samples/sec)
        - Memory usage (GPU/CPU)
        - Model size on disk
        - Accuracy preservation

        ## Summary and Key Takeaways

        ```mermaid
        mindmap
          root((You're Now a Workflow Architect))
            Custom Pipelines
              Preprocessing Magic
              Component Swapping
              Pipeline Composition
              Business Logic Integration
            Data Mastery
              Efficient Loading
              Streaming Scale
              Quality Annotation
              Version Control
            Optimization Arsenal
              10x Batching
              INT4 Quantization
              Edge Deployment
              PEFT Adaptation
            Synthetic Superpowers
              LLM Generation
              Diffusion Creation
              Quality Control
              Fairness Boost
            Production Ready
              Cost Reduction
              Speed Gains
              Scale Handling
              Robust Workflows
        ```

        You've transformed from pipeline user to **workflow architect**. Let's recap your new superpowers:

        ### 1. Pipeline Mastery
        ```python
        # You can now build THIS
        custom_pipeline = compose_pipelines(
            preprocessing=custom_cleaner,
            main_model=sentiment_analyzer,
            post_processing=business_filter,
            output_format=company_standard
        )
        ```

        ### 2. Data at Scale
        ```python
        # Handle millions without breaking a sweat
        massive_dataset = load_dataset("your_data", streaming=True)
        processed = massive_dataset.map(transform, batched=True)
        ```

        ### 3. Optimization Excellence
        ```python
        # 75% cost reduction, same accuracy
        optimized_model = quantize_and_compile(
            model,
            target="int4",
            hardware="mobile"
        )
        ```

        ### 4. Synthetic Data Mastery
        ```python
        # Fill gaps, boost fairness
        augmented_data = generate_synthetic(
            minority_class="rare_defects",
            count=10000,
            validate=True
        )
        ```

        **You're now equipped for the entire transformer lifecycle.** Next stop: Article 11's advanced dataset curation.

        ### Quick Reference

        | Skill | Before | After | Impact |
        |-------|--------|-------|---------|
        | Pipeline Usage | `pipeline()` only | Custom components, composition | 10x flexibility |
        | Data Handling | Memory limits | Streaming, parallel processing | 1000x scale |
        | Inference Cost | $1000/month | $250/month (INT8+batching) | 75% savings |
        | Model Size | 400MB BERT | 50MB MiniLM INT4 | Deploy anywhere |
        | Training Data | Real only | Real + validated synthetic | 2x performance |

        ### What's Next?

        - **Article 11:** Advanced dataset curation techniques
        - **Article 12:** LoRA/QLoRA for efficient large model adaptation  
        - **Article 14:** Comprehensive evaluation strategies
        - **Article 16:** Responsible AI and fairness

        **Remember:** Great AI isn't about using the fanciest models. It's about building robust, efficient workflows that solve real problems. You now have the tools. **Go build something amazing!**

        ## Summary

        This chapter transformed you from a pipeline user to a workflow architect. You learned to customize Hugging Face pipelines, handle data at massive scale with ðŸ¤— Datasets, optimize models for 75% cost reduction, and generate high-quality synthetic data. These skillsâ€”from INT4 quantization to streaming datasets to PEFT methodsâ€”form the foundation of production-ready AI systems. You're now equipped to build efficient, scalable transformer solutions that handle real-world complexity.

        ## Exercises

        ### Exercise 1: Modify a standard Hugging Face pipeline to include a custom pre-processing function (e.g., lowercasing or removing stopwords) before inference.

        **Hint:** Subclass the Pipeline class or use the 'preprocess' method to add your custom logic.

        ### Exercise 2: Load a large dataset from the Hugging Face Hub and apply a transformation using the map function. Measure the time and memory usage with and without streaming.

        **Hint:** Use load_dataset with and without streaming=True; use Python's time and memory profiling tools.

        ### Exercise 3: Quantize a transformer model using PyTorch dynamic quantization and compare its inference speed and memory footprint to the original model.

        **Hint:** Follow the quantization code example in the chapter and use timing/memory tools like timeit and torch.cuda.memory_allocated().

        ### Exercise 4: Generate synthetic text samples for a minority class in your dataset and use them to augment your training data. Evaluate the impact on model performance.

        **Hint:** Use a text-generation pipeline to create new samples, retrain your model, and compare evaluation metrics before and after augmentation.

        ### Exercise 5: Debug a pipeline that produces unexpected outputs by enabling verbose logging and tracing the flow of data through each component.

        **Hint:** Set logging to DEBUG, inspect log outputs, and check the configuration of your model, tokenizer, and pipeline arguments.
      metadata:
        extension: .md
        size_bytes: 29623
        language: markdown
    docs/art_08i.md:
      content: |-
        # Customizing Pipelines and Data Workflows: Advanced Models and Efficient Processing

        ```mermaid
        mindmap
          root((Workflow Mastery))
            Pipeline Anatomy
              Components
              Customization
              Debugging
              Registration
            Custom Workflows
              Preprocessing
              Composition
              Business Logic
              Production Scale
            Efficient Data
              Datasets Library
              Streaming
              Transformation
              Annotation
            Optimization
              Batching
              Quantization
              Deployment
              Edge Computing
            Synthetic Data
              Text Generation
              Image Creation
              Quality Control
              Fairness
        ```

        **Step-by-Step Explanation:**
        - Root node focuses on **Workflow Mastery**â€”transforming from user to architect
        - Branch covers **Pipeline Anatomy** including components, customization, debugging
        - Branch explores **Custom Workflows** with preprocessing, composition, business logic
        - Branch details **Efficient Data** handling with Datasets library and streaming
        - Branch shows **Optimization** techniques from batching to edge deployment
        - Branch presents **Synthetic Data** generation for augmentation and fairness

        ## Environment Setup

        Before diving into custom pipelines, you'll establish a robust development environment. This foundation ensures smooth workflow development and consistent results across different machines.

        ### Poetry Setup (Recommended for Projects)

        Poetry provides dependency isolation and version lockingâ€”critical for reproducible ML workflows. Here's how to set up your environment:

        ```bash
        # Install poetry if not already installed
        curl -sSL https://install.python-poetry.org | python3 -

        # Create new project
        poetry new huggingface-workflows
        cd huggingface-workflows

        # Add dependencies with 2025 versions
        poetry add "transformers>=4.53.0,<5.0.0" torch torchvision torchaudio
        poetry add "datasets>=3.6.0" "diffusers>=0.30.0" accelerate sentencepiece
        poetry add "peft>=1.0.0" pillow soundfile bitsandbytes
        poetry add --group dev jupyter ipykernel matplotlib

        # Activate environment
        poetry shell
        ```

        **Note:** Check the [Hugging Face documentation](https://huggingface.co/docs) for the absolute latest versions, as rapid updates occur (e.g., Transformers v4.53.2 released July 11, 2025).

        **Step-by-Step Explanation:**
        - Poetry installation provides isolated package management
        - Project creation establishes clean workspace structure
        - Flexible versioning (e.g., `>=4.40.0,<5.0.0`) allows minor updates while preventing breaking changes
        - Development dependencies (`--group dev`) separate production from exploration tools
        - Shell activation ensures all commands use the correct environment

        ### Mini-conda Setup (Alternative)

        For teams preferring conda's binary package management, this setup provides similar isolation:

        ```bash
        # Download and install mini-conda from https://docs.conda.io/en/latest/miniconda.html

        # Create environment with Python 3.12.10
        conda create -n huggingface-workflows python=3.12.10
        conda activate huggingface-workflows

        # Install packages
        conda install -c pytorch -c huggingface transformers torch torchvision torchaudio
        conda install -c conda-forge datasets diffusers accelerate pillow soundfile matplotlib
        pip install sentencepiece bitsandbytes
        ```

        **Step-by-Step Explanation:**
        - Conda environments provide complete Python isolation
        - Channel specification (`-c pytorch`) ensures compatible binaries
        - Mixed conda/pip installation handles packages not available in conda
        - Python 3.12.9 provides latest stable features and performance

        ### Traditional pip with pyenv

        For lightweight setups or CI/CD pipelines, pyenv with pip offers flexibility:

        ```bash
        # Install pyenv (macOS/Linux)
        curl https://pyenv.run | bash
        # Configure shell (add to ~/.bashrc or ~/.zshrc)
        export PATH="$HOME/.pyenv/bin:$PATH"
        eval "$(pyenv init -)"

        # Install Python 3.12.10 with pyenv
        pyenv install 3.12.10
        pyenv local 3.12.10

        # Create virtual environment
        python -m venv venv
        source venv/bin/activate  # On Windows: venv\Scripts\activate

        # Install packages with 2025 versions
        pip install "transformers>=4.53.0,<5.0.0" torch torchvision torchaudio
        pip install "datasets>=3.6.0" "diffusers>=0.30.0" "peft>=1.0.0" accelerate sentencepiece
        pip install pillow soundfile bitsandbytes jupyter matplotlib
        ```

        **Step-by-Step Explanation:**
        - Pyenv manages multiple Python versions without system conflicts
        - Local Python version (`.python-version` file) ensures consistency
        - Virtual environment isolation prevents package conflicts
        - Flexible versioning protects against breaking changes

        ### Handling Breaking Changes

        As of 2025, the rapid pace of ML library updates requires careful version management:

        ```bash
        # Safe update process
        poetry update --dry-run  # Preview changes
        poetry show --outdated   # Check available updates

        # Update specific packages safely
        poetry add transformers@latest --dry-run
        poetry add transformers@~4.53.0  # Allow patch updates only

        # For pip users
        pip install --upgrade transformers --dry-run
        pip install transformers==4.53.2  # Pin to specific version
        ```

        **Migration Resources:**
        - [Transformers Changelog](https://github.com/huggingface/transformers/releases) - Check for breaking changes
        - [Migration Guides](https://huggingface.co/docs/transformers/migration) - Version-specific upgrade paths
        - Use `transformers-cli env` to diagnose version conflicts

        **Pro tip:** Create a test environment before major updates to validate compatibility.

        ## Introduction: From Magic Pipelines to Master Chefâ€”Why Custom Data Workflows Matter

        Picture Hugging Face pipelines as meal kits: quick, convenient, perfect for a fast start. **Drop in. Run. Done.** But what happens when your customers have allergies? When the recipe doesn't scale to a hundred guests? When you need that secret sauce only you know how to make?

        This chapter transforms you from pipeline user to workflow architect. You'll peek inside Hugging Face pipelines, swap components, and design data workflows that handle scale, complexity, and real business needs.

        Let's explore how pipelines make things easyâ€”and where their limits begin.

        ### Quick Start: Hugging Face Pipeline (2025 Best Practice)

        The following code demonstrates modern pipeline initialization with explicit configuration for reproducibility:

        ```python
        # Modern quick-start with explicit model and device
        from transformers import pipeline

        # Specify model checkpoint and device for reproducibility
        clf = pipeline(
            'sentiment-analysis',
            model='microsoft/Phi-3-mini-4k-instruct',  # 2025 state-of-the-art
            device=0  # 0 for CUDA GPU, -1 for CPU, 'mps' for Apple Silicon
        )

        # Run prediction on text
        result = clf('I love Hugging Face!')
        print(result)
        # Output: [{'label': 'POSITIVE', 'score': 0.9998}]

        # Check model card: https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english
        ```

        **Step-by-Step Explanation:**
        - Pipeline initialization downloads and caches the model automatically
        - Explicit model specification ensures consistent results across runs
        - Device parameter controls hardware acceleration (GPU/CPU/MPS)
        - Single function call handles tokenization, inference, and result formatting
        - Model card URL provides training details and limitations

        **Magic!** This single command downloads a pre-trained model, loads the tokenizer, and formats your data. Instant resultsâ€”no deep setup required. By specifying model and device, you ensure reproducibility.

        But out-of-the-box pipelines crumble when you need:
        - Custom data cleaning (HTML, emojis, multilingual text)
        - Chained models (sentiment + entity recognition)  
        - Speed optimization (batching, device placement)
        - Business logic (filtering, compliance checks)
        - Scale (streaming, batch processing)

        Ever felt like a chef stuck with meal kits when you need to cater a wedding?

        Here's a real scenario: Your retail chain processes customer reviews from multiple platforms. Standard pipelines work for demos but fail when you need to:
        - Clean data from Twitter, Amazon, and internal systems
        - Add product metadata
        - Process 10,000 reviews per minute
        - Log for compliance
        - Stream from S3 buckets

        **Without custom workflows? Bottlenecks. Errors. Missed SLAs.**

        ### Custom Preprocessing Before Inference

        The next example shows how to add preprocessing logic while maintaining pipeline simplicity:

        ```python
        def custom_preprocess(text):
            # Normalize text for consistent predictions
            import string
            text = text.lower()
            return text.translate(str.maketrans('', '', string.punctuation))

        texts = ["Wow! Amazing product!!!", "I don't like this..."]

        # Clean then predict
        cleaned = [custom_preprocess(t) for t in texts]
        results = clf(cleaned, batch_size=16)  # Batch for speed!
        print(results)
        ```

        **Step-by-Step Explanation:**
        1. Define preprocessing (lowercase, strip punctuation)
        2. Clean inputs before pipeline processing
        3. Use `batch_size` for 5x faster inference
        4. Get reliable predictions on normalized data

        For production systems, you'll want preprocessing embedded directly into the pipeline:

        ### Advanced: Pipeline Subclassing

        This approach creates reusable, production-ready pipelines with built-in preprocessing:

        ```python
        from transformers import Pipeline

        class CustomSentimentPipeline(Pipeline):
            def preprocess(self, inputs):
                # Strip HTML, normalize text
                text = inputs.lower()
                import string
                text = text.translate(str.maketrans('', '', string.punctuation))
                return super().preprocess(text)
            
            def postprocess(self, outputs):
                # Add confidence thresholds
                results = super().postprocess(outputs)
                for r in results:
                    r['confident'] = r['score'] > 0.95
                return results
        ```

        **Step-by-Step Explanation:**
        - `preprocess` method intercepts input before tokenization
        - Text normalization ensures consistent model behavior
        - `postprocess` method enhances output with business logic
        - Confidence flag enables downstream filtering
        - Inheritance preserves all pipeline functionality

        ### Streaming Large-Scale Data

        When processing massive datasets, memory efficiency becomes critical:

        ```python
        from datasets import load_dataset

        # Stream massive datasets without memory issues
        dataset = load_dataset('csv', data_files='reviews.csv', 
                              split='train', streaming=True)

        batch_size = 32
        batch = []
        for example in dataset:
            batch.append(custom_preprocess(example['text']))
            if len(batch) == batch_size:
                results = clf(batch, batch_size=batch_size)
                # Process results (save, log, etc.)
                batch = []
        ```

        **Step-by-Step Explanation:**
        - Streaming mode loads data on-demand, not all at once
        - Batch accumulation balances memory use and processing speed
        - Results processing happens immediately, preventing memory buildup
        - Empty batch reset prevents memory leaks
        - Pattern scales to terabyte-sized datasets

        **Key Takeaways:**
        - Pipelines provide fast starts but limit production flexibility
        - Always specify model + device for reproducibility
        - Custom workflows handle real business requirements
        - Batch processing can deliver 10x throughput improvements

        Ready to peek under the hood? Let's explore pipeline anatomy.

        ## From Pipeline to Custom Components

        ```mermaid
        classDiagram
            class Pipeline {
                +model: PreTrainedModel
                +tokenizer: PreTrainedTokenizer  
                +processor: Processor
                +framework: str
                +device: torch.device
                +preprocess(inputs)
                +_forward(model_inputs)
                +postprocess(outputs)
                +__call__(inputs)
            }
            
            class Tokenizer {
                +vocab_size: int
                +model_max_length: int
                +encode(text)
                +decode(ids)
                +batch_encode_plus(texts)
            }
            
            class Model {
                +config: PretrainedConfig
                +num_parameters()
                +forward(input_ids)
                +to(device)
                +eval()
            }
            
            class Processor {
                +feature_extractor
                +tokenizer
                +__call__(inputs)
                +batch_decode(outputs)
            }
            
            Pipeline --> Tokenizer : uses
            Pipeline --> Model : uses
            Pipeline --> Processor : optional
        ```

        **Step-by-Step Explanation:**
        - `Pipeline` orchestrates the complete inference workflow
        - `Tokenizer` converts text to model-compatible token IDs
        - `Model` performs actual neural network computation
        - `Processor` handles multimodal inputs (images, audio)
        - Arrows show dependency relationships between components

        ### Pipeline Components: Under the Hood

        Think of pipelines as assembly lines. **Raw input â†’ Predictions.** Three workers make it happen:

        - **Tokenizer:** The translator. Converts "Hello world" â†’ [101, 7592, 2088, 102]
        - **Model:** The brain. Neural network processing tokens â†’ predictions
        - **Processor:** The prep cook. Resizes images, extracts audio features (multimodal tasks)

        Let's inspect these components in action:

        ```python
        from transformers import pipeline

        clf = pipeline('text-classification')
        print('Model:', clf.model)
        print('Tokenizer:', clf.tokenizer)  
        print('Processor:', getattr(clf, 'processor', None))
        print('Framework:', clf.framework)  # pytorch or tensorflow
        ```

        **Step-by-Step Explanation:**
        - Pipeline creation automatically selects compatible components
        - Model inspection reveals architecture details
        - Tokenizer check ensures vocabulary compatibility
        - Processor presence indicates multimodal capabilities
        - Framework detection helps with debugging

        **Why inspect?** When predictions look wrong, check if model and tokenizer match. Transformers now warns about mismatches!

        ### Customizing Pipelines: Modern Approach

        Real projects demand more than vanilla pipelines. As of Transformers 4.40+, customize via:

        1. **Swap components**â€”Use custom models/tokenizers
        2. **Compose pipelines**â€”Chain multiple tasks
        3. **Register new types**â€”Create reusable workflows

        Let's combine sentiment analysis + entity recognition:

        ```python
        from transformers import Pipeline, pipeline
        from transformers.pipelines import register_pipeline

        class SentimentNERPipeline(Pipeline):
            def __init__(self, sentiment_pipeline, ner_pipeline, **kwargs):
                self.sentiment_pipeline = sentiment_pipeline
                self.ner_pipeline = ner_pipeline
                super().__init__(
                    model=sentiment_pipeline.model,
                    tokenizer=sentiment_pipeline.tokenizer,
                    **kwargs
                )
            
            def _forward(self, inputs):
                sentiment = self.sentiment_pipeline(inputs)
                entities = self.ner_pipeline(inputs)
                return {"sentiment": sentiment, "entities": entities}

        # Register for reuse
        register_pipeline(
            task="sentiment-ner",
            pipeline_class=SentimentNERPipeline,
            pt_model=True
        )

        # Use it!
        pipe = pipeline("sentiment-ner")
        result = pipe("Apple Inc. makes amazing products!")
        # {'sentiment': [{'label': 'POSITIVE', 'score': 0.99}],
        #  'entities': [{'word': 'Apple Inc.', 'entity': 'ORG'}]}
        ```

        **Step-by-Step Explanation:**
        - Custom pipeline class combines two existing pipelines
        - `_forward` method orchestrates both models
        - Registration makes pipeline reusable across projects
        - Single call returns combined analysis
        - Pattern extends to any pipeline combination

        **Pro tip:** Composition beats inheritance. Build complex workflows from simple parts.

        ### Debugging Pipelines

        When things break (they will), make errors visible:

        ```python
        from transformers.utils import logging
        logging.set_verbosity_debug()

        # Now see EVERYTHING
        clf = pipeline('text-classification')
        result = clf('Debug me!')
        ```

        **Step-by-Step Explanation:**
        - Debug logging exposes tokenization steps
        - Model loading progress becomes visible
        - Token-to-ID mapping appears in logs
        - Inference timing helps identify bottlenecks

        **Common issues:**
        - Model/tokenizer mismatch â†’ Check families match
        - Wrong input format â†’ Pipelines expect strings, lists, or dicts
        - Memory errors â†’ Reduce batch size or max_length
        - Slow inference â†’ Enable Flash Attention (GPU) or batch more

        **Next:** Let's handle data at scale with ðŸ¤— Datasets.

        ## Efficient Data Handling with ðŸ¤— Datasets

        ```mermaid
        flowchart LR
            A[Raw Data Sources] --> B{Load Dataset}
            B -->|Small Data| C[In-Memory Dataset]
            B -->|Large Data| D[Streaming Dataset]
            
            C --> E[Transform with map]
            D --> F[Stream + Transform]
            
            E --> G[Filter Examples]
            F --> G
            
            G --> H[Batch Processing]
            H --> I[Model Inference]
            
            J[Version Control] -.->|lakeFS| C
            J -.->|Track Changes| E
            
            K[Annotation Tools] -->|Argilla| C
            K -->|Quality Labels| G
        ```

        **Step-by-Step Explanation:**
        - Raw data flows through loading decision based on size
        - Small datasets load entirely into memory for speed
        - Large datasets stream to avoid memory limits
        - Transformations apply via `map` operations
        - Filtering removes unwanted examples
        - Batch processing optimizes model inference
        - Version control and annotation integrate seamlessly

        Ever tried loading Wikipedia into pandas? **Memory explosion!** The ðŸ¤— Datasets library handles millions of examples without breaking a sweat.

        ### Loading and Transforming Data

        The Datasets library provides efficient data handling with automatic caching and memory mapping:

        ```python
        from datasets import load_dataset

        # Load IMDB reviews
        dataset = load_dataset('imdb', split='train')
        print(f"Dataset size: {len(dataset)}")  # 25,000 examples
        print(dataset[0])  # {'text': '...', 'label': 1}

        # Custom data? Easy!
        custom = load_dataset('csv', data_files='reviews.csv')
        ```

        **Step-by-Step Explanation:**
        - `load_dataset` downloads and caches data automatically
        - Split specification loads only needed portions
        - First access downloads; subsequent uses hit cache
        - CSV loading works identically to hub datasets
        - Memory mapping prevents RAM overflow

        Transform data efficiently with parallel processing:

        ```python
        def preprocess(batch):
            # Process entire batches at once
            batch['text'] = [text.lower() for text in batch['text']]
            batch['length'] = [len(text.split()) for text in batch['text']]
            return batch

        # Transform with parallel processing
        dataset = dataset.map(preprocess, batched=True, num_proc=4)

        # Filter short reviews
        dataset = dataset.filter(lambda x: x['length'] > 20)
        ```

        **Step-by-Step Explanation:**
        - Batch processing operates on multiple examples simultaneously
        - List comprehensions process entire columns efficiently
        - Parallel processing (`num_proc=4`) uses multiple CPU cores
        - Filtering creates view without copying data
        - Chaining operations maintains memory efficiency

        **Performance boost:** `batched=True` processes 100x faster than one-by-one!

        ### Streaming Massive Datasets

        What about Wikipedia-scale data? **Stream it!**

        ```python
        # Stream without loading everything
        wiki = load_dataset('wikipedia', '20220301.en', 
                           split='train', streaming=True)

        # Process as you go
        for i, article in enumerate(wiki):
            if i >= 1000:  # Process first 1000
                break
            # Your processing here
            process_article(article['text'])
        ```

        **Step-by-Step Explanation:**
        - `streaming=True` enables lazy loading
        - Data loads only when accessed
        - Enumeration provides progress tracking
        - Early break prevents infinite processing
        - Memory stays constant regardless of dataset size

        **Memory usage:** 200MB instead of 100GB. **Magic? No. Smart engineering.**

        ### Modern Annotation Workflow

        Great models need great labels. Here's a production-ready annotation pipeline:

        ```python
        # Best practices for annotation
        from datasets import Dataset

        # 1. Start small - annotate 100 examples
        pilot_data = dataset.select(range(100))

        # 2. Use Argilla for team annotation
        # See Article 12 for Argilla + HF integration

        # 3. Version your annotations
        # dataset.push_to_hub("company/product-reviews-v2")

        # 4. Track changes with lakeFS for compliance
        ```

        **Step-by-Step Explanation:**
        - Pilot annotation validates labeling guidelines
        - Small batches prevent wasted effort
        - Team tools ensure consistency
        - Version control enables reproducibility
        - Change tracking satisfies compliance requirements

        **Remember:** Bad labels create bad models. Invest in quality annotation.

        ## Optimized Inference and Cost Management

        ```mermaid
        flowchart TD
            A[Original Model] --> B{Optimization Technique}
            
            B -->|Quantization| C[INT8/INT4 Model]
            B -->|Pruning| D[Sparse Model]
            B -->|Compilation| E[Optimized Model]
            
            C --> F[Mobile/Edge]
            C --> G[CPU Deployment]
            D --> H[Cloud API]
            E --> I[GPU Server]
            
            J[Batching] --> K[5-10x Throughput]
            L[Flash Attention] --> M[2x GPU Speed]
            
            style C fill:#90EE90
            style K fill:#FFB6C1
            style M fill:#87CEEB
        ```

        **Step-by-Step Explanation:**
        - Original model branches into three optimization paths
        - Quantization reduces precision for smaller models
        - Pruning removes unnecessary parameters
        - Compilation optimizes for specific hardware
        - Each technique targets different deployment scenarios
        - Performance gains stack when combined

        Deploying transformers resembles running a busy restaurant kitchen. **Speed matters. Costs matter more.**

        ### Batching for 10x Throughput

        Single-item processing wastes computational resources. Batching transforms performance:

        ```python
        # Slow: One by one
        texts = ["Review 1", "Review 2", "Review 3"]
        for text in texts:
            result = clf(text)  # 3 separate calls

        # Fast: Batch processing
        results = clf(texts, 
                     padding=True,      # Align lengths
                     truncation=True,   # Cap at max_length
                     max_length=128)    # Prevent memory spikes
        # 10x faster on GPU!
        ```

        **Step-by-Step Explanation:**
        - Individual calls waste GPU parallelism
        - Batching fills GPU compute units
        - Padding ensures uniform tensor shapes
        - Truncation prevents memory overflow
        - Max length balances speed and accuracy

        **Real numbers:** Single inference: 50ms. Batch of 32: 200ms. **That's 8x speedup!**

        ### Modern Quantization: Slash Costs Dramatically

        Quantization reduces model precision while maintaining accuracy:

        ```python
        from transformers import AutoModelForSequenceClassification

        # Standard model: 400MB
        model = AutoModelForSequenceClassification.from_pretrained(
            "bert-base-uncased"
        )

        # Quantized model: 100MB, 4x faster!
        model_int8 = AutoModelForSequenceClassification.from_pretrained(
            "bert-base-uncased",
            load_in_8bit=True,
            device_map="auto"
        )

        # For LLMs: INT4 quantization
        model_int4 = AutoModelForCausalLM.from_pretrained(
            "meta-llama/Llama-2-7b-hf",
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16
        )
        ```

        **Step-by-Step Explanation:**
        - INT8 quantization reduces memory by 75%
        - `device_map="auto"` optimally distributes layers
        - INT4 enables 7B parameter models on consumer GPUs
        - Compute dtype maintains accuracy during forward pass
        - Automatic mixed precision balances speed and quality

        **Cost impact:** AWS inference costs drop 75% with INT8. **Same accuracy. Quarter the price.**

        ### Edge Deployment Strategy

        Deploy models where compute happensâ€”at the edge:

        ```python
        # 1. Choose efficient model
        model_name = "microsoft/MiniLM-L6-H256-uncased"  # 6x smaller than BERT

        # 2. Quantize for edge
        import torch
        quantized = torch.quantization.quantize_dynamic(
            model, {torch.nn.Linear}, dtype=torch.qint8
        )

        # 3. Export to ONNX/GGUF
        model.save_pretrained("model_mobile", push_to_hub=False)

        # 4. Benchmark on target device
        # iPhone 14: 15ms/inference
        # Raspberry Pi: 100ms/inference
        ```

        **Step-by-Step Explanation:**
        - Model selection prioritizes size over absolute accuracy
        - Dynamic quantization adapts to input ranges
        - Export formats enable cross-platform deployment
        - Device-specific benchmarking ensures performance
        - Edge inference eliminates network latency

        **Real example:** Retail chain deploys MiniLM on 10,000 handheld scanners. Instant product search. No cloud costs.

        ### Advanced: PEFT for Large Models

        Parameter-Efficient Fine-Tuning makes large models accessible:

        ```python
        from peft import LoraConfig, get_peft_model, TaskType

        # Adapt Llama-2 with 0.1% of parameters
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=16,  # LoRA rank
            lora_alpha=32,
            lora_dropout=0.1,
            target_modules=["q_proj", "v_proj"]
        )

        model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-hf")
        peft_model = get_peft_model(model, peft_config)

        # Only 40MB of trainable parameters instead of 13GB!
        peft_model.print_trainable_parameters()
        # trainable params: 4,194,304 || all params: 6,738,415,616 || trainable%: 0.06%
        ```

        **Step-by-Step Explanation:**
        - LoRA adds small trainable matrices to frozen model
        - Rank (`r=16`) controls capacity vs efficiency tradeoff
        - Target modules focus updates on attention layers
        - Dropout prevents overfitting on small datasets
        - 0.06% trainable parameters enable consumer GPU training

        **Impact:** Fine-tune Llama-2 on a single GPU. Deploy updates as small adapters. **Efficiency unlocked.**

        ### Advanced Fine-Tuning with QLoRA and Liger Kernels

        QLoRA (Quantized LoRA) pushes efficiency even further, enabling large model fine-tuning on consumer hardware:

        ```python
        from peft import LoraConfig, get_peft_model, TaskType
        from transformers import AutoModelForCausalLM, BitsAndBytesConfig
        import torch

        # QLoRA configuration for 4-bit quantization
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )

        # Load Llama-3 with 4-bit quantization
        model = AutoModelForCausalLM.from_pretrained(
            "meta-llama/Llama-3-8b-hf",  # Updated for 2025
            quantization_config=quantization_config,
            device_map="auto",
        )

        # Configure LoRA for quantized model
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=16,
            lora_alpha=32,
            lora_dropout=0.1,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
            bias="none",
        )

        # Create PEFT model
        peft_model = get_peft_model(model, peft_config)
        peft_model.print_trainable_parameters()
        ```

        **Step-by-Step Explanation:**
        - 4-bit quantization reduces memory by 75% vs standard LoRA
        - NF4 (NormalFloat4) maintains accuracy better than INT4
        - Double quantization further compresses quantization constants
        - Target all attention projections for comprehensive adaptation
        - Compatible with Flash Attention 2 for speed

        **Comparison: LoRA vs QLoRA**

        | Method | Memory Usage | Trainable Params | Speed Impact | Min GPU VRAM |
        |--------|--------------|------------------|--------------|--------------|
        | LoRA   | High (13GB)  | 0.06%           | Moderate     | 24GB         |
        | QLoRA  | Low (4GB)    | 0.06%           | High (2x)    | 8GB          |

        **Real-world impact:** In 2025, a startup fine-tuned Llama-3-70B on a single RTX 4090 using QLoRA, achieving 95% of full fine-tuning performance at 5% of the cost.

        **Pro tip:** Use `device_map="auto"` to automatically distribute layers across available GPUs for multi-GPU setups.

        ## Synthetic Data Generation

        ```mermaid
        flowchart LR
            A[Analyze Dataset] --> B{Data Issues?}
            
            B -->|Class Imbalance| C[Generate Minority Examples]
            B -->|Rare Events| D[Simulate Edge Cases]
            B -->|Privacy Concerns| E[Create Safe Data]
            
            C --> F[LLM Text Generation]
            D --> G[Diffusion Images]
            E --> H[Structured Data GANs]
            
            F --> I[Quality Filters]
            G --> I
            H --> I
            
            I --> J[Validation]
            J --> K[Augmented Dataset]
            
            style F fill:#FFE4B5
            style G fill:#E6E6FA
            style H fill:#F0E68C
        ```

        **Step-by-Step Explanation:**
        - Dataset analysis identifies gaps and issues
        - Different problems require different generation approaches
        - LLMs handle text, diffusion models create images
        - Quality filters ensure synthetic data improves training
        - Validation confirms synthetic data matches real distribution

        Ever wished you had more training data? **Synthetic data is your genie.**

        ### Text Generation with Modern LLMs

        Generate high-quality synthetic text using state-of-the-art models:

        ```python
        from transformers import pipeline

        # Latest open LLM
        gen = pipeline(
            'text-generation',
            model='mistralai/Mistral-7B-Instruct-v0.2',
            device_map='auto'
        )

        # Generate product reviews
        prompt = """Generate a realistic negative product review for headphones.
        Include specific details about sound quality and comfort."""

        reviews = gen(
            prompt,
            max_new_tokens=100,
            num_return_sequences=5,
            temperature=0.8  # More variety
        )

        # Quality check
        for review in reviews:
            if is_realistic(review['generated_text']):
                dataset.add_item(review)
        ```

        **Step-by-Step Explanation:**
        - Instruction-tuned models follow prompts precisely
        - Specific details in prompts improve generation quality
        - Temperature controls creativity vs consistency
        - Multiple sequences provide variety
        - Quality checking prevents model collapse

        **Pro tip:** Always validate synthetic data. Bad synthetic data creates bad models.

        ### Image Generation with SDXL

        Create training images for visual tasks:

        ```python
        from diffusers import DiffusionPipeline
        import torch

        # Load latest Stable Diffusion
        pipe = DiffusionPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            torch_dtype=torch.float16,
            variant="fp16"
        )
        pipe = pipe.to("cuda")

        # Generate training images
        prompts = [
            "smartphone with cracked screen, product photo",
            "laptop with coffee spill damage, repair documentation",
            "pristine condition vintage watch, auction listing"
        ]

        for prompt in prompts:
            image = pipe(prompt, num_inference_steps=30).images[0]
            # Add to training set with appropriate labels
        ```

        **Step-by-Step Explanation:**
        - SDXL provides photorealistic generation
        - FP16 variant reduces memory usage
        - Specific prompts create targeted training data
        - Inference steps balance quality and speed
        - Generated images augment rare classes

        ### Synthetic Data Validation

        Quality control ensures synthetic data improves your model:

        ```python
        def validate_synthetic_data(synthetic, real):
            """Ensure synthetic data improves dataset"""
            
            # 1. Statistical similarity
            real_stats = calculate_statistics(real)
            synth_stats = calculate_statistics(synthetic)
            assert similarity(real_stats, synth_stats) > 0.85
            
            # 2. Diversity check
            assert len(set(synthetic)) / len(synthetic) > 0.95
            
            # 3. Quality filters
            synthetic = filter_nsfw(synthetic)
            synthetic = filter_toxic(synthetic)
            
            # 4. Human review sample
            sample = random.sample(synthetic, 100)
            # Send sample for manual QA
            
            return synthetic
        ```

        **Step-by-Step Explanation:**
        - Statistical validation ensures distribution match
        - Diversity check prevents repetitive generation
        - Content filters remove inappropriate material
        - Human review catches subtle quality issues
        - Multi-stage validation prevents model degradation

        **Remember:** Synthetic data augments, not replaces, real data.

        ## Ethical AI and Fairness in Workflows

        Building production AI systems requires proactive fairness consideration. In 2025, responsible AI isn't optionalâ€”it's essential for sustainable deployment.

        ### Bias Detection in Pipeline Outputs

        Monitor and measure bias systematically:

        ```python
        from transformers import pipeline
        import pandas as pd

        class FairnessPipeline:
            """Wrapper that adds fairness monitoring to any pipeline."""
            
            def __init__(self, base_pipeline):
                self.pipeline = base_pipeline
                self.bias_keywords = self._load_bias_keywords()
                
            def __call__(self, texts, **kwargs):
                # Get predictions
                results = self.pipeline(texts, **kwargs)
                
                # Analyze for bias
                bias_analysis = self._analyze_bias(texts, results)
                
                # Log if bias detected
                if bias_analysis['bias_score'] > 0.15:
                    self._log_bias_incident(bias_analysis)
                    
                return results, bias_analysis
            
            def _analyze_bias(self, texts, results):
                """Detect potential bias in predictions."""
                bias_metrics = {
                    'demographic_parity': self._check_demographic_parity(results),
                    'equalised_odds': self._check_equalised_odds(texts, results),
                    'keyword_bias': self._check_keyword_bias(texts, results)
                }
                
                bias_score = sum(bias_metrics.values()) / len(bias_metrics)
                return {
                    'bias_score': bias_score,
                    'metrics': bias_metrics,
                    'flagged_samples': self._get_flagged_samples(texts, results)
                }

        # Usage
        sentiment = pipeline('sentiment-analysis')
        fair_sentiment = FairnessPipeline(sentiment)

        results, bias_report = fair_sentiment([
            "The doctor was excellent at her job",
            "The nurse helped him with the medication",
            "They are a talented engineer"
        ])

        print(f"Bias score: {bias_report['bias_score']:.3f}")
        ```

        **Step-by-Step Explanation:**
        - Wrapper pattern adds fairness monitoring to any pipeline
        - Bias keywords help detect problematic patterns
        - Multiple metrics capture different fairness dimensions
        - Automatic logging enables bias tracking over time
        - Non-intrusive design maintains pipeline compatibility

        ### Fairness-Aware Data Augmentation

        Generate synthetic data that improves model fairness:

        ```python
        def generate_fairness_augmented_data(dataset, protected_attributes):
            """Generate synthetic data to improve fairness."""
            
            # Analyze representation gaps
            representation = analyze_representation(dataset, protected_attributes)
            
            # Generate diverse examples
            augmented_samples = []
            for underrep_group in representation['underrepresented']:
                prompts = create_diverse_prompts(underrep_group)
                
                for prompt in prompts:
                    synthetic = generate_synthetic_text(
                        prompt,
                        num_samples=100,
                        diversity_penalty=0.8  # Encourage variety
                    )
                    
                    # Quality and bias check
                    filtered = filter_biased_samples(synthetic)
                    augmented_samples.extend(filtered)
            
            # Validate fairness improvement
            combined_dataset = dataset + augmented_samples
            fairness_metrics = evaluate_fairness(combined_dataset)
            
            return augmented_samples, fairness_metrics
        ```

        **Step-by-Step Explanation:**
        - Representation analysis identifies data gaps
        - Targeted generation fills underrepresented groups
        - Diversity penalty prevents stereotypical patterns
        - Bias filtering removes problematic generations
        - Fairness validation ensures actual improvement

        ### Continuous Fairness Monitoring

        Deploy with ongoing fairness tracking:

        ```python
        class FairnessMonitor:
            """Production fairness monitoring system."""
            
            def __init__(self, model_name):
                self.model_name = model_name
                self.metrics_history = []
                
            def log_prediction(self, input_text, prediction, metadata=None):
                """Log individual predictions for fairness analysis."""
                self.metrics_history.append({
                    'timestamp': datetime.now(),
                    'input': input_text,
                    'prediction': prediction,
                    'metadata': metadata or {}
                })
                
                # Periodic fairness check
                if len(self.metrics_history) % 1000 == 0:
                    self._run_fairness_audit()
            
            def _run_fairness_audit(self):
                """Analyze recent predictions for bias patterns."""
                recent = self.metrics_history[-1000:]
                
                # Group by protected attributes
                fairness_report = {
                    'overall_bias': self._calculate_overall_bias(recent),
                    'demographic_parity': self._check_demographic_parity(recent),
                    'false_positive_equity': self._check_fp_equity(recent)
                }
                
                # Alert if thresholds exceeded
                if fairness_report['overall_bias'] > 0.2:
                    self._send_fairness_alert(fairness_report)
                
                return fairness_report
        ```

        **Step-by-Step Explanation:**
        - Continuous logging captures all predictions
        - Periodic audits detect bias drift
        - Multiple fairness metrics provide comprehensive view
        - Automated alerts enable rapid response
        - Historical tracking shows fairness trends

        **Key Principles:**
        1. **Measure continuously** - Bias can emerge over time
        2. **Use multiple metrics** - No single metric captures all fairness aspects
        3. **Act on findings** - Detection without action is insufficient
        4. **Document decisions** - Fairness tradeoffs should be explicit
        5. **Involve stakeholders** - Technical metrics need human context

        ## Production Workflows in Practice

        This section demonstrates how to combine all techniques into production-ready systems that handle real-world complexity.

        ### RetailReviewWorkflow: Complete End-to-End Example

        The codebase includes a sophisticated production workflow that demonstrates real-world integration:

        ```python
        class RetailReviewWorkflow:
            """End-to-end production workflow for retail review analysis."""
            
            def __init__(self):
                # Multi-pipeline architecture
                self.sentiment_pipeline = pipeline('sentiment-analysis')
                self.category_pipeline = pipeline('zero-shot-classification',
                    candidate_labels=['product_quality', 'shipping', 'customer_service', 'pricing'])
                
            def process_review_batch(self, reviews: List[Dict]) -> Dict[str, Any]:
                # Business logic integration
                for review in reviews:
                    # Sentiment analysis
                    sentiment = self.sentiment_pipeline(review['text'])
                    
                    # Category classification  
                    categories = self.category_pipeline(review['text'])
                    
                    # Priority scoring based on keywords
                    priority_score = self._calculate_priority(review['text'])
                    
                    # Generate insights
                    if priority_score > 0.8:
                        self._trigger_alert(review)
                
                return self._generate_business_insights(processed_reviews)
        ```

        **Step-by-Step Explanation:**
        - Multi-pipeline architecture handles complex analysis
        - Business logic integrates with ML predictions
        - Priority scoring identifies urgent issues
        - Alert system enables real-time response
        - Insight generation provides actionable intelligence

        **Key Features:**
        - Multi-pipeline orchestration (sentiment + classification)
        - Business rule integration (priority scoring)
        - Real-time alert system for urgent issues
        - Automated insight generation for decision-making

        ### Advanced Utilities: Production-Grade Tools

        Production systems need robust monitoring and debugging capabilities.

        #### Memory Tracking and Resource Management

        Monitor GPU memory usage during inference:

        ```python
        @contextmanager
        def track_memory(device: str = "cuda"):
            """Context manager for GPU memory profiling."""
            if device == "cuda" and torch.cuda.is_available():
                torch.cuda.synchronize()
                start_memory = torch.cuda.memory_allocated()
                yield
                torch.cuda.synchronize()
                end_memory = torch.cuda.memory_allocated()
                print(f"Memory used: {format_size(end_memory - start_memory)}")
        ```

        **Step-by-Step Explanation:**
        - Context manager ensures proper cleanup
        - CUDA synchronization captures accurate measurements
        - Memory difference shows actual usage
        - Automatic formatting improves readability

        **Cross-platform device detection:**
        ```python
        def get_optimal_device() -> torch.device:
            """Automatically detect best available device."""
            if torch.cuda.is_available():
                return torch.device("cuda")
            elif torch.backends.mps.is_available():  # Apple Silicon
                return torch.device("mps")
            else:
                return torch.device("cpu")
        ```

        **Step-by-Step Explanation:**
        - CUDA check identifies NVIDIA GPUs
        - MPS check finds Apple Silicon acceleration
        - CPU fallback ensures universal compatibility
        - Single function handles all platforms

        #### Model Card Generation System

        Automated documentation for deployed models ensures transparency:

        ```python
        def generate_model_card(model, dataset_info, performance_metrics):
            """Generate comprehensive model documentation."""
            return {
                "model_details": {
                    "architecture": model.config.model_type,
                    "parameters": count_parameters(model),
                    "training_data": dataset_info
                },
                "performance": performance_metrics,
                "limitations": analyze_model_limitations(model),
                "ethical_considerations": generate_bias_report(model, dataset_info)
            }
        ```

        **Step-by-Step Explanation:**
        - Architecture details enable reproduction
        - Parameter count indicates computational requirements
        - Performance metrics set expectations
        - Limitations section prevents misuse
        - Ethical considerations ensure responsible deployment

        ### Configuration Management Framework

        The project includes a sophisticated configuration system that adapts to different environments:

        ```python
        class Config:
            """Centralized configuration with environment fallbacks."""
            
            # Device configuration with automatic detection
            DEVICE = get_optimal_device()
            
            # Model configurations with env overrides
            DEFAULT_SENTIMENT_MODEL = os.getenv(
                "SENTIMENT_MODEL", 
                "distilbert-base-uncased-finetuned-sst-2-english"
            )
            
            # Performance settings
            BATCH_SIZE = int(os.getenv("BATCH_SIZE", "32"))
            ENABLE_FLASH_ATTENTION = os.getenv("ENABLE_FLASH_ATTENTION", "true").lower() == "true"
            
            # Directory management with auto-creation
            DATA_PATH = Path(os.getenv("DATA_PATH", "./data"))
            DATA_PATH.mkdir(exist_ok=True)
        ```

        **Step-by-Step Explanation:**
        - Environment variables enable deployment flexibility
        - Fallback values ensure sensible defaults
        - Type conversion prevents configuration errors
        - Automatic directory creation prevents runtime failures
        - Single source of truth simplifies maintenance

        ### Quality Validation Pipeline for Synthetic Data

        Multi-metric validation ensures high-quality synthetic data:

        ```python
        def validate_synthetic_data(synthetic_samples, real_samples):
            """Comprehensive quality validation pipeline."""
            
            metrics = {
                "length_similarity": calculate_length_distribution_similarity(
                    synthetic_samples, real_samples
                ),
                "vocabulary_overlap": calculate_vocabulary_overlap(
                    synthetic_samples, real_samples
                ),
                "diversity_score": calculate_diversity(synthetic_samples),
                "quality_flags": check_quality_issues(synthetic_samples)
            }
            
            # Filter based on thresholds
            filtered_samples = []
            for sample in synthetic_samples:
                if all([
                    not has_repetition(sample),
                    len(sample.split()) > MIN_LENGTH,
                    not is_truncated(sample),
                    passes_profanity_check(sample)
                ]):
                    filtered_samples.append(sample)
            
            return filtered_samples, metrics
        ```

        **Step-by-Step Explanation:**
        - Length similarity ensures realistic text generation
        - Vocabulary overlap confirms domain relevance
        - Diversity score prevents repetitive patterns
        - Quality checks filter problematic samples
        - Multi-stage filtering maintains high standards

        ### Comprehensive Benchmarking System

        The codebase includes sophisticated benchmarking utilities that compare optimization techniques:

        ```python
        def benchmark_optimization_techniques(model_name: str):
            """Compare all optimization techniques systematically."""
            
            results = {}
            
            # Baseline
            baseline_model = load_model(model_name)
            results["baseline"] = benchmark_model(baseline_model)
            
            # Quantization techniques
            for technique in ["dynamic_int8", "static_int8", "int4_nf4"]:
                quantized = apply_quantization(baseline_model, technique)
                results[technique] = benchmark_model(quantized)
            
            # Batching optimization
            for batch_size in [1, 8, 32, 64]:
                results[f"batch_{batch_size}"] = benchmark_batching(
                    baseline_model, batch_size
                )
            
            # Generate comparison report
            return generate_benchmark_report(results)
        ```

        **Step-by-Step Explanation:**
        - Baseline measurement provides comparison point
        - Multiple quantization techniques reveal tradeoffs
        - Batch size sweep finds optimal throughput
        - Automated reporting simplifies decision-making
        - Systematic approach ensures fair comparison

        **Benchmark metrics captured:**
        - Inference latency (ms)
        - Throughput (samples/sec)
        - Memory usage (GPU/CPU)
        - Model size on disk
        - Accuracy preservation

        ## Summary and Key Takeaways

        ```mermaid
        mindmap
          root((You're Now a Workflow Architect))
            Custom Pipelines
              Preprocessing Magic
              Component Swapping
              Pipeline Composition
              Business Logic Integration
            Data Mastery
              Efficient Loading
              Streaming Scale
              Quality Annotation
              Version Control
            Optimization Arsenal
              10x Batching
              INT4 Quantization
              Edge Deployment
              PEFT Adaptation
            Synthetic Superpowers
              LLM Generation
              Diffusion Creation
              Quality Control
              Fairness Boost
            Production Ready
              Cost Reduction
              Speed Gains
              Scale Handling
              Robust Workflows
        ```

        **Step-by-Step Explanation:**
        - Root celebrates your transformation to workflow architect
        - Custom pipelines branch shows flexibility gained
        - Data mastery indicates scale capabilities
        - Optimization arsenal lists cost-saving techniques
        - Synthetic superpowers expand data possibilities
        - Production ready confirms real-world applicability

        You've transformed from pipeline user to **workflow architect**. Let's recap your new superpowers:

        ### 1. Pipeline Mastery
        ```python
        # You can now build THIS
        custom_pipeline = compose_pipelines(
            preprocessing=custom_cleaner,
            main_model=sentiment_analyzer,
            post_processing=business_filter,
            output_format=company_standard
        )
        ```

        ### 2. Data at Scale
        ```python
        # Handle millions without breaking a sweat
        massive_dataset = load_dataset("your_data", streaming=True)
        processed = massive_dataset.map(transform, batched=True)
        ```

        ### 3. Optimization Excellence
        ```python
        # 75% cost reduction, same accuracy
        optimized_model = quantize_and_compile(
            model,
            target="int4",
            hardware="mobile"
        )
        ```

        ### 4. Synthetic Data Mastery
        ```python
        # Fill gaps, boost fairness
        augmented_data = generate_synthetic(
            minority_class="rare_defects",
            count=10000,
            validate=True
        )
        ```

        **You're now equipped for the entire transformer lifecycle.** Next stop: Article 11's advanced dataset curation.

        ### Quick Reference

        | Skill | Before | After | Impact |
        |-------|--------|-------|---------|
        | Pipeline Usage | `pipeline()` only | Custom components, composition | 10x flexibility |
        | Data Handling | Memory limits | Streaming, parallel processing | 1000x scale |
        | Inference Cost | $1000/month | $250/month (INT8+batching) | 75% savings |
        | Model Size | 400MB BERT | 50MB MiniLM INT4 | Deploy anywhere |
        | Training Data | Real only | Real + validated synthetic | 2x performance |

        ### What's Next?

        - **Article 11:** Advanced dataset curation techniques
        - **Article 12:** LoRA/QLoRA for efficient large model adaptation  
        - **Article 14:** Comprehensive evaluation strategies
        - **Article 16:** Responsible AI and fairness

        **Remember:** Great AI isn't about using the fanciest models. It's about building robust, efficient workflows that solve real problems. You now have the tools. **Go build something amazing!**

        ## Summary

        This chapter transformed you from a pipeline user to a workflow architect. You learned to customize Hugging Face pipelines, handle data at massive scale with ðŸ¤— Datasets, optimize models for 75% cost reduction, and generate high-quality synthetic data. These skillsâ€”from INT4 quantization to streaming datasets to PEFT methodsâ€”form the foundation of production-ready AI systems. You're now equipped to build efficient, scalable transformer solutions that handle real-world complexity.

        ## Exercises

        ### Exercise 1: Modify a standard Hugging Face pipeline to include a custom pre-processing function (e.g., lowercasing or removing stopwords) before inference.

        **Hint:** Subclass the Pipeline class or use the 'preprocess' method to add your custom logic.

        ### Exercise 2: Load a large dataset from the Hugging Face Hub and apply a transformation using the map function. Measure the time and memory usage with and without streaming.

        **Hint:** Use load_dataset with and without streaming=True; use Python's time and memory profiling tools.

        ### Exercise 3: Quantize a transformer model using PyTorch dynamic quantization and compare its inference speed and memory footprint to the original model.

        **Hint:** Follow the quantization code example in the chapter and use timing/memory tools like timeit and torch.cuda.memory_allocated().

        ### Exercise 4: Generate synthetic text samples for a minority class in your dataset and use them to augment your training data. Evaluate the impact on model performance.

        **Hint:** Use a text-generation pipeline to create new samples, retrain your model, and compare evaluation metrics before and after augmentation.

        ### Exercise 5: Debug a pipeline that produces unexpected outputs by enabling verbose logging and tracing the flow of data through each component.

        **Hint:** Set logging to DEBUG, inspect log outputs, and check the configuration of your model, tokenizer, and pipeline arguments.
      metadata:
        extension: .md
        size_bytes: 49298
        language: markdown
    .yamlproject/config.yaml:
      content: |
        # Default configuration for YAML Project
        include_pattern: null
        exclude_pattern: null
        temp_dir: null
        backup_dir: null
        supported_extensions:
          .py: python
          .sh: bash
          .java: java
          .js: javascript
          .jsx: javascript
          .ts: typescript
          .tsx: typescript
          .html: html
          .css: css
          .md: markdown
          .yml: yaml
          .yaml: yaml
          .json: json
          .txt: text
          .go: go
          .rs: rust
          .rb: ruby
          .php: php
          .c: c
          .cpp: cpp
          .h: c
          .hpp: cpp
          .cs: csharp
          .toml: toml
          .xml: xml
          .sql: sql
          .kt: kotlin
          .swift: swift
          .dart: dart
          .r: r
          .scala: scala
          .pl: perl
          .lua: lua
          .ini: ini
          .cfg: ini
          .properties: properties
          .ipynb: ipynb
        forbidden_dirs:
          - __pycache__
          - node_modules
          - dist
          - cdk.out
          - env
          - venv
          - .venv
          - .idea
          - build
          - .git
          - .svn
          - .hg
          - .DS_Store
          - .vs
          - .vscode
          - target
          - bin
          - obj
          - out
          - Debug
          - Release
          - tmp
          - .tox
          - .pytest_cache
          - __MACOSX
          - .mypy_cache
          - tests
        outfile: project.yaml
        log_level: INFO
        max_file_size: 204800  # 200KB
        metadata_fields:
          - extension
          - size_bytes
          - language
        yaml_format:
          indent: 2
          width: 120
      metadata:
        extension: .yaml
        size_bytes: 1119
        language: yaml
    examples/retail_workflow.py:
      content: |
        """Real-world retail workflow example."""

        import sys
        from pathlib import Path
        sys.path.append(str(Path(__file__).parent.parent))

        from src.production_workflows import RetailReviewWorkflow
        import pandas as pd
        import matplotlib.pyplot as plt
        from datetime import datetime
        import json

        def analyze_retail_reviews():
            """Complete retail review analysis workflow."""
            
            print("=== Retail Review Analysis System ===\n")
            
            # Initialize workflow
            workflow = RetailReviewWorkflow()
            
            # Simulate loading reviews from different sources
            reviews_sources = {
                'website': [
                    "Excellent product quality! Fast shipping and great packaging.",
                    "The item broke after just one week. Very disappointed.",
                    "Good value for money, but customer service could be better.",
                    "URGENT: Wrong item delivered! Need immediate assistance!",
                    "Perfect! Exactly what I was looking for."
                ],
                'mobile_app': [
                    "App crashed during checkout. Lost my cart!",
                    "Love the new features in the app update.",
                    "Shipping took too long, but product is good.",
                    "5 stars! Great experience from start to finish.",
                    "Product damaged in shipping. Need refund ASAP!"
                ],
                'email': [
                    "Thank you for the quick resolution to my issue.",
                    "Still waiting for my refund after 2 weeks...",
                    "The product quality has really declined lately.",
                    "Best online shopping experience ever!",
                    "Package never arrived. Tracking shows delivered."
                ]
            }
            
            # Process reviews by source
            all_results = []
            source_insights = {}
            
            for source, reviews in reviews_sources.items():
                print(f"Processing {len(reviews)} reviews from {source}...")
                results = workflow.process_batch(reviews)
                
                # Add source information
                for result in results:
                    result['source'] = source
                
                all_results.extend(results)
                
                # Generate source-specific insights
                insights = workflow.generate_insights(results)
                source_insights[source] = insights
            
            # Overall analysis
            print(f"\nTotal reviews processed: {len(all_results)}")
            
            # Create DataFrame for analysis
            df = pd.DataFrame(all_results)
            
            # Sentiment by source
            print("\n=== Sentiment Analysis by Source ===")
            sentiment_by_source = df.groupby(['source', 'sentiment']).size().unstack(fill_value=0)
            print(sentiment_by_source)
            
            # Priority distribution
            print("\n=== Priority Distribution ===")
            priority_dist = df['priority'].value_counts()
            print(priority_dist)
            
            # Category analysis
            print("\n=== Top Categories by Source ===")
            for source in reviews_sources.keys():
                source_df = df[df['source'] == source]
                categories = []
                for cats in source_df['categories']:
                    categories.extend(cats)
                
                if categories:
                    cat_counts = pd.Series(categories).value_counts()
                    print(f"\n{source}:")
                    print(cat_counts.head(3))
            
            # Generate visualizations
            create_visualizations(df, source_insights)
            
            # Generate action items
            print("\n=== Action Items ===")
            generate_action_items(df)
            
            # Save detailed report
            save_report(all_results, source_insights)

        def create_visualizations(df, source_insights):
            """Create visualization plots."""
            
            # Set up the plot style
            plt.style.use('seaborn-v0_8-darkgrid')
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            
            # 1. Sentiment distribution pie chart
            ax1 = axes[0, 0]
            sentiment_counts = df['sentiment'].value_counts()
            ax1.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%')
            ax1.set_title('Overall Sentiment Distribution')
            
            # 2. Priority levels bar chart
            ax2 = axes[0, 1]
            priority_counts = df['priority'].value_counts()
            ax2.bar(priority_counts.index, priority_counts.values)
            ax2.set_title('Review Priority Levels')
            ax2.set_xlabel('Priority')
            ax2.set_ylabel('Count')
            
            # 3. Sentiment by source
            ax3 = axes[1, 0]
            sentiment_by_source = df.groupby(['source', 'sentiment']).size().unstack(fill_value=0)
            sentiment_by_source.plot(kind='bar', ax=ax3)
            ax3.set_title('Sentiment by Source')
            ax3.set_xlabel('Source')
            ax3.set_ylabel('Count')
            ax3.legend(title='Sentiment')
            
            # 4. Category frequency
            ax4 = axes[1, 1]
            all_categories = []
            for cats in df['categories']:
                all_categories.extend(cats)
            cat_series = pd.Series(all_categories)
            cat_counts = cat_series.value_counts().head(5)
            ax4.barh(cat_counts.index, cat_counts.values)
            ax4.set_title('Top 5 Categories')
            ax4.set_xlabel('Frequency')
            
            plt.tight_layout()
            plt.savefig('retail_analysis.png', dpi=300, bbox_inches='tight')
            print("\nVisualization saved as 'retail_analysis.png'")

        def generate_action_items(df):
            """Generate actionable insights from the analysis."""
            
            # Urgent reviews
            urgent_reviews = df[df['priority'].isin(['urgent', 'high'])]
            
            if not urgent_reviews.empty:
                print(f"\n1. URGENT: {len(urgent_reviews)} reviews require immediate attention")
                for _, review in urgent_reviews.iterrows():
                    print(f"   - {review['source']}: \"{review['original_text'][:60]}...\"")
            
            # Negative sentiment analysis
            negative_reviews = df[df['sentiment'] == 'NEGATIVE']
            if not negative_reviews.empty:
                neg_categories = []
                for cats in negative_reviews['categories']:
                    neg_categories.extend(cats)
                
                if neg_categories:
                    top_neg_category = pd.Series(neg_categories).value_counts().index[0]
                    print(f"\n2. Most negative feedback is about: {top_neg_category}")
                    print(f"   Recommendation: Review and improve {top_neg_category} processes")
            
            # Source-specific insights
            source_sentiments = df.groupby('source')['sentiment'].apply(
                lambda x: (x == 'NEGATIVE').sum() / len(x)
            )
            worst_source = source_sentiments.idxmax()
            
            print(f"\n3. Highest negative rate from: {worst_source} ({source_sentiments[worst_source]:.1%})")
            print(f"   Recommendation: Investigate {worst_source} user experience")

        def save_report(results, insights):
            """Save detailed analysis report."""
            
            report = {
                'timestamp': datetime.now().isoformat(),
                'summary': {
                    'total_reviews': len(results),
                    'sources': list(insights.keys()),
                    'urgent_count': sum(1 for r in results if r['priority'] in ['urgent', 'high'])
                },
                'source_insights': insights,
                'detailed_results': results
            }
            
            report_path = Path('retail_analysis_report.json')
            with open(report_path, 'w') as f:
                json.dump(report, f, indent=2, default=str)
            
            print(f"\nDetailed report saved to: {report_path}")

        if __name__ == "__main__":
            analyze_retail_reviews()
      metadata:
        extension: .py
        size_bytes: 7082
        language: python
    notebooks/pipeline_exploration.ipynb:
      content: |
        {
         "cells": [
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "# Pipeline Exploration Notebook\n",
            "\n",
            "This notebook provides interactive examples for exploring Hugging Face pipelines."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "import sys\n",
            "sys.path.append('..')\n",
            "\n",
            "from transformers import pipeline, logging\n",
            "import torch\n",
            "from src.config import get_device, DEFAULT_SENTIMENT_MODEL"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## 1. Basic Pipeline Usage"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Create a simple pipeline\n",
            "device = get_device()\n",
            "print(f\"Using device: {device}\")\n",
            "\n",
            "clf = pipeline(\n",
            "    'sentiment-analysis',\n",
            "    model=DEFAULT_SENTIMENT_MODEL,\n",
            "    device=0 if device == 'cuda' else -1\n",
            ")"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Test the pipeline\n",
            "texts = [\n",
            "    \"I love this product!\",\n",
            "    \"This is terrible.\",\n",
            "    \"It's okay, not great.\"\n",
            "]\n",
            "\n",
            "results = clf(texts)\n",
            "for text, result in zip(texts, results):\n",
            "    print(f\"{text}: {result}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## 2. Pipeline Internals"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Inspect pipeline components\n",
            "print(\"Model architecture:\")\n",
            "print(clf.model)\n",
            "\n",
            "print(\"\\nTokenizer info:\")\n",
            "print(f\"Vocab size: {clf.tokenizer.vocab_size}\")\n",
            "print(f\"Max length: {clf.tokenizer.model_max_length}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## 3. Custom Pipeline Creation"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "from src.custom_pipelines import CustomSentimentPipeline\n",
            "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
            "\n",
            "# Create custom pipeline\n",
            "model = AutoModelForSequenceClassification.from_pretrained(DEFAULT_SENTIMENT_MODEL)\n",
            "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_SENTIMENT_MODEL)\n",
            "\n",
            "custom_pipe = CustomSentimentPipeline(\n",
            "    model=model,\n",
            "    tokenizer=tokenizer,\n",
            "    device=0 if device == 'cuda' else -1\n",
            ")\n",
            "\n",
            "# Test with messy input\n",
            "messy_texts = [\n",
            "    \"<p>AMAZING PRODUCT!!!</p>\",\n",
            "    \"terrible... just terrible!!!!!!\",\n",
            "    \"   Good value   \"\n",
            "]\n",
            "\n",
            "custom_results = custom_pipe(messy_texts)\n",
            "for text, result in zip(messy_texts, custom_results):\n",
            "    print(f\"\\nInput: {text}\")\n",
            "    print(f\"Result: {result}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## 4. Performance Comparison"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "import time\n",
            "\n",
            "# Generate test data\n",
            "test_texts = [\"This is a test sentence.\"] * 100\n",
            "\n",
            "# Test different batch sizes\n",
            "batch_sizes = [1, 8, 16, 32]\n",
            "\n",
            "for batch_size in batch_sizes:\n",
            "    start = time.time()\n",
            "    _ = clf(test_texts, batch_size=batch_size)\n",
            "    end = time.time()\n",
            "    \n",
            "    throughput = len(test_texts) / (end - start)\n",
            "    print(f\"Batch size {batch_size}: {throughput:.1f} samples/sec\")"
           ]
          }
         ],
         "metadata": {
          "kernelspec": {
           "display_name": "Python 3",
           "language": "python",
           "name": "python3"
          }
         },
         "nbformat": 4,
         "nbformat_minor": 4
        }
      metadata:
        extension: .ipynb
        size_bytes: 4077
        language: ipynb
    notebooks/optimization_benchmarks.ipynb:
      content: |
        {
         "cells": [
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "# Model Optimization Benchmarks\n",
            "\n",
            "This notebook demonstrates various optimization techniques and their impact."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 1,
           "metadata": {},
           "outputs": [],
           "source": [
            "import sys\n",
            "sys.path.append('..')\n",
            "\n",
            "import torch\n",
            "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
            "from src.optimization import benchmark_inference\n",
            "from src.utils import calculate_model_size, MemoryTracker\n",
            "import matplotlib.pyplot as plt\n",
            "import numpy as np"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## 1. Load Models"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 2,
           "metadata": {},
           "outputs": [
            {
             "name": "stderr",
             "output_type": "stream",
             "text": [
              "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
              "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
             ]
            },
            {
             "name": "stdout",
             "output_type": "stream",
             "text": [
              "Model: bert-base-uncased\n",
              "Size: 417.7MB\n",
              "Parameters: 109,483,778\n"
             ]
            }
           ],
           "source": [
            "# Load base model\n",
            "model_name = \"bert-base-uncased\"\n",
            "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
            "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
            "\n",
            "# Model info\n",
            "model_info = calculate_model_size(model)\n",
            "print(f\"Model: {model_name}\")\n",
            "print(f\"Size: {model_info['total_size_mb']:.1f}MB\")\n",
            "print(f\"Parameters: {model_info['total_params']:,}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## 2. Quantization Comparison"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 3,
           "metadata": {},
           "outputs": [
            {
             "name": "stdout",
             "output_type": "stream",
             "text": [
              "FP32 Model\n",
              "  Time: 0.741s (24.7ms per sample)\n",
              "  Memory: 0.0MB\n",
              "  Throughput: 40.5 samples/sec\n"
             ]
            },
            {
             "ename": "RuntimeError",
             "evalue": "Didn't find engine for operation quantized::linear_prepack NoQEngine",
             "output_type": "error",
             "traceback": [
              "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
              "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
              "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     17\u001b[39m     fp16_time, fp16_mem = benchmark_inference(\n\u001b[32m     18\u001b[39m         model_fp16, tokenizer, test_texts, \u001b[33m\"\u001b[39m\u001b[33mFP16 Model\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     19\u001b[39m     )\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# INT8 for CPU\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     model_int8 = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantize_dynamic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLinear\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mqint8\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m     int8_time, int8_mem = benchmark_inference(\n\u001b[32m     26\u001b[39m         model_int8, tokenizer, test_texts, \u001b[33m\"\u001b[39m\u001b[33mINT8 Model\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     27\u001b[39m     )\n",
              "\u001b[36mFile \u001b[39m\u001b[32m~/src/art_hug_08/.venv/lib/python3.12/site-packages/torch/ao/quantization/quantize.py:468\u001b[39m, in \u001b[36mquantize_dynamic\u001b[39m\u001b[34m(model, qconfig_spec, dtype, mapping, inplace)\u001b[39m\n\u001b[32m    466\u001b[39m model.eval()\n\u001b[32m    467\u001b[39m propagate_qconfig_(model, qconfig_spec)\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
              "\u001b[36mFile \u001b[39m\u001b[32m~/src/art_hug_08/.venv/lib/python3.12/site-packages/torch/ao/quantization/quantize.py:553\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict)\u001b[39m\n\u001b[32m    551\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[32m    552\u001b[39m     module = copy.deepcopy(module)\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[32m    557\u001b[39m     _remove_qconfig(module)\n",
              "\u001b[36mFile \u001b[39m\u001b[32m~/src/art_hug_08/.venv/lib/python3.12/site-packages/torch/ao/quantization/quantize.py:591\u001b[39m, in \u001b[36m_convert\u001b[39m\u001b[34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[39m\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module.named_children():\n\u001b[32m    587\u001b[39m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[32m    588\u001b[39m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[32m    589\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    590\u001b[39m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[32m    592\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    593\u001b[39m     reassign[name] = swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign.items():\n",
              "\u001b[36mFile \u001b[39m\u001b[32m~/src/art_hug_08/.venv/lib/python3.12/site-packages/torch/ao/quantization/quantize.py:591\u001b[39m, in \u001b[36m_convert\u001b[39m\u001b[34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[39m\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module.named_children():\n\u001b[32m    587\u001b[39m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[32m    588\u001b[39m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[32m    589\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    590\u001b[39m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[32m    592\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    593\u001b[39m     reassign[name] = swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign.items():\n",
              "    \u001b[31m[... skipping similar frames: _convert at line 591 (3 times)]\u001b[39m\n",
              "\u001b[36mFile \u001b[39m\u001b[32m~/src/art_hug_08/.venv/lib/python3.12/site-packages/torch/ao/quantization/quantize.py:591\u001b[39m, in \u001b[36m_convert\u001b[39m\u001b[34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[39m\n\u001b[32m    586\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module.named_children():\n\u001b[32m    587\u001b[39m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[32m    588\u001b[39m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[32m    589\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    590\u001b[39m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[32m--> \u001b[39m\u001b[32m591\u001b[39m         \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# inplace\u001b[39;49;00m\n\u001b[32m    592\u001b[39m \u001b[43m                 \u001b[49m\u001b[43mis_reference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_custom_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    593\u001b[39m     reassign[name] = swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign.items():\n",
              "\u001b[36mFile \u001b[39m\u001b[32m~/src/art_hug_08/.venv/lib/python3.12/site-packages/torch/ao/quantization/quantize.py:593\u001b[39m, in \u001b[36m_convert\u001b[39m\u001b[34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[39m\n\u001b[32m    589\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[32m    590\u001b[39m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[32m    591\u001b[39m         _convert(mod, mapping, \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace\u001b[39;00m\n\u001b[32m    592\u001b[39m                  is_reference, convert_custom_config_dict)\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m     reassign[name] = \u001b[43mswap_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign.items():\n\u001b[32m    596\u001b[39m     module._modules[key] = value\n",
              "\u001b[36mFile \u001b[39m\u001b[32m~/src/art_hug_08/.venv/lib/python3.12/site-packages/torch/ao/quantization/quantize.py:626\u001b[39m, in \u001b[36mswap_module\u001b[39m\u001b[34m(mod, mapping, custom_module_class_mapping)\u001b[39m\n\u001b[32m    624\u001b[39m         new_mod = qmod.from_float(mod, weight_qparams)\n\u001b[32m    625\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m         new_mod = \u001b[43mqmod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    627\u001b[39m     swapped = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m swapped:\n\u001b[32m    630\u001b[39m     \u001b[38;5;66;03m# Preserve module's pre forward hooks. They'll be called on quantized input\u001b[39;00m\n",
              "\u001b[36mFile \u001b[39m\u001b[32m~/src/art_hug_08/.venv/lib/python3.12/site-packages/torch/ao/nn/quantized/dynamic/modules/linear.py:116\u001b[39m, in \u001b[36mLinear.from_float\u001b[39m\u001b[34m(cls, mod)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mUnsupported dtype specified for dynamic quantized Linear!\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m qlinear = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmod\u001b[49m\u001b[43m.\u001b[49m\u001b[43min_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m qlinear.set_weight_bias(qweight, mod.bias)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m qlinear\n",
              "\u001b[36mFile \u001b[39m\u001b[32m~/src/art_hug_08/.venv/lib/python3.12/site-packages/torch/ao/nn/quantized/dynamic/modules/linear.py:40\u001b[39m, in \u001b[36mLinear.__init__\u001b[39m\u001b[34m(self, in_features, out_features, bias_, dtype)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, in_features, out_features, bias_=\u001b[38;5;28;01mTrue\u001b[39;00m, dtype=torch.qint8):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43min_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# We don't muck around with buffers or attributes or anything here\u001b[39;00m\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# to keep the module simple. *everything* is simply a Python attribute.\u001b[39;00m\n\u001b[32m     43\u001b[39m     \u001b[38;5;66;03m# Serialization logic is explicitly handled in the below serialization and\u001b[39;00m\n\u001b[32m     44\u001b[39m     \u001b[38;5;66;03m# deserialization modules\u001b[39;00m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m.version = \u001b[32m4\u001b[39m\n",
              "\u001b[36mFile \u001b[39m\u001b[32m~/src/art_hug_08/.venv/lib/python3.12/site-packages/torch/ao/nn/quantized/modules/linear.py:151\u001b[39m, in \u001b[36mLinear.__init__\u001b[39m\u001b[34m(self, in_features, out_features, bias_, dtype)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mUnsupported dtype specified for quantized Linear!\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m \u001b[38;5;28mself\u001b[39m._packed_params = \u001b[43mLinearPackedParams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[38;5;28mself\u001b[39m._packed_params.set_weight_bias(qweight, bias)\n\u001b[32m    153\u001b[39m \u001b[38;5;28mself\u001b[39m.scale = \u001b[32m1.0\u001b[39m\n",
              "\u001b[36mFile \u001b[39m\u001b[32m~/src/art_hug_08/.venv/lib/python3.12/site-packages/torch/ao/nn/quantized/modules/linear.py:27\u001b[39m, in \u001b[36mLinearPackedParams.__init__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtype == torch.float16:\n\u001b[32m     26\u001b[39m     wq = torch.zeros([\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m], dtype=torch.float)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mset_weight_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
              "\u001b[36mFile \u001b[39m\u001b[32m~/src/art_hug_08/.venv/lib/python3.12/site-packages/torch/ao/nn/quantized/modules/linear.py:32\u001b[39m, in \u001b[36mLinearPackedParams.set_weight_bias\u001b[39m\u001b[34m(self, weight, bias)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;129m@torch\u001b[39m.jit.export\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_weight_bias\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight: torch.Tensor, bias: Optional[torch.Tensor]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtype == torch.qint8:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m         \u001b[38;5;28mself\u001b[39m._packed_params = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantized\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear_prepack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtype == torch.float16:\n\u001b[32m     34\u001b[39m         \u001b[38;5;28mself\u001b[39m._packed_params = torch.ops.quantized.linear_prepack_fp16(weight, bias)\n",
              "\u001b[36mFile \u001b[39m\u001b[32m~/src/art_hug_08/.venv/lib/python3.12/site-packages/torch/_ops.py:755\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    750\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    751\u001b[39m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[32m    752\u001b[39m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[32m    753\u001b[39m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[32m    754\u001b[39m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
              "\u001b[31mRuntimeError\u001b[39m: Didn't find engine for operation quantized::linear_prepack NoQEngine"
             ]
            }
           ],
           "source": [
            "# Test texts\n",
            "test_texts = [\n",
            "    \"This is a positive review.\",\n",
            "    \"This is a negative review.\",\n",
            "    \"This is a neutral statement.\"\n",
            "] * 10\n",
            "\n",
            "# Benchmark FP32\n",
            "fp32_time, fp32_mem = benchmark_inference(\n",
            "    model, tokenizer, test_texts, \"FP32 Model\"\n",
            ")\n",
            "\n",
            "# Quantize and benchmark\n",
            "if torch.cuda.is_available():\n",
            "    # FP16\n",
            "    model_fp16 = model.half()\n",
            "    fp16_time, fp16_mem = benchmark_inference(\n",
            "        model_fp16, tokenizer, test_texts, \"FP16 Model\"\n",
            "    )\n",
            "else:\n",
            "    # INT8 for CPU\n",
            "    model_int8 = torch.quantization.quantize_dynamic(\n",
            "        model, {torch.nn.Linear}, dtype=torch.qint8\n",
            "    )\n",
            "    int8_time, int8_mem = benchmark_inference(\n",
            "        model_int8, tokenizer, test_texts, \"INT8 Model\"\n",
            "    )"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## 3. Batch Size Impact"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 4,
           "metadata": {},
           "outputs": [
            {
             "name": "stdout",
             "output_type": "stream",
             "text": [
              "Batch size 1\n",
              "  Time: 0.068s (68.3ms per sample)\n",
              "  Memory: 0.0MB\n",
              "  Throughput: 14.6 samples/sec\n",
              "Batch size 2\n",
              "  Time: 0.044s (22.1ms per sample)\n",
              "  Memory: 0.0MB\n",
              "  Throughput: 45.2 samples/sec\n",
              "Batch size 4\n",
              "  Time: 0.032s (8.1ms per sample)\n",
              "  Memory: 0.0MB\n",
              "  Throughput: 124.0 samples/sec\n",
              "Batch size 8\n",
              "  Time: 0.032s (4.0ms per sample)\n",
              "  Memory: 0.0MB\n",
              "  Throughput: 248.4 samples/sec\n",
              "Batch size 16\n",
              "  Time: 0.044s (2.7ms per sample)\n",
              "  Memory: 0.0MB\n",
              "  Throughput: 365.7 samples/sec\n",
              "Batch size 32\n",
              "  Time: 0.058s (1.9ms per sample)\n",
              "  Memory: 0.0MB\n",
              "  Throughput: 515.6 samples/sec\n"
             ]
            },
            {
             "data": {
              "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAIjCAYAAAAJLyrXAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAd21JREFUeJzt3XdcleX/x/H3ARkighucuHLgLHOQM/fMQSlmObJlbnNkaY6GZUMbNr5ZmZV7ZJmZaK6chVquTE1z4hacgJz798f94xDi4ChwHw6v5+PB43vOdV8cPnBxvvnmuu7rshmGYQgAAAAAkGYeVhcAAAAAAFkNQQoAAAAAnESQAgAAAAAnEaQAAAAAwEkEKQAAAABwEkEKAAAAAJxEkAIAAAAAJxGkAAAAAMBJBCkAAAAAcBJBCgBczKpVq2Sz2TRv3jyrS7kjBw8elM1m09tvv211KbiJkiVLqm3bthn+dcaOHSubzZbhXwcArECQAoBMYLPZ0vSxatUqq0vN8pYsWaKxY8daXYZDyZIlU4yxr6+v7rnnHg0bNkxnz569o9dcv369xo4dq/Pnz6dvsWl08eJFjRkzRpUrV1auXLmUP39+Va9eXQMHDtSxY8csqQkAMlsOqwsAgOzg66+/TvF8+vTpioyMTNVesWJF7d69OzNLcztLlizRlClTXCpMVa9eXc8//7wk6erVq4qKitLkyZO1evVqbd682enXW79+vcaNG6eePXsqT5486VztrSUkJKhBgwb666+/1KNHD/Xv318XL17Uzp07NWPGDHXs2FFFihSRJI0aNUovvPBCptYHAJmFIAUAmeCxxx5L8Xzjxo2KjIxM1S7proPU5cuX5efnd1evgfRVtGjRFGP95JNPyt/fX2+//bb27t2re+65x8LqnPPdd99p69at+vbbb/Xoo4+muHb16lXFx8c7nufIkUM5cvBPDQDuiaV9AOCi7Ha7XnvtNRUrVky+vr5q0qSJ9u3bl6JPo0aNVLlyZUVFRalBgwby8/PTiy++KEk6efKkevfuraCgIPn6+qpatWr66quvUnx+0v1Y1y8pTLrPadq0aSna586dq9DQUPn6+qpy5cpauHChevbsqZIlS97we/jf//6nMmXKyMfHRzVr1tRvv/2W4nrPnj3l7++vf/75Ry1atFCuXLlUpEgRjR8/XoZhOF1nz549NWXKFEkpl1PeTNu2bVW6dOkbXgsLC9P999/veB4ZGal69eopT5488vf3V/ny5R0/6zsRHBwsSSmCxp9//qmePXuqdOnS8vX1VXBwsJ544gmdOXPG0Wfs2LEaNmyYJKlUqVKO7/HgwYOOPt98841q1aolPz8/5c2bVw0aNNCyZctS1fDrr7+qVq1a8vX1VenSpTV9+vTb1r1//35JUt26dVNd8/X1VUBAQIpa//vz79mz502Xtf53BjEuLk5jxoxR2bJl5ePjo+LFi2v48OGKi4u7bX0AkFn4MxEAuKg33nhDHh4eGjp0qGJiYjRx4kR169ZNmzZtStHvzJkzatWqlSIiIvTYY48pKChIV65cUaNGjbRv3z7169dPpUqV0ty5c9WzZ0+dP39eAwcOdLqeH3/8UV26dFGVKlU0YcIEnTt3Tr1791bRokVv2H/GjBm6cOGCnnnmGdlsNk2cOFGdOnXSP//8Iy8vL0e/xMREtWzZUnXq1NHEiRO1dOlSjRkzRteuXdP48eOdqvGZZ57RsWPHbrhs8ka6dOmi7t2767ffflPNmjUd7f/++682btyot956S5K0c+dOtW3bVlWrVtX48ePl4+Ojffv2ad26dWmqKyEhQadPn5Zkztps3bpV7777rho0aKBSpUo5+kVGRuqff/5Rr169FBwcrJ07d+p///ufdu7cqY0bN8pms6lTp076+++/NXPmTE2aNEkFChSQJBUsWFCSNG7cOI0dO1YPPPCAxo8fL29vb23atEm//PKLmjdv7vha+/bt08MPP6zevXurR48e+uKLL9SzZ0/VqFFDlSpVuun3EhISIslcnjpq1CinNpN45pln1LRp0xRtS5cu1bfffqtChQpJMv+A8NBDD+nXX3/V008/rYoVK2r79u2aNGmS/v77b3333Xdp/noAkKEMAECm69u3r3Gz/wteuXKlIcmoWLGiERcX52h/7733DEnG9u3bHW0NGzY0JBmffPJJiteYPHmyIcn45ptvHG3x8fFGWFiY4e/vb8TGxqb4WitXrkzx+QcOHDAkGV9++aWjrUqVKkaxYsWMCxcuONpWrVplSDJCQkJSfW7+/PmNs2fPOtoXLVpkSDJ++OEHR1uPHj0MSUb//v0dbXa73WjTpo3h7e1tnDp1yuk6b/WzvV5MTIzh4+NjPP/88ynaJ06caNhsNuPff/81DMMwJk2aZEhy1OOMkJAQQ1Kqj7p16xqnT59O0ffy5cupPn/mzJmGJGPNmjWOtrfeesuQZBw4cCBF37179xoeHh5Gx44djcTExBTX7HZ7qpr++5onT5684c/iepcvXzbKly/vGPeePXsan3/+uXHixIlUfceMGXPLsdi7d68RGBhoNGvWzLh27ZphGIbx9ddfGx4eHsbatWtT9P3kk08MSca6detuWR8AZBaW9gGAi+rVq5e8vb0dz+vXry9J+ueff1L08/HxUa9evVK0LVmyRMHBwerataujzcvLSwMGDNDFixe1evVqp2o5duyYtm/fru7du8vf39/R3rBhQ1WpUuWGn9OlSxflzZv3tvVLUr9+/RyPbTab+vXrp/j4eC1fvtypOp0VEBCgVq1aac6cOSmWEs6ePVt16tRRiRIlJMmxocOiRYtkt9ud/jq1a9dWZGSkIiMjtXjxYr322mvauXOnHnroIV25csXRL2fOnI7HV69e1enTp1WnTh1J0pYtW277db777jvZ7Xa9/PLL8vBI+Z/462eOQkNDHWMimTNa5cuXv+H4/FfOnDm1adMmx/LCadOmqXfv3ipcuLD69++f5uV3ly5dUseOHZU3b17NnDlTnp6ekszloxUrVlSFChV0+vRpx0fjxo0lSStXrkzT6wNARiNIAYCLSvpHfJKkUHLu3LkU7UWLFk0RuCRzado999yT6h/TFStWdFx3RlL/smXLprp2ozYp7fV7eHikuk+pXLlykpTivp+M0qVLFx0+fFgbNmyQZN4DFBUVpS5duqToU7duXT355JMKCgpSRESE5syZk+ZQVaBAATVt2lRNmzZVmzZt9OKLL2rq1Klav369pk6d6uh39uxZDRw4UEFBQcqZM6cKFizoWPoXExNz26+zf/9+eXh4KDQ09LZ9rx8fyRyj68fnRgIDAzVx4kQdPHhQBw8e1Oeff67y5cvrww8/1CuvvHLbz5ekp556Svv379fChQuVP39+R/vevXu1c+dOFSxYMMVH0u/EyZMn0/T6AJDRuEcKAFxU0l/or/ffmRMp5SyGs252f0tiYuIdv2aStNafFhlZZ7t27eTn56c5c+bogQce0Jw5c+Th4aFHHnnE0Sdnzpxas2aNVq5cqR9//FFLly7V7Nmz1bhxYy1btuym3+utNGnSRJK0Zs0a9e/fX5LUuXNnrV+/XsOGDVP16tXl7+8vu92uli1b3tFM2K2k1/iEhIToiSeeUMeOHVW6dGl9++23evXVV2/5Oe+9955mzpypb775RtWrV09xzW63q0qVKnr33Xdv+LnFixd3qj4AyCgEKQBwQyEhIfrzzz9lt9tTzEr99ddfjutS8izR9Qe7Xj9jldT/+l0Db9bmDLvdrn/++ccx4yBJf//9tyQ5dgNMa53SzUPXzeTKlUtt27bV3Llz9e6772r27NmqX7++4yykJB4eHmrSpImaNGmid999V6+//rpeeuklrVy5MtUGCmlx7do1SebhtpI5U7dixQqNGzdOL7/8sqPf3r17U33uzb7HMmXKyG63a9euXakCSkbLmzevypQpox07dtyy39q1azV06FANGjRI3bp1S3W9TJky+uOPP9SkSROnxxIAMhNL+wDADbVu3VrR0dGaPXu2o+3atWv64IMP5O/vr4YNG0oyA5Knp6fWrFmT4vM/+uijFM+LFCmiypUra/r06Y5/+EvS6tWrtX379ruu98MPP3Q8NgxDH374oby8vByzNmmtUzKDkZQ6dN1Kly5ddOzYMU2dOlV//PFHimV9krnk7npJQeVOt+T+4YcfJEnVqlWTlDxDdP2M0OTJk1N97s2+xw4dOsjDw0Pjx49PNYN1JzOBN/LHH384diD8r3///Ve7du1S+fLlb/q5x48fV+fOnVWvXj3HjojX69y5s44eParPPvss1bUrV67o0qVLd148AKQjZqQAwA09/fTT+vTTT9WzZ09FRUWpZMmSmjdvntatW6fJkycrd+7cksx7XR555BF98MEHstlsKlOmjBYvXnzD+1Bef/11tW/fXnXr1lWvXr107tw5ffjhh6pcuXKKcOUsX19fLV26VD169FDt2rX1008/6ccff9SLL77o2NLbmTpr1KghSRowYIBatGghT09PRURE3LKG1q1bK3fu3Bo6dKg8PT0VHh6e4vr48eO1Zs0atWnTRiEhITp58qQ++ugjFStWTPXq1bvt93j06FF98803kqT4+Hj98ccf+vTTT1WgQAHHsr6AgAA1aNBAEydOVEJCgooWLaply5bpwIEDN/0eX3rpJUVERMjLy0vt2rVT2bJl9dJLL+mVV15R/fr11alTJ/n4+Oi3335TkSJFNGHChNvWejuRkZEaM2aMHnroIdWpU8dxDtgXX3yhuLi4FOdBXW/AgAE6deqUhg8frlmzZqW4VrVqVVWtWlWPP/645syZo2effVYrV65U3bp1lZiYqL/++ktz5szRzz//nOJ8LwCwjIU7BgJAtpWW7c/nzp2bov1GW303bNjQqFSp0g1f58SJE0avXr2MAgUKGN7e3kaVKlVSfG6SU6dOGeHh4Yafn5+RN29e45lnnjF27NiR6msZhmHMmjXLqFChguHj42NUrlzZ+P77743w8HCjQoUKqep86623Un0tScaYMWMcz3v06GHkypXL2L9/v9G8eXPDz8/PCAoKMsaMGZNq++601nnt2jWjf//+RsGCBQ2bzZbmrdC7detmSDKaNm2a6tqKFSuM9u3bG0WKFDG8vb2NIkWKGF27djX+/vvv277u9dufe3h4GIUKFTK6du1q7Nu3L0XfI0eOGB07djTy5MljBAYGGo888ohx7NixVD83wzCMV155xShatKjh4eGRaiv0L774wrj33nsNHx8fI2/evEbDhg2NyMjIFDW1adMmVa0NGzY0GjZseMvv559//jFefvllo06dOkahQoWMHDlyGAULFjTatGlj/PLLLyn6Xr/9edJ2/Tf6+O/3Fx8fb7z55ptGpUqVHN9DjRo1jHHjxhkxMTG3rA8AMovNMNJprh8AkC1Vr15dBQsWVGRkpNOf27NnT82bN++uZrQAALAC90gBANIkISHBsUFCklWrVumPP/5Qo0aNrCkKAACLcI8UACBNjh49qqZNm+qxxx5TkSJF9Ndff+mTTz5RcHCwnn32WavLAwAgUxGkAABpkjdvXtWoUUNTp07VqVOnlCtXLrVp00ZvvPFGigNVAQDIDrhHCgAAAACcxD1SAAAAAOAkghQAAAAAOIl7pCTZ7XYdO3ZMuXPnls1ms7ocAAAAABYxDEMXLlxQkSJF5OFx83kngpSkY8eOqXjx4laXAQAAAMBFHD58WMWKFbvpdYKUpNy5c0syf1gBAQG37Z+QkKBly5apefPm8vLyyujykEEYx6yPMXQPjKN7YByzPsbQPTCOdy82NlbFixd3ZISbIUhJjuV8AQEBaQ5Sfn5+CggI4Bc0C2Mcsz7G0D0wju6Bccz6GEP3wDimn9vd8sNmEwAAAADgJIIUAAAAADiJIAUAAAAATiJIAQAAAICTCFIAAAAA4CSCFAAAAAA4iSAFAAAAAE4iSAEAAACAkwhSAAAAAOAkghQAAAAAOIkgBQAAAABOIkgBAAAAgJMIUgAAAADgpBxWFwAAAAAge0pMlNaulY4flwoXlurXlzw9ra4qbQhSAAAAADLdggXSwIHSkSPJbcWKSe+9J3XqZF1dacXSPgAAAACZasEC6eGHU4YoSTp61GxfsMCaupxBkAIAAACQaRITzZkow0h9Lalt0CCznysjSAEAAADINGvXpp6J+i/DkA4fNvu5Mu6RAgAAAJDhLl+WFi+W3norbf2PH8/Yeu4WQQoAAABAhoiPl5Ytk2bOlBYtki5dSvvnFi6ccXWlB4IUAAAAgHSTmCitWiXNmiXNny+dO5d8rWRJqUsXado06eTJG98nZbOZu/fVr59JBd8hghQAAACAu2IY0oYNZniaM0c6cSL5WuHCUufOUteuUq1aZlCqVcvcnc9mSxmmbDbzfydPdv3zpAhSAAAAAJxmGNIff5jL9mbPlv79N/lavnxmUIqIkBo0SB2KOnWS5s278TlSkydnjXOkCFIAAAAA0mzPHjM8zZplPk7i7y917GiGp6ZNJW/vW79Op05S+/bm7nzHj5szV/Xru/5MVBKCFAAAAIBb+vdfc9Zp5kxp27bkdh8fqW1bc9le69ZSzpzOva6np9SoUXpWmnkIUgAAAABSOXHCvN9p1ixp/frk9hw5pObNzZmn9u2lgADrarQSQQoAAACAJHOHvQULzJmnlSslu91st9nMmaOICHNJXoEClpbpEjys/OJjx46VzWZL8VGhQgXH9atXr6pv377Knz+//P39FR4erhP/3QJE0qFDh9SmTRv5+fmpUKFCGjZsmK5du5bZ3woAAACQJV28KM2YIT30kBQUJD35pLRihRmiateWJk0yN4T45Rfp6acJUUksn5GqVKmSli9f7nieI0dySYMHD9aPP/6ouXPnKjAwUP369VOnTp20bt06SVJiYqLatGmj4OBgrV+/XsePH1f37t3l5eWl119/PdO/FwAAACAruHpV+uknc9neDz9IV64kX6ta1Zx56tJFKl3auhpdneVBKkeOHAoODk7VHhMTo88//1wzZsxQ48aNJUlffvmlKlasqI0bN6pOnTpatmyZdu3apeXLlysoKEjVq1fXK6+8ohEjRmjs2LHyvt1WIQAAAEA2kZBgzirNnCktXCjFxiZfK1vW3DAiIkIKDbWuxqzE8iC1d+9eFSlSRL6+vgoLC9OECRNUokQJRUVFKSEhQU2bNnX0rVChgkqUKKENGzaoTp062rBhg6pUqaKgoCBHnxYtWqhPnz7auXOn7r333ht+zbi4OMXFxTmex/7/b1FCQoISEhJuW3NSn7T0hetiHLM+xtA9MI7ugXHM+hhD93D9ONrt0rp1Ns2ZY9P8+R46fdrm6FusmKFHHrGrSxe77r03+TDc7P4rkNb3gKVBqnbt2po2bZrKly+v48ePa9y4capfv7527Nih6OhoeXt7K0+ePCk+JygoSNHR0ZKk6OjoFCEq6XrStZuZMGGCxo0bl6p92bJl8vPzS3P9kZGRae4L18U4Zn2MoXtgHN0D45j1MYZZn2FIH330m9auLap164rqzJnkPckDA+P0wAPHVK/eEVWseFYeHlJ0tLnMD6bLly+nqZ+lQapVq1aOx1WrVlXt2rUVEhKiOXPmKKezm9A7YeTIkRoyZIjjeWxsrIoXL67mzZsrIA37NyYkJCgyMlLNmjWTl5dXhtWJjMU4Zn2MoXtgHN0D45j1MYZZ386d5rK96dPjFB3t72gPDDTUoYOhzp3tevBBD+XIUUxSMesKdXGx/13zeAuWL+37rzx58qhcuXLat2+fmjVrpvj4eJ0/fz7FrNSJEycc91QFBwdr8+bNKV4jaVe/G913lcTHx0c+Pj6p2r28vJz6Pw5n+8M1MY5ZH2PoHhhH98A4Zn2MYdayf3/yQbk7diS1eilnTkMPPWRT165Sy5Y2+fjYZPGG3VlGWn//XeqnefHiRe3fv1+FCxdWjRo15OXlpRUrVjiu79mzR4cOHVJYWJgkKSwsTNu3b9fJkycdfSIjIxUQEKBQ7pIDAACAGzp61NySvHZtc5OIl14yQ5SXl9S2rV1Dhvyuo0evadYs88DcG8wfIB1YOiM1dOhQtWvXTiEhITp27JjGjBkjT09Pde3aVYGBgerdu7eGDBmifPnyKSAgQP3791dYWJjq1KkjSWrevLlCQ0P1+OOPa+LEiYqOjtaoUaPUt2/fG844AQAAAFnR6dPSvHnmduVr1pj3QUmSh4fUpIm5217HjpK/f6KWLDkqf/9q1hacDVgapI4cOaKuXbvqzJkzKliwoOrVq6eNGzeqYMGCkqRJkybJw8ND4eHhiouLU4sWLfTRRx85Pt/T01OLFy9Wnz59FBYWply5cqlHjx4aP368Vd8SAAAAkC5iY6XvvjPDU2SkdO1a8rW6dc3tyh9+2DxEN0l233EvM1kapGbNmnXL676+vpoyZYqmTJly0z4hISFasmRJepcGAAAAZLorV6TFi83w9OOP0n9O7NF995nhqXNnqUQJ62qEyaU2mwAAAACym/h4c8Zp5kxp0SLp4sXkaxUqJB+UW66cdTUiNYIUAAAAkMkSE6XVq82Zp/nzpbNnk6+VLGkGp4gIqWrV5INy4VoIUgAAAEAmMAxp40YzPM2ZYx6EmyQ42Fyy17WruRsf4cn1EaQAAACADGIY0p9/msv2Zs2S/v03+VrevOZmERERUsOGkqendXXCeQQpAAAAIJ39/bcZnGbOlP76K7nd31/q0MEMT82aSd7elpWIu0SQAgAAANLBoUPS7NlmeNq6Nbndx0dq08Zctte6teTnZ12NSD8EKQAAAOAOnTghzZ1rzj6tW5fc7ukpNW9uzjx16CAFBFhWIjIIQQoAAABwwrlz0sKF5szTL79IdrvZbrOZ9zpFREjh4VKBAtbWiYxFkAIAAABu49Il6fvvzfC0dKmUkJB8rVYtc9neI49IRYtaVyMyF0EKAAAAuIG4ODM0zZwp/fCDdPly8rUqVZLPeipd2roaYR2CFAAAAPD/rl0zl+vNnGku34uJSb5Wpow58xQRIVWqZF2NcA0EKQAAAGRrdru5UcSsWebGEadOJV8rWlTq0sUMUDVqcFAukhGkAAAAkO0YhhQVZYan2bOlI0eSrxUoYN7vFBEh1asneXhYVydcF0EKAAAA2cauXeayvVmzpH37ktsDAqROnczw1KSJlIN/JeM2+BUBAACAW/vnn+SDcrdvT27PmVNq185ctteypeTra12NyHoIUgAAAHA7x45Jc+aYM0+bNiW3e3mZoSkiQnroIcnf37oakbURpAAAAOAWzpyR5s0zw9Pq1eZ9UJJ5j1PjxmZ46tRJypvX2jrhHghSAAAAyLJiY6VFi8xle5GR5vblSR54wFy29/DDUnCwdTXCPRGkAAAAkKVcuSL9+KM58/Tjj9LVq8nX7r3XnHnq0kUKCbGuRrg/ghQAAABcXkKCOeM0c6b03XfSxYvJ18qXTz4ot3x5y0pENkOQAgAAgEtKTJTWrDFnnubNk86eTb4WEmIGp4gIqVo1DspF5iNIAQAAwGUYhrnL3qxZ5q57x48nXwsKkjp3Nmef6tQhPMFaBCkAAABYyjDM852SDso9eDD5Wt68Uni4OfPUqJHk6WlVlUBKBCkAAABYYu9eMzjNnCnt3p3cniuX1L69OfPUvLnk7W1djcDNEKQAAACQaQ4flmbPNsPTli3J7T4+UuvWZnhq00by87OuRiAtCFIAAADIUCdPSnPnmrNPv/6a3O7pKTVrZi7b69BBCgy0rETAaQQpAAAApLvz56WFC82ZpxUrJLvdbLfZpAYNzPAUHi4VLGhpmcAdI0gBAAAgXVy6JP3wgxmeli6V4uOTr9WsaS7be+QRqVgx62oE0gtBCgAAAHcsLs4MTbNmSd9/L12+nHytcuXks57KlLGuRiAjEKQAAADglGvXpJUrzZmnBQukmJjka6VLmzNPERFmkALcFUEKAAAAt2W3S+vXmzNPc+eaG0gkKVpU6tLFDE/3389BucgeCFIAAAC4IcOQtm41Z55mzza3Lk+SP795v1PXrlK9epKHh3V1AlYgSAEAACCF3bvN8DRrlnlobpKAAKljR3PmqUkTycvLuhoBqxGkAAAAoAMHpPnz79Ho0Tm0fXtyu6+v1K6dOfPUqpX5HABBCgAAINs6flyaM8ecedq40UtSqCRzpqlFC3Pm6aGHpNy5ra0TcEUEKQAAgGzkzBlp/nwzPK1aZd4HJUkeHoYqVz6t557Lq0ceyaF8+SwtE3B5BCkAAAA3d+GCtGiRed/TsmXm9uVJwsLMZXsdOlzTli3r1bp1a+59AtKAIAUAAOCGrlyRliwxZ54WL5auXk2+Vr26uWyvSxepZEmzLSHBiiqBrIsgBQAA4CYSEqTISDM8ffedOROVpFy55INyK1SwrETAbRCkAAAAsrDERGntWnPZ3rx50tmzyddKlDCDU0SEOQvFQblA+iFIAQAAZDGGIW3ebM48zZ5t7r6XpFAhqXNnc/apTh0OygUyCkEKAAAgCzAMaceO5INyDxxIvpYnjxQebs48NWok5eBfeECG420GAADgwvbtM4PTzJnSrl3J7X5+Uvv25sxT8+aSj491NQLZEUEKAADAxRw+bB6UO3OmFBWV3O7tLbVubYanNm2kXLmsqxHI7ghSAAAALuDUKWnuXHP2ae3a5HZPT6lpU3PZXseOUmCgdTUCSEaQAgAAsMj58+Y25TNnSitWmDvwJalf35x5Cg83N5AA4FoIUgAAAJno0iXzgNyZM6WffpLi45Ov3X+/GZ46d5aKFbOuRgC3R5ACAADIYHFx0s8/m8v2vv/eDFNJQkOTD8otW9a6GgE4hyAFAACQAa5dk1atMmeeFiwwl/ElKV06+aDcKlWsqhDA3SBIAQAApBO7XdqwwQxPc+dKJ08mXytSROrSxQxPNWtKNpt1dQK4ewQpAACAu2AY0tat5rK92bOlQ4eSr+XPLz38sLl0r149cwc+AO6BIAUAAHAH/vrLnHmaNUv6++/k9ty5zW3KIyLMbcu9vKyrEUDGIUgBAACk0cGD5qzTzJnSH38kt/v6Sm3bmjNPrVpJOXNaViKATEKQAgAAuIXoaGnOHHPmacOG5PYcOaQWLcyZp/btzZkoANkHQQoAAOA6Z89K8+eb4WnVKnMTCcncIOLBB83w1KmTeQ8UgOyJIAUAACDpwgXzjKeZM80zn65dS75Wp465bO+RR6TCha2rEYDrIEgBAIBs6+pVackSc+Zp8WLpypXka9WqmTNPXbpIpUpZVyMA10SQAgAA2UpCgrR8uRmeFi40Z6KS3HOPOfMUESFVrGhdjQBcH0EKAAC4PbtdWrvWXLY3b5505kzyteLFzeAUESHdey8H5QJIG4IUAABwS4Yh/fZb8kG5x44lXytUyLzfqWtXKSxM8vCwrk4AWRNBCgAAuJXt283wNGuW9M8/ye2BgVJ4uBmeGjUyty8HgDvF/4UAAIAsb9++5PC0c2dyu5+fecZTRIR55pOPj3U1AnAvBCkAAJAlHTliHpQ7c6b0++/J7d7eUqtW5sxT27ZSrlzW1QjAfRGkAABAlnHqlLlZxKxZ5uYRhmG2e3pKTZqYM08dO0p58lhaJoBsgCAFAABcWkyM9N135szT8uVSYmLytXr1zJmnhx82N5AAgMxCkAIAAC7n8mXzgNxZs8wDc+Pikq/VqGGGp86dza3LAcAKBCkAAOAS4uOln382w9OiRdKlS8nXKlZMPij3nnusqxEAkhCkAACAZRITpVWrzGV78+dL588nXytVKvmg3CpVOCgXgGshSAEAgExlt0sbN5rhae5c6cSJ5GuFC0tdupjhqVYtwhMA10WQAgAAGc4wpG3bks96OnQo+Vq+fOZmEV27SvXrmzvwAYCrI0gBAIAMs2ePOfM0a5b5OIm/v7lNeUSE1KyZ5OVlXY0AcCcIUgAAIF39+2/yzNO2bcntvr7mAbkREVLr1lLOnJaVCAB3jSAFAADuWnS0tHhxKb3xhqc2bkxuz5FDat7cXLb30ENSQIB1NQJAevKwuoAkb7zxhmw2mwYNGuRou3r1qvr27av8+fPL399f4eHhOvHfO1IlHTp0SG3atJGfn58KFSqkYcOG6dq1a5lcPQAA2c/Zs9LUqVLTplLJkjk0dWpVbdzoIZtNevBB6dNPpePHpR9/lB57jBAFwL24xIzUb7/9pk8//VRVq1ZN0T548GD9+OOPmjt3rgIDA9WvXz916tRJ69atkyQlJiaqTZs2Cg4O1vr163X8+HF1795dXl5eev311634VgAAcGsXL0rff2/e9/Tzz1JCQtIVm8qVO6tnnglURISnihSxskoAyHiWz0hdvHhR3bp102effaa8efM62mNiYvT555/r3XffVePGjVWjRg19+eWXWr9+vTb+/5qBZcuWadeuXfrmm29UvXp1tWrVSq+88oqmTJmi+Ph4q74lAADcytWr0sKF5rbkhQpJ3bpJixebIapqVen116U9exI0ceJa9e9vJ0QByBYsn5Hq27ev2rRpo6ZNm+rVV191tEdFRSkhIUFNmzZ1tFWoUEElSpTQhg0bVKdOHW3YsEFVqlRRUFCQo0+LFi3Up08f7dy5U/fee+8Nv2ZcXJzi4uIcz2NjYyVJCQkJSkj+09pNJfVJS1+4LsYx62MM3QPj6JoSEqRffrFpzhwPLVpkU2xs8oFOZcsa6tzZrs6d7QoNTeqfoN27GcesjPeie2Ac715af3aWBqlZs2Zpy5Yt+u2331Jdi46Olre3t/LkyZOiPSgoSNHR0Y4+/w1RSdeTrt3MhAkTNG7cuFTty5Ytk5+fX5rrj4yMTHNfuC7GMetjDN0D42g9u13avTu/1q4tqvXriyg21sdxLX/+K6pf/4jq1TuqMmViZLNJBw+aH//FOGZ9jKF7YBzv3OXLl9PUz7IgdfjwYQ0cOFCRkZHy9fXN1K89cuRIDRkyxPE8NjZWxYsXV/PmzRWQhjthExISFBkZqWbNmsmLgy+yLMYx62MM3QPjaC3DkKKibJozx6a5cz109GjyzFPBgobCw+3q0sVQWFgOeXiUlFTyhq/DOGZ9jKF7YBzvXtJqtduxLEhFRUXp5MmTuu+++xxtiYmJWrNmjT788EP9/PPPio+P1/nz51PMSp04cULBwcGSpODgYG3evDnF6ybt6pfU50Z8fHzk4+OTqt3Ly8upXzhn+8M1MY5ZH2PoHhjHzLVjR/JZT/v3J7cHBkqdOplnPTVubFOOHJ5OvS7jmPUxhu6Bcbxzaf25WRakmjRpou3bt6do69WrlypUqKARI0aoePHi8vLy0ooVKxQeHi5J2rNnjw4dOqSwsDBJUlhYmF577TWdPHlShQoVkmROYwYEBCg0adE2AACQZAampPC0Y0dyu5+fecZTRITUsqV0g781AgCuY1mQyp07typXrpyiLVeuXMqfP7+jvXfv3hoyZIjy5cungIAA9e/fX2FhYapTp44kqXnz5goNDdXjjz+uiRMnKjo6WqNGjVLfvn1vOOMEAEB2c/SoNGeOuV35f29J9vKSWrUyD8pt107Klcu6GgEgK7J8175bmTRpkjw8PBQeHq64uDi1aNFCH330keO6p6enFi9erD59+igsLEy5cuVSjx49NH78eAurBgDAWqdOSfPnmzNPa9aY90FJkoeH1KSJGZ46dJD+c+oIAMBJLhWkVq1aleK5r6+vpkyZoilTptz0c0JCQrRkyZIMrgwAANcWEyN9950ZniIjpcTE5Gt165rh6eGHpes2uwUA3CGXClIAACDtLl+WfvzRXLa3ZIn0nyMSdd99Znjq3FkqUcK6GgHAXRGkAADIQuLjpWXLzJmnRYukixeTr1WoYIaniAipXDnragSA7IAgBQCAi0tMlFatMsPT/PnSuXPJ10qWNINTRIRUtapks93sVQAA6YkgBQCACzIMaeNGMzzNmSNFRydfCw6WunQxw1Pt2oQnALACQQoAABdhGNIffySf9fTvv8nX8uY1N4vo2lVq0EDydO6cXABAOiNIAQBgsb//NjeMmDVL+uuv5HZ/f3Ob8ogIqVkzydvbshIBANchSAEAYIFDh6TZs80AtXVrcruPj9S2rRmeWreW/PysqxEAcHMEKQAAMsmJE9LcuWZ4Wr8+uT1HDnPGqWtXqX17KSDAuhoBAGlDkAIAIAOdOyctWGAu2/vlF8luN9ttNqlhQ3PmKTxcKlDA2joBAM4hSAEAkM4uXpS+/94MT0uXSgkJyddq1TJnnh55RCpa1LoaAQB3hyAFAEA6uHrVDE0zZ0o//CBduZJ8rUqV5LOeSpe2rkYAQPohSAEAcIeuXZNWrDBnnhYskGJjk6+VKWPOPEVESJUqWVcjACBjEKQAAHCC3S79+qsZnubOlU6fTr5WtGjyzFONGhyUCwDujCAFAMBtGIYUFWUu25s9Wzp6NPlagQLm/U5du0p160oeHtbVCQDIPAQpAABuYudOc+Zp1ixp377k9oAAqVMnc+apSRNz+3IAQPbC//UDAPAf//yTHJ62b09uz5lTeughMzy1bCn5+lpXIwDAegQpAIBbSkyU1q6Vjh+XCheW6teXPD1v3PfYMWnOHHPp3ubNye1eXmZo6tpVatdO8vfPnNoBAK7vjoLUoUOH9O+//+ry5csqWLCgKlWqJB8fn/SuDQCAO7JggTRwoHTkSHJbsWLSe++ZS/Ikc5OI+fPNmafVq837oCTzHqfGjc2Zp06dpLx5M79+AIDrS3OQOnjwoD7++GPNmjVLR44ckZH0XxxJ3t7eql+/vp5++mmFh4fLgzttAQAWWbBAevjh5GCU5OhRs71fP/N+p8hIc/vyJHXrmuHpkUekoKDMrRkAkPWkKfEMGDBA1apV04EDB/Tqq69q165diomJUXx8vKKjo7VkyRLVq1dPL7/8sqpWrarffvsto+sGACCVxERzJur6ECWZbYYhffCB9NNPZoi6915p4kTp4EFzS/N+/QhRAIC0SdOMVK5cufTPP/8of/78qa4VKlRIjRs3VuPGjTVmzBgtXbpUhw8fVs2aNdO9WAAAbmXt2pTL+W6mRw9p5EipfPmMrwkA4J7SFKQmTJiQ5hds2bLlHRcDAMDdOH48bf1atCBEAQDujtM3Mx04cEB79+5N1b53714dPHgwPWoCAMBpFy5IS5emrW/hwhlbCwDA/TkdpHr27Kn169enat+0aZN69uyZHjUBAJBm8fHSlClS2bLS9Om37muzScWLm1uhAwBwN5wOUlu3blXdunVTtdepU0fbtm1Lj5oAALgtwzDPfgoNNTeJOHnSDFPPP28GJpstZf+k55Mn3/w8KQAA0srpIGWz2XThwoVU7TExMUpMTEyXogAAuJWVK6VataQuXaT9+6VChcxZqV27pLfflubNk4oWTfk5xYqZ7UnnSAEAcDecPpC3QYMGmjBhgmbOnCnP//+TXmJioiZMmKB69eqle4EAACT5809pxIjke6Fy5ZKGDTNnofz9k/t16iS1b2/u4nf8uHlPVP36zEQBANKP00HqzTffVIMGDVS+fHnV//9F5mvXrlVsbKx++eWXdC8QAIB//5VGj5a++cZc0pcjh/TMM2bbzc598vSUGjXK1DIBANmI00v7QkND9eeff6pz5846efKkLly4oO7du+uvv/5S5cqVM6JGAEA2deaMNHSoVK6c9PXXZojq3NlcwvfhhxyeCwCwjtMzUpJUpEgRvf766+ldCwAAkqQrV6T335cmTJBiYsy2Ro2kiRMlznsHALgCp2ekJHMp32OPPaYHHnhAR48elSR9/fXX+vXXX9O1OABA9pKYKH3xhXTPPdILL5ghqkoVackS6ZdfCFEAANfhdJCaP3++WrRooZw5c2rLli2Ki4uTZO7axywVAOBOGIa0eLFNVatKvXtLR49KJUpIX30lbd0qtWqVejtzAACs5HSQevXVV/XJJ5/os88+k5eXl6O9bt262rJlS7oWBwBwfxs32vTSS3XVqVMO7dol5c1rbmG+Z4/UvTs77QEAXJPT90jt2bNHDRo0SNUeGBio8+fPp0dNAIBsYM8e6cUXpQULckgqIF9fQwMH2jRihBmmAABwZU7PSAUHB2vfvn2p2n/99VeVLl06XYoCALiv48elZ5+VKlWSFiyQPDwMNWnyr3buvKY33iBEAQCyBqeD1FNPPaWBAwdq06ZNstlsOnbsmL799lsNHTpUffr0yYgaAQBuIDbWPPepbFnp00/NjSXatZN+//2a+vffpuLFra4QAIC0c3pp3wsvvCC73a4mTZro8uXLatCggXx8fDR06FD1798/I2oEAGRh8fHSJ59Ir7winT5tttWpI735ptSggZSQIB06ZG2NAAA4y+kgZbPZ9NJLL2nYsGHat2+fLl68qNDQUPn7+2dEfQCALMpul2bPll56STpwwGwrV848G6pjR3bhAwBkbXd0jpQkeXt7KzQ0VBUqVNDy5cu1e/fu9KwLAJCFLV9unvn06KNmiAoONmeldu6UOnUiRAEAsj6ng1Tnzp314YcfSpKuXLmimjVrqnPnzqpatarmz5+f7gUCALKOrVulFi2kZs2kLVuk3LnNJX379knPPCPlcHodBAAArsnpILVmzRrVr19fkrRw4ULZ7XadP39e77//vl599dV0LxAA4PoOHJAee0y67z5p2TLJy0saMEDav18aNUrKlcvqCgEASF9OB6mYmBjly5dPkrR06VKFh4fLz89Pbdq00d69e9O9QACA6zp9Who8WKpQQfr2W7Ota1fpr7+k996TCha0tj4AADKK00GqePHi2rBhgy5duqSlS5eqefPmkqRz587J19c33QsEALiey5el11+XypSRJk82d+Zr2lSKipJmzJA4VhAA4O6cXq0+aNAgdevWTf7+/goJCVGjRo0kmUv+qlSpkt71AQBcyLVr0pdfSmPGmAfrSlL16uZW5v//dzUAALIFp4PUc889p1q1aunw4cNq1qyZPDzMSa3SpUtzjxQAuCnDkBYtkkaONJftSVLJktKrr5pL+TzueA9YAACypjQHqfr166t9+/Zq37697r//ft1///0prrdp0ybdiwMAWG/dOmn4cGn9evN5/vzmBhJ9+kg+PtbWBgCAVdL8N8SnnnpKGzZs0H333aeKFStqxIgRWrdunQzDyMj6AAAW2b1b6tBBqlfPDFE5c0ovvmjuxDdoECEKAJC9pTlIde/eXfPnz9fp06f1zjvv6Pz583rkkUcUHBysJ554Qt99952uXLmSkbUCADLB0aPSU09JlSuby/k8PMzn+/ZJr70mBQZaXSEAANZzelW7j4+PWrdurU8//VTHjh3T999/r8KFC2v06NHKnz+/2rZtq3Xr1mVErQCADBQTY8443XOPNHWqZLebM1I7dkj/+59UpIjVFQIA4Dru+oz52rVrq3bt2nrttde0f/9+ff/99zqetJUTAMDlxcVJH31kbhxx9qzZ9sAD0sSJUt261tYGAICrcjpIHT58WDabTcWKFZMkbd68WTNmzFBoaKiefvppDR48ON2LBACkP7vdPPNp1Cjp33/NtgoVpDfekB56SLLZrK0PAABX5vTSvkcffVQrV66UJEVHR6tp06bavHmzXnrpJY0fPz7dCwQApC/DkH7+WbrvPunxx80QVaSI9Nln0vbtUvv2hCgAAG7H6SC1Y8cO1apVS5I0Z84cValSRevXr9e3336radOmpXd9AIB0FBUlNWsmtWwp/fGHFBAgvf66tHev9OSTUo67XvANAED24PR/MhMSEuTz/3veLl++XA899JAkqUKFCtwbBQAu6p9/pJdekmbNMp97e0t9+5qbSxQoYG1tAABkRU7PSFWqVEmffPKJ1q5dq8jISLVs2VKSdOzYMeXPnz/dCwQA3LlTp6QBA8x7n2bNMpfsPfaYtGeP9O67hCgAAO6U00HqzTff1KeffqpGjRqpa9euqlatmiTp+++/dyz5AwBY69Il6ZVXpDJlpA8+kBISpBYtpC1bpK+/lkqWtLpCAACyNqeX9jVq1EinT59WbGys8ubN62h/+umn5efnl67FAQCck5Agff65NG6cFB1ttt13n7mVeZMm1tYGAIA7cXpGSpIMw1BUVJQ+/fRTXbhwQZLk7e1NkAIAixiGNH++VLmy1KePGaJKl5ZmzpR++40QBQBAenN6Rurff/9Vy5YtdejQIcXFxalZs2bKnTu33nzzTcXFxemTTz7JiDoBADexdq00fLi0caP5vEAB6eWXpWeeMTeVAAAA6c/pGamBAwfq/vvv17lz55QzZ05He8eOHbVixYp0LQ4AcHM7d0rt2kkNGpghys9PGj1a2r9f6t+fEAUAQEZyekZq7dq1Wr9+vbyv+y90yZIldfTo0XQrDABwY0eOmDNOX30l2e2Sp6f01FNmW+HCVlcHAED24HSQstvtSkxMTNV+5MgR5c6dO12KAgCkdu6c9MYb0vvvS1evmm3h4dJrr0nly1tbGwAA2Y3TS/uaN2+uyZMnO57bbDZdvHhRY8aMUevWrdOzNgCAzND09tvmVuYTJ5rP69eXNmyQ5s0jRAEAYAWnZ6TeeecdtWjRQqGhobp69aoeffRR7d27VwUKFNDMmTMzokYAyJYSE6VvvzXvezp0yGyrVMmclWrTxjxcFwAAWMPpIFWsWDH98ccfmjVrlv78809dvHhRvXv3Vrdu3VJsPgEAuDOGIS1dKo0YIW3fbrYVKyaNHy91727eEwUAAKzldJCSpBw5cuixxx5L71oAINv77TdzK/NVq8zngYHSiy+au/DxtyoAAFxHmoLU999/n+YXfOihh+64GADIrvbtMwPT3Lnmcx8fMzyNHCnly2dtbQAAILU0BakOHTqk6cVsNtsNd/QDANzYiRPmkr3//U+6ds287+nxx822kBCrqwMAADeTpiBlt9szug4AyFYuXJDefdfcje/iRbOtVStzI4mqVa2tDQAA3N4d3SMFALgzCQnSZ59J48ZJJ0+abTVrSm++KT34oLW1AQCAtHP6HClJWrFihdq2basyZcqoTJkyatu2rZYvX57etQGA2zAM8/6n0FCpb18zRJUtK82ZI23aRIgCACCrcTpIffTRR2rZsqVy586tgQMHauDAgQoICFDr1q01ZcqUjKgRALK0Vauk2rWlzp3NTSUKFZKmTJF27ZIeeYTzoAAAyIqcXtr3+uuva9KkSerXr5+jbcCAAapbt65ef/119e3bN10LBICs6s8/pRdekH76yXyeK5c0dKj0/PNS7tzW1gYAAO6O0zNS58+fV8uWLVO1N2/eXDExMU691scff6yqVasqICBAAQEBCgsL009J/+KQdPXqVfXt21f58+eXv7+/wsPDdeLEiRSvcejQIbVp00Z+fn4qVKiQhg0bpmvXrjn7bQFAujl0SOrZU6pe3QxROXJIzz0n7d8vjR1LiAIAwB04HaQeeughLVy4MFX7okWL1LZtW6deq1ixYnrjjTcUFRWl33//XY0bN1b79u21c+dOSdLgwYP1ww8/aO7cuVq9erWOHTumTp06OT4/MTFRbdq0UXx8vNavX6+vvvpK06ZN08svv+zstwUAd+3sWWnYMKlcOemrr8z7oh55xFzCN2WKFBRkdYUAACC9OL20LzQ0VK+99ppWrVqlsLAwSdLGjRu1bt06Pf/883r//fcdfQcMGHDL12rXrl2K56+99po+/vhjbdy4UcWKFdPnn3+uGTNmqHHjxpKkL7/8UhUrVtTGjRtVp04dLVu2TLt27dLy5csVFBSk6tWr65VXXtGIESM0duxYeXt73/DrxsXFKS4uzvE8NjZWkpSQkKCEhITb/gyS+qSlL1wX45j1ucoYXrkiTZnioYkTPXT+vHnDU8OGdr3+ul01axqSzN36cGOuMo64O4xj1scYugfG8e6l9WdnMwzDcOaFS5UqlbYXttn0zz//pPl1ExMTNXfuXPXo0UNbt25VdHS0mjRponPnzilPnjyOfiEhIRo0aJAGDx6sl19+Wd9//722bdvmuH7gwAGVLl1aW7Zs0b333nvDrzV27FiNGzcuVfuMGTPk5+eX5poBZG+JidKqVSU0Y0YFnTmTU5IUEhKj7t136b77TrKJBAAAWdDly5f16KOPKiYmRgEBATft5/SM1IEDB+6qsOtt375dYWFhunr1qvz9/bVw4UKFhoZq27Zt8vb2ThGiJCkoKEjR0dGSpOjoaAVdt1Ym6XlSnxsZOXKkhgwZ4ngeGxur4sWLq3nz5rf8YSVJSEhQZGSkmjVrJi8vr7R+q3AxjGPWZ9UYGoa0ZIlNo0d7atcuMy2VKGFozJhEPfqonzw978+0WtwB70X3wDhmfYyhe2Ac717SarXbsfxA3vLly2vbtm2KiYnRvHnz1KNHD61evTpDv6aPj498fHxStXt5eTn1C+dsf7gmxjHry8wx3LhRGjFCWrPGfJ43r/TSS1Lfvjb5+lr+f6lZGu9F98A4Zn2MoXtgHO9cWn9uTv9X3zAMzZs3TytXrtTJkydlt9tTXF+wYIFTr+ft7a2yZctKkmrUqKHffvtN7733nrp06aL4+HidP38+xazUiRMnFBwcLEkKDg7W5s2bU7xe0q5+SX0AID3s2WMGpvnzzee+vtLAgWaoypvX2toAAEDmc3rXvkGDBunxxx/XgQMH5O/vr8DAwBQfd8tutysuLk41atSQl5eXVqxY4bi2Z88eHTp0yLHJRVhYmLZv366TJ086+kRGRiogIEChoaF3XQsAHD8u9ekjVapkhigPD+mJJ6S//5beeIMQBQBAduX0jNTXX3+tBQsWqHXr1nf9xUeOHKlWrVqpRIkSunDhgmbMmKFVq1bp559/VmBgoHr37q0hQ4YoX758CggIUP/+/RUWFqY6depIMs+uCg0N1eOPP66JEycqOjpao0aNUt++fW+4dA8A0io2Vnr7bemdd6TLl822du2k11+XKle2tjYAAGA9p4NUYGCgSpcunS5f/OTJk+revbuOHz+uwMBAVa1aVT///LOaNWsmSZo0aZI8PDwUHh6uuLg4tWjRQh999JHj8z09PbV48WL16dNHYWFhypUrl3r06KHx48enS30Asp/4eOnTT6VXXpFOnTLb6tSR3nxTatDA2toAAIDrcDpIJW0d/sUXXyhnzpx39cU///zzW1739fXVlClTNGXKlJv2CQkJ0ZIlS+6qDgCw26U5c8z7oJJObihXTpowQerYUWxlDgAAUnA6SHXu3FkzZ85UoUKFVLJkyVS7WmzZsiXdigOAzLBihblpRFSU+Tw4WBo71rwXig2PAADAjTgdpHr06KGoqCg99thjCgoKko0/0wLIorZtk154Qfr5Z/N57tzS8OHS4MFSrlyWlgYAAFyc00Hqxx9/1M8//6x69eplRD0AkOEOHpRGj5a+/dY8XNfLy9yZb9QoqWBBq6sDAABZgdNBqnjx4goICMiIWgAgQ505I732mjRlirmphCR17WpuLFGmjLW1AQCArMXpc6TeeecdDR8+XAcPHsyAcgAg/V2+bG4aUbq0NGmSGaKaNJF+/12aMYMQBQAAnOf0jNRjjz2my5cvq0yZMvLz80u12cTZs2fTrTgAuBvXrknTpkljxkjHjplt1aubW5k3a8ZOfAAA4M45HaQmT56cAWUAQPoxDOn776WRI6Xdu822kiWlV181l/J5OD0XDwAAkNId7doHAK5q3TpzK/N168zn+fObm0j06SP5+FhbGwAAcB9OB6n/unr1quKT7tj+f2xEAcAKu3ebM1CLFpnPc+Y0tzEfPlwKDLS2NgAA4H6cDlKXLl3SiBEjNGfOHJ05cybV9cTExHQpDADS4uxZX/Xp46kvv5TsdnPZ3hNPmAfqFi1qdXUAAMBdOX2nwPDhw/XLL7/o448/lo+Pj6ZOnapx48apSJEimj59ekbUCACpxMRIo0d76Nlnm+jzzz1kt0vt20s7dkiffUaIAgAAGcvpGakffvhB06dPV6NGjdSrVy/Vr19fZcuWVUhIiL799lt169YtI+oEAElSXJz08cfmxhFnznhKksLC7HrrLQ/VrWtxcQAAINtwekbq7NmzKl26tCTzfqik7c7r1aunNWvWpG91APD/7Hbp22+lChXMe5/OnJHKlzc0cuQmrVqVSIgCAACZyukgVbp0aR04cECSVKFCBc2ZM0eSOVOVJ0+edC0OACRp2TKpRg3pscekgwelIkXM5Xtbt15T7drRnAcFAAAyndNBqlevXvrjjz8kSS+88IKmTJkiX19fDR48WMOGDUv3AgFkX1FR5sG5LVpI27ZJAQHSa69Je/dKTz4p5birfUcBAADunNP/DBk8eLDjcdOmTfXXX38pKipKZcuWVdWqVdO1OADZ0z//mGc/zZxpPvfykvr2lV56SSpQwNraAAAApLs8R0qSQkJCFBgYyLI+AHft1ClzE4mPP5YSEsy2bt2kV16RSpWytjYAAID/cnpp35tvvqnZs2c7nnfu3Fn58+dX0aJFHUv+AMAZly6ZAapMGen9980Q1by5tGWL9M03hCgAAOB6nA5Sn3zyiYoXLy5JioyMVGRkpH766Se1atWKe6QAOCUhQfr0U6lsWWn0aOnCBem++6TISOnnn6V777W6QgAAgBtzemlfdHS0I0gtXrxYnTt3VvPmzVWyZEnVrl073QsE4H4MQ1q4UBo5Uvr7b7OtVClzI4kuXSQPp//EAwAAkLmc/udK3rx5dfjwYUnS0qVL1bRpU0mSYRhKTExM3+oAuJ21a6UHHpDCw80QVaCA9N570l9/SV27EqIAAEDW4PSMVKdOnfToo4/qnnvu0ZkzZ9SqVStJ0tatW1W2bNl0LxCAe9i505yB+uEH87mfnzRkiDRsmLmtOQAAQFbidJCaNGmSSpYsqcOHD2vixIny9/eXJB0/flzPPfdcuhcIIGs7ckQaM0aaNk2y2yVPT/MMqDFjpMKFra4OAADgzjgdpLy8vDR06NBU7f89XwoAzp+X3njDXLZ39arZ1qmT9PrrUvnylpYGAABw19J0N8LGjRvT/IKXL1/Wzp0777ggAFnb1avSO+9IpUtLb75pPq9XT1q/Xpo/nxAFAADcQ5qC1OOPP64WLVpo7ty5unTp0g377Nq1Sy+++KLKlCmjqKiodC0SgOtLTJSmTzeD0tCh0rlzUmio9P330po1UliY1RUCAACknzQt7du1a5c+/vhjjRo1So8++qjKlSunIkWKyNfXV+fOndNff/2lixcvqmPHjlq2bJmqVKmS0XUDcBGGIS1dKr3wgvTnn2Zb0aLS+PFSjx7mPVEAAADuJk1BysvLSwMGDNCAAQP0+++/69dff9W///6rK1euqFq1aho8eLAefPBB5cuXL6PrBeBCfvtNGjFCWrnSfB4YaO7MN2CAlDOntbUBAABkJKc3m7j//vt1//33Z0QtALKIffukl16S5swxn3t7S/37Sy++KPH3FAAAkB04HaQAZF8nT5pL9j79VLp2TbLZpMcfN9tCQqyuDgAAIPMQpADc1sWL5k58b79tPpakVq3M7c2rVrW2NgAAACsQpAAoMVFau1Y6ftw8JLd+fXOTiIQE6bPPpHHjzNkoSapZ09zW/MEHra0ZAADASgQpIJtbsEAaOFA6ciS5rVgxqUsXadEi834oSSpb1jxM9+GHzSV9AAAA2VmazpH6r+nTpysuLi5Ve3x8vKZPn54uRQHIHAsWmMHovyFKMp+/844ZogoVkqZMkXbtkh55hBAFAAAg3UGQ6tWrl2JiYlK1X7hwQb169UqXogBkvMREcybKMG7eJyBA2rNHeu45ycsr82oDAABwdU4HKcMwZLvBn6SPHDmiwMDAdCkKQMZbuzb1TNT1YmOlbdsypRwAAIAsJc33SN17772y2Wyy2Wxq0qSJcuRI/tTExEQdOHBALVu2zJAiAaS/48fTtx8AAEB2kuYg1aFDB0nStm3b1KJFC/n7+zuueXt7q2TJkgoPD0/3AgFkjMKF07cfAABAdpLmIDVmzBhJUsmSJdWlSxf5+vpmWFEAMl7NmpK3txQff+PrNpu5e1/9+plbFwAAQFbg9PbnPXr0yIg6AGQiu13q1Ss5RNlsKTedSLoNcvJk8zwpAAAApOT0ZhMeHh7y9PS86QcA1zdihDR3rrkT39ixUtGiKa8XKybNmyd16mRJeQAAAC7P6RmpBQsWpNi1LyEhQVu3btVXX32lcePGpWtxANLfRx9Jb79tPv7yS6lbN2nUKHMXv+PHzXui6tdnJgoAAOBWnA5SSZtO/NfDDz+sSpUqafbs2erdu3d61AUgA/zwg9S/v/n41VfNECWZoalRI8vKAgAAyHKcXtp3M3Xq1NGKFSvS6+UApLPff5ciIsz7o558UnrxRasrAgAAyLrSJUhduXJF77//vopef6MFAJdw8KDUtq10+bLUooW5vO8G52oDAAAgjZxe2pc3b94U90gZhqELFy7Iz89P33zzTboWB+DunTsntW4tnTghVauWvMkEAAAA7pzTQWry5Mkpnnt4eKhgwYKqXbu28ubNm151AUgHcXFSx47S7t3mTnw//ijlzm11VQAAAFkf50gBbspul554Qlq92gxPP/6YeptzAAAA3Bmng5QknTt3Tp9//rl2794tSQoNDVWvXr2UL1++dC0OwJ0bPVqaMUPKkUOaP1+qWtXqigAAANyH05tNrFmzRiVLltT777+vc+fO6dy5c3r//fdVqlQprVmzJiNqBOCkzz6TXn89+XGzZtbWAwAA4G6cnpHq27evunTpoo8//lie/39iZ2Jiop577jn17dtX27dvT/ciAaTdTz9JffqYj19+WerZ09JyAAAA3JLTM1L79u3T888/7whRkuTp6akhQ4Zo37596VocAOds2yZ17iwlJkrdu0tjx1pdEQAAgHtyOkjdd999jnuj/mv37t2qVq1auhQFwHmHD0tt2kgXL0qNG5tL+jgrCgAAIGM4vbRvwIABGjhwoPbt26c6depIkjZu3KgpU6bojTfe0J9//unoW5W724FMERNjnhV17JhUqZK5uYS3t9VVAQAAuC+ng1TXrl0lScOHD7/hNZvNJsMwZLPZlJiYePcVAril+HgpPFzasUMqXFhaskTKk8fqqgAAANyb00HqwIEDGVEHgDtgGNLTT0srVki5cplnRZUoYXVVAAAA7s/pIBUSEpIRdQC4A+PHS199JXl6SnPnSvfea3VFAAAA2cMdHci7d+9erVy5UidPnpTdbk9x7eWXX06XwgDc2rRpybvyffSR1KqVldUAAABkL04Hqc8++0x9+vRRgQIFFBwcLNt/tgWz2WwEKSATLF8uPfWU+XjkSHN5HwAAADKP00Hq1Vdf1WuvvaYRI0ZkRD0AbmP7dnNziWvXpK5dpVdftboiAACA7Mfpc6TOnTunRx55JCNqAXAbR4+a25zHxkoNGkhffil5OP0uBgAAwN1y+p9gjzzyiJYtW5YRtQC4hQsXzAN3jxyRypeXFi6UfHysrgoAACB7StPSvvfff9/xuGzZsho9erQ2btyoKlWqyMvLK0XfAQMGpG+FAJSQIHXuLP3xh1SokPTTT1K+fFZXBQAAkH2lKUhNmjQpxXN/f3+tXr1aq1evTtFus9kIUkA6MwzpueekpUslPz9p8WKpVCmrqwIAAMje0hSkOIQXsM6ECdLUqea9UDNnSjVrWl0RAAAAuE0dcGEzZkgvvWQ+fv996aGHrK0HAAAAJqe3Px8yZMgN2202m3x9fVW2bFm1b99e+biBA7grq1dLvXqZj59/Xurb19p6AAAAkMzpILV161Zt2bJFiYmJKl++vCTp77//lqenpypUqKCPPvpIzz//vH799VeFhoame8FAdrB7t9ShgxQfLz38sDRxotUVAQAA4L+cXtrXvn17NW3aVMeOHVNUVJSioqJ05MgRNWvWTF27dtXRo0fVoEEDDR48OCPqBdxedLTUqpV0/rz0wAPS9OmcFQUAAOBqnP7n2VtvvaVXXnlFAQEBjrbAwECNHTtWEydOlJ+fn15++WVFRUWla6FAdnDpktS2rfTvv9I990iLFkk5c1pdFQAAAK7ndJCKiYnRyZMnU7WfOnVKsbGxkqQ8efIoPj7+7qsDspFr16SICCkqSipQQFqyxPxfAAAAuJ47Wtr3xBNPaOHChTpy5IiOHDmihQsXqnfv3urQoYMkafPmzSpXrlx61wq4LcOQBg40z4jy9ZW+/14qW9bqqgAAAHAzTgepTz/9VE2aNFFERIRCQkIUEhKiiIgINWnSRJ988okkqUKFCpo6deptX2vChAmqWbOmcufOrUKFCqlDhw7as2dPij5Xr15V3759lT9/fvn7+ys8PFwnTpxI0efQoUNq06aN/Pz8VKhQIQ0bNkzXrl1z9lsDLPPOO9JHH0k2m/TNN1JYmNUVAQAA4FacDlL+/v767LPPdObMGW3dulVbt27VmTNn9L///U+5cuWSJFWvXl3Vq1e/7WutXr1affv21caNGxUZGamEhAQ1b95cly5dcvQZPHiwfvjhB82dO1erV6/WsWPH1KlTJ8f1xMREtWnTRvHx8Vq/fr2++uorTZs2TS+//LKz3xpgiblzpWHDzMfvvCOFh1tbDwAAAG7P6e3Pk/j7+6tq1ap39cWXLl2a4vm0adNUqFAhRUVFqUGDBoqJidHnn3+uGTNmqHHjxpKkL7/8UhUrVtTGjRtVp04dLVu2TLt27dLy5csVFBSk6tWr65VXXtGIESM0duxYeXt731WNQEZat056/HHzcf/+0qBBlpYDAACANHI6SD344IOy2Ww3vf7LL7/ccTExMTGS5DjMNyoqSgkJCWratKmjT4UKFVSiRAlt2LBBderU0YYNG1SlShUFBQU5+rRo0UJ9+vTRzp07de+996b6OnFxcYqLi3M8T9okIyEhQQkJCbetM6lPWvrCdVk9jn//LT30UA7FxdnUrp1dEycmihWpzrF6DJE+GEf3wDhmfYyhe2Ac715af3ZOB6nrl+wlJCRo27Zt2rFjh3r06OHsyznY7XYNGjRIdevWVeXKlSVJ0dHR8vb2Vp48eVL0DQoKUnR0tKPPf0NU0vWkazcyYcIEjRs3LlX7smXL5Ofnl+aaIyMj09wXrsuKcYyJ8dbw4Q109qyX7rnnnB57bJ1+/jkx0+twF7wX3QPj6B4Yx6yPMXQPjOOdu3z5cpr6OR2kJk2adMP2sWPH6uLFi86+nEPfvn21Y8cO/frrr3f8Gmk1cuRIDRkyxPE8NjZWxYsXV/PmzVOcj3UzCQkJioyMVLNmzeTl5ZWRpSIDWTWOly9LzZt76sQJD5UqZWjlSn8VKtQi076+O+G96B4YR/fAOGZ9jKF7YBzvXtJqtdu543ukrvfYY4+pVq1aevvtt53+3H79+mnx4sVas2aNihUr5mgPDg5WfHy8zp8/n2JW6sSJEwoODnb02bx5c4rXS9rVL6nP9Xx8fOTj45Oq3cvLy6lfOGf7wzVl5jgmJkq9ekmbN0t580o//WRT0aL8Dt0t3ovugXF0D4xj1scYugfG8c6l9efm9K59N7Nhwwb5+vo69TmGYahfv35auHChfvnlF5UqVSrF9Ro1asjLy0srVqxwtO3Zs0eHDh1S2P/vDx0WFqbt27enOCQ4MjJSAQEBCg0NvYvvCEh/Q4dKCxdK3t7SokVS+fJWVwQAAIA74fSM1H+3HpfMMHT8+HH9/vvvGj16tFOv1bdvX82YMUOLFi1S7ty5Hfc0BQYGKmfOnAoMDFTv3r01ZMgQ5cuXTwEBAerfv7/CwsJUp04dSVLz5s0VGhqqxx9/XBMnTlR0dLRGjRqlvn373nDWCbDKe+9Jkyebj6dPl+rXt7QcAAAA3AWng1RgYGCK5x4eHipfvrzGjx+v5s2bO/VaH3/8sSSpUaNGKdq//PJL9ezZU5J5T5aHh4fCw8MVFxenFi1a6KOPPnL09fT01OLFi9WnTx+FhYUpV65c6tGjh8aPH+/stwZkmIULpcGDzcdvvCF16WJtPQAAALg7TgepL7/8Mt2+uGEYt+3j6+urKVOmaMqUKTftExISoiVLlqRbXUB62rRJevRRyTCkZ5+Vhg+3uiIAAADcrTvebCIqKkq7d++WJFWqVOmG5zUB2d3+/VK7dtLVq1KbNtIHH0i3OIYNAAAAWYTTQerkyZOKiIjQqlWrHDvpnT9/Xg8++KBmzZqlggULpneNQJZ05ozUqpV06pR0333SrFlSjnTbJxMAAABWcnrXvv79++vChQvauXOnzp49q7Nnz2rHjh2KjY3VgAEDMqJGIMu5elVq317au1cqUUJavFjy97e6KgAAAKQXp/8+vnTpUi1fvlwVK1Z0tIWGhmrKlClObzYBuCO7XerRQ1q3TgoMlJYskQoXtroqAAAApCenZ6TsdvsND6ny8vKS3W5Pl6KArGzkSGnOHMnLy9ytr1IlqysCAABAenM6SDVu3FgDBw7UsWPHHG1Hjx7V4MGD1aRJk3QtDshqPv5YmjjRfPz559KDD1pbDwAAADKG00Hqww8/VGxsrEqWLKkyZcqoTJkyKlWqlGJjY/XBBx9kRI1AlrB4sdSvn/n4lVekxx+3th4AAABkHKfvkSpevLi2bNmi5cuX66+//pIkVaxYUU2bNk334oCs4vffzUN27Xapd2/ppZesrggAAAAZyakglZCQoJw5c2rbtm1q1qyZmjVrllF1AVnGwYNS27bS5ctS8+bm8j7OigIAAHBvTi3t8/LyUokSJZSYmJhR9QBZyrlzUuvW0okTUtWq0ty55iYTAAAAcG9O3yP10ksv6cUXX9TZs2czoh4gy4iLkzp1knbvlooWlX78UQoIsLoqAAAAZAan75H68MMPtW/fPhUpUkQhISHKlStXiutbtmxJt+IAV2UY5r1Qq1ZJuXObZ0UVK2Z1VQAAAMgsTgepDh06ZEAZQNYyerT07beSp6c0b565rA8AAADZh9NBasyYMRlRB5BlTJ0qvfaa+fh//zM3mAAAAED24nSQShIfH6+TJ0/KbrenaC9RosRdFwW4qp9/lp591nw8erT0xBPW1gMAAABrOB2k/v77b/Xu3Vvr169P0W4Yhmw2Gzv6wW1t2yY9/LCUmGgetjtunNUVAQAAwCpOB6levXopR44cWrx4sQoXLiwbB+YgGzh8WGrTRrp4UXrwQXN5H7/6AAAA2ZfTQWrbtm2KiopShQoVMqIewOXExJgh6tgxqVIlacECydvb6qoAAABgJafPkQoNDdXp06czohbA5SQkmMv5tm+XgoPNs6Ly5LG6KgAAAFgtTUEqNjbW8fHmm29q+PDhWrVqlc6cOZPiWmxsbEbXC2Qaw5CeflpavlzKlcsMUSEhVlcFAAAAV5CmpX158uRJcS+UYRhq0qRJij5sNgF388or0rRpkoeHNGeOdN99VlcEAAAAV5GmILVy5cqMrgNwKV99JSUdmfbRR1Lr1tbWAwAAANeSpiDVsGFDjR8/XkOHDpWfn19G1wRYasUK6cknzccjRkjPPGNtPQAAAHA9ad5sYty4cbp48WJG1gJYbscOqVMn6do1KSJCev11qysCAACAK0pzkDIMIyPrACx37Ji5hC82VqpfP/n+KAAAAOB6Tv0zkcN34a4uXDDPijp8WCpfXvruO8nHx+qqAAAA4KqcOpC3XLlytw1TZ8+evauCgMx27ZrUubO0bZtUsKC0ZImUL5/VVQEAAMCVORWkxo0bp8DAwIyqBch0hiENGOChpUulnDmlxYul0qWtrgoAAACuzqkgFRERoUKFCmVULUCmW7DgHn39tadsNmnmTKlWLasrAgAAQFaQ5nukuD8K7mbmTJu+/jpUkvTee1L79hYXBAAAgCyDXfuQLa1ZIz31lKckadCgRPXvb3FBAAAAyFLSvLTPbrdnZB1Aptm925x9io+3KSzsmN54o6AkT6vLAgAAQBbCKTnIVk6cMM+KOn9eqlPHrkGDojgrCgAAAE7jn5DINi5dktq2lQ4elMqWlebPT5SPDzOtAAAAcB5BCtlCYqLUtav0++9S/vzSTz+ZZ0YBAAAAd4IgBbdnGNLAgdIPP0g+PtL335szUgAAAMCdIkjB7b37rjRlimSzSd98Iz3wgNUVAQAAIKsjSMGtzZsnDR1qPn77benhh62tBwAAAO6BIAW3tX699Nhj5uN+/aTBg62tBwAAAO6DIAW3tHev9NBDUlyc+b+TJ5tL+wAAAID0QJCC2zl1SmrVSjpzRqpZU5oxQ/LkvF0AAACkI4IU3MqVK+YM1P79UqlS5k59uXJZXRUAAADcDUEKbiMx0bwnauNGKW9eackSKSjI6qoAAADgjghScBvDhkkLFkje3tJ330kVKlhdEQAAANwVQQpu4YMPpEmTzMfTpkkNGlhaDgAAANwcQQpZ3qJF0sCB5uMJE6SuXa2tBwAAAO6PIIUsbfNmMzgZhvT009KIEVZXBAAAgOyAIIUs659/pLZtzZ36WreWpkzhrCgAAABkDoIUsqQzZ8zwdOqUdO+90uzZUo4cVlcFAACA7IIghSzn6lWpQwdpzx6peHFp8WLJ39/qqgAAAJCdEKSQpdjtUs+e0q+/SgEB5llRRYpYXRUAAACyG4IUspQXXzSX8Xl5SQsXSpUrW10RAAAAsiPuKoHLSkyU1q6Vjh+XCheWdu2S3nzTvDZ1qtS4sbX1AQAAIPsiSMElLVhgng115Ejqa+PGSd27Z35NAAAAQBKCFFzOggXSww+bZ0PdSKVKmVsPAAAAcD3ukYJLSUw0Z6JuFqJsNmnwYLMfAAAAYBWCFFzK2rU3Xs6XxDCkw4fNfgAAAIBVCFJwKcePp28/AAAAICMQpOBSChdO334AAABARiBIwaXUr3/rA3ZtNql4cbMfAAAAYBWCFFyKp6dUvvyNr9ls5v9Onmz2AwAAAKxCkIJL+f57aeVK83HBgimvFSsmzZsndeqU+XUBAAAA/8U5UnAZp05JTz1lPh42TJowwdyd7/hx856o+vWZiQIAAIBrIEjBJRiG9Mwz0smTUuXK0vjxZmhq1MjqygAAAIDUWNoHl/DNN9LChZKXl/T115Kvr9UVAQAAADdHkILlDh+W+vUzH48dK1WvbmU1AAAAwO0RpGApu13q1UuKjZXq1JGGD7e6IgAAAOD2CFKw1JQp0ooVkp+fNH26lIO79gAAAJAFEKRgmb/+Sp6Beust6Z57rK0HAAAASCuCFCxx7ZrUvbt09arUrJnUp4/VFQEAAABpR5CCJSZMkH77TQoMlL74QrLZrK4IAAAASDuCFDJdVJR5TpRk3iNVrJi19QAAAADOIkghU125Ij3+uLm07+GHpUcftboiAAAAwHmWBqk1a9aoXbt2KlKkiGw2m7777rsU1w3D0Msvv6zChQsrZ86catq0qfbu3Zuiz9mzZ9WtWzcFBAQoT5486t27ty5evJiJ3wWcMWqUtHu3FBQkffwxS/oAAACQNVkapC5duqRq1appypQpN7w+ceJEvf/++/rkk0+0adMm5cqVSy1atNDVq1cdfbp166adO3cqMjJSixcv1po1a/T0009n1rcAJ6xaJU2aZD6eOlUqUMDScgAAAIA7ZumpPa1atVKrVq1ueM0wDE2ePFmjRo1S+/btJUnTp09XUFCQvvvuO0VERGj37t1aunSpfvvtN91///2SpA8++ECtW7fW22+/rSJFimTa94Jbi42VevaUDEN68kmpbVurKwIAAADunMsef3rgwAFFR0eradOmjrbAwEDVrl1bGzZsUEREhDZs2KA8efI4QpQkNW3aVB4eHtq0aZM6dux4w9eOi4tTXFyc43lsbKwkKSEhQQkJCbetLalPWvrCNHCgp/7910MlSxp6881rcoUfHeOY9TGG7oFxdA+MY9bHGLoHxvHupfVn57JBKjo6WpIUFBSUoj0oKMhxLTo6WoUKFUpxPUeOHMqXL5+jz41MmDBB48aNS9W+bNky+fn5pbnGyMjINPfNzjZvDta0abVlsxl66ql1Wrv2jNUlpcA4Zn2MoXtgHN0D45j1MYbugXG8c5cvX05TP5cNUhlp5MiRGjJkiON5bGysihcvrubNmysgIOC2n5+QkKDIyEg1a9ZMXl5eGVlqlnfqlPT00+av2eDBdg0bVtviipIxjlkfY+geGEf3wDhmfYyhe2Ac717SarXbcdkgFRwcLEk6ceKEChcu7Gg/ceKEqlev7uhz8uTJFJ937do1nT171vH5N+Lj4yMfH59U7V5eXk79wjnbP7sxDKl/f+nkSalSJem11zzl5eVpdVmpMI5ZH2PoHhhH98A4Zn2MoXtgHO9cWn9uLnuOVKlSpRQcHKwVK1Y42mJjY7Vp0yaFhYVJksLCwnT+/HlFRUU5+vzyyy+y2+2qXdt1Zj6yq2+/lRYskHLkkL7+WvL1tboiAAAAIH1YOiN18eJF7du3z/H8wIED2rZtm/Lly6cSJUpo0KBBevXVV3XPPfeoVKlSGj16tIoUKaIOHTpIkipWrKiWLVvqqaee0ieffKKEhAT169dPERER7NhnscOHpX79zMdjx0r33mtpOQAAAEC6sjRI/f7773rwwQcdz5PuW+rRo4emTZum4cOH69KlS3r66ad1/vx51atXT0uXLpXvf6Y2vv32W/Xr109NmjSRh4eHwsPD9f7772f694JkdrvUq5cUEyPVri2NGGF1RQAAAED6sjRINWrUSIZh3PS6zWbT+PHjNX78+Jv2yZcvn2bMmJER5eEOffSRtGKFlDOnNH26ubQPAAAAcCcue48UsqY9e6Thw83Hb70llStnbT0AAABARiBIId1cuyZ17y5duSI1ayb16WN1RQAAAEDGIEgh3bzxhrR5sxQYKH3xheTBbxcAAADcFP/URbrYskUaN858PGWKVKyYtfUAAAAAGYkghbt29ar0+OPm0r6HH5YefdTqigAAAICMRZDCXRs1Stq1SwoKkj7+WLLZrK4IAAAAyFgEKdyV1auld981H0+dKhUoYG09AAAAQGYgSOGOxcZKPXtKhiH17i21bWt1RQAAAEDmIEjhjg0ZIh08KJUsmTwrBQAAAGQHBCnckR9+kD7/3Lwfato0KSDA6ooAAACAzEOQgtNOnZKefNJ8PGSI1LChtfUAAAAAmY0gBacYhvTss9LJk1JoqPTqq1ZXBAAAAGQ+ghSc8u230oIFUo4c0tdfS76+VlcEAAAAZD6CFNLs8GGpXz/z8Zgx0n33WVsPAAAAYBWCFNLEbpeeeEKKiZFq1ZJeeMHqigAAAADrEKSQJh9/LC1fLuXMKU2fbi7tAwAAALIrghRu6++/pWHDzMcTJ0rly1tbDwAAAGA1ghRu6do1qXt36coVqWlT6bnnrK4IAAAAsB5BCrf05pvSpk1SYKD0xReSB78xAAAAAEEKN7d1qzR2rPn4ww+l4sUtLQcAAABwGQQp3NDVq9Ljj5tL+8LDpW7drK4IAAAAcB0EKdzQ6NHSzp1SUJC5Y5/NZnVFAAAAgOsgSCGVNWukd94xH3/2mVSwoLX1AAAAAK6GIIUULlyQevSQDEPq3Vtq187qigAAAADXQ5BCCkOGSAcPSiVLSu++a3U1AAAAgGsiSMFh8WJp6lTzfqhp06SAAKsrAgAAAFwTQQqSpNOnpSefNB8PGSI1bGhtPQAAAIArI0hBhiE9+6x04oQUGiq9+qrVFQEAAACujSAFzZghzZ8v5cghff215OtrdUUAAACAayNIZXNHjkh9+5qPX35Zuu8+a+sBAAAAsgKCVDZmt0u9ekkxMVKtWtLIkVZXBAAAAGQNBKls7OOPpeXLzaV806ebS/sAAAAA3B5BKpv6+29p2DDz8cSJUvny1tYDAAAAZCUEqWzo2jWpe3fpyhWpSZPke6QAAAAApA2LubKJxERp7Vrp+HFpxQpp0yYpMFD68kvJgzgNAAAAOIUglQ0sWCANHGju0Pdf3btLxYtbUxMAAACQlTEX4eYWLJAefjh1iJKkDz80rwMAAABwDkHKjSUmmjNRhnHzPoMGmf0AAAAApB1Byo2tXXvjmagkhiEdPmz2AwAAAJB2BCk3dvx4+vYDAAAAYCJIubHChdO3HwAAAAATQcqNVa4s5bjFvow2m7lrX/36mVcTAAAA4A4IUm7q6lWpUyfz8F3JDE3/lfR88mTJ0zNTSwMAAACyPIKUG7LbzTOi1q6VAgKkd96RihZN2adYMWnePDNsAQAAAHAOB/K6GcOQhgyR5s6VvLyk776THnzQ3AZ97VpzY4nChc3lfMxEAQAAAHeGIOVm3n1Xeu898/FXX5khSjJDU6NGlpUFAAAAuBWW9rmRWbOkoUPNx2+9JXXtam09AAAAgLsiSLmJlSulHj3MxwMGSM8/b209AAAAgDsjSLmB7dulDh2k+HgpPNxc3nf9Ln0AAAAA0g9BKos7fFhq1UqKjTU3kPjmGzaRAAAAADIaQSoLO3/eDFFHj0oVK5o79Pn6Wl0VAAAA4P4IUllUXJy5nG/nTnM786VLpXz5rK4KAAAAyB4IUllQ0oG7q1dLuXNLP/0klShhdVUAAABA9kGQyoKGDZPmzDEP3F24UKpWzeqKAAAAgOyFIJXFTJpk7sonSV9+KTVpYm09AAAAQHZEkMpC5syRhgwxH7/5ptStm7X1AAAAANkVQSqLWL1aevxx83G/fubyPgAAAADWIEhlATt2SO3bmwfuduwoTZ7MgbsAAACAlQhSLu7IEfOsqJgYqW5d6dtvOXAXAAAAsBpByoXFxEitW5thqkIF6fvvpZw5ra4KAAAAQA6rC0CyxERp7Vrp+HEpf35pwgRp+3YpONg8K4oDdwEAAADXQJByEQsWSAMHmrNP/+Xra4aokiUtKQsAAADADbC0zwUsWCA9/HDqECVJV69K//yT+TUBAAAAuDmClMUSE82ZKMO48XWbTRo0yOwHAAAAwDUQpCy2du2NZ6KSGIZ0+LDZDwAAAIBrIEhZ7Pjx9O0HAAAAIOMRpCxWuHD69gMAAACQ8QhSFqtfXypWzLwX6kZsNql4cbMfAAAAANdAkLKYp6f03nvm4+vDVNLzyZPNfgAAAABcA0HKBXTqJM2bJxUtmrK9WDGzvVMna+oCAAAAcGMcyOsiOnWS2rc3d+c7fty8J6p+fWaiAAAAAFdEkHIhnp5So0ZWVwEAAADgdljaBwAAAABOcpsgNWXKFJUsWVK+vr6qXbu2Nm/ebHVJAAAAANyUWwSp2bNna8iQIRozZoy2bNmiatWqqUWLFjp58qTVpQEAAABwQ24RpN5991099dRT6tWrl0JDQ/XJJ5/Iz89PX3zxhdWlAQAAAHBDWX6zifj4eEVFRWnkyJGONg8PDzVt2lQbNmy44efExcUpLi7O8Tw2NlaSlJCQoISEhNt+zaQ+aekL18U4Zn2MoXtgHN0D45j1MYbugXG8e2n92dkMwzAyuJYMdezYMRUtWlTr169XWFiYo3348OFavXq1Nm3alOpzxo4dq3HjxqVqnzFjhvz8/DK0XgAAAACu6/Lly3r00UcVExOjgICAm/bL8jNSd2LkyJEaMmSI43lsbKyKFy+u5s2b3/KHlSQhIUGRkZFq1qyZvLy8MrJUZCDGMetjDN0D4+geGMesjzF0D4zj3UtarXY7WT5IFShQQJ6enjpx4kSK9hMnTig4OPiGn+Pj4yMfH59U7V5eXk79wjnbH66Jccz6GEP3wDi6B8Yx62MM3QPjeOfS+nPL8ptNeHt7q0aNGlqxYoWjzW63a8WKFSmW+gEAAABAesnyM1KSNGTIEPXo0UP333+/atWqpcmTJ+vSpUvq1auX1aUBAAAAcENuEaS6dOmiU6dO6eWXX1Z0dLSqV6+upUuXKigoyOrSAAAAALghtwhSktSvXz/169fP6jIAAAAAZANZ/h4pAAAAAMhsbjMjdTeSjtJK61aHCQkJunz5smJjY9kNJQtjHLM+xtA9MI7ugXHM+hhD98A43r2kTHC743YJUpIuXLggSSpevLjFlQAAAABwBRcuXFBgYOBNr9uM20WtbMBut+vYsWPKnTu3bDbbbfsnHeB7+PDhNB3gC9fEOGZ9jKF7YBzdA+OY9TGG7oFxvHuGYejChQsqUqSIPDxuficUM1KSPDw8VKxYMac/LyAggF9QN8A4Zn2MoXtgHN0D45j1MYbugXG8O7eaiUrCZhMAAAAA4CSCFAAAAAA4iSB1B3x8fDRmzBj5+PhYXQruAuOY9TGG7oFxdA+MY9bHGLoHxjHzsNkEAAAAADiJGSkAAAAAcBJBCgAAAACcRJACAAAAACcRpAAAAADASQSpOzBlyhSVLFlSvr6+ql27tjZv3mx1SUijsWPHymazpfioUKGC1WXhNtasWaN27dqpSJEistls+u6771JcNwxDL7/8sgoXLqycOXOqadOm2rt3rzXF4qZuN449e/ZM9f5s2bKlNcXihiZMmKCaNWsqd+7cKlSokDp06KA9e/ak6HP16lX17dtX+fPnl7+/v8LDw3XixAmLKsaNpGUcGzVqlOr9+Oyzz1pUMa738ccfq2rVqo5Dd8PCwvTTTz85rvM+zBwEKSfNnj1bQ4YM0ZgxY7RlyxZVq1ZNLVq00MmTJ60uDWlUqVIlHT9+3PHx66+/Wl0SbuPSpUuqVq2apkyZcsPrEydO1Pvvv69PPvlEmzZtUq5cudSiRQtdvXo1kyvFrdxuHCWpZcuWKd6fM2fOzMQKcTurV69W3759tXHjRkVGRiohIUHNmzfXpUuXHH0GDx6sH374QXPnztXq1at17NgxderUycKqcb20jKMkPfXUUynejxMnTrSoYlyvWLFieuONNxQVFaXff/9djRs3Vvv27bVz505JvA8zjQGn1KpVy+jbt6/jeWJiolGkSBFjwoQJFlaFtBozZoxRrVo1q8vAXZBkLFy40PHcbrcbwcHBxltvveVoO3/+vOHj42PMnDnTggqRFtePo2EYRo8ePYz27dtbUg/uzMmTJw1JxurVqw3DMN97Xl5exty5cx19du/ebUgyNmzYYFWZuI3rx9EwDKNhw4bGwIEDrSsKTsubN68xdepU3oeZiBkpJ8THxysqKkpNmzZ1tHl4eKhp06basGGDhZXBGXv37lWRIkVUunRpdevWTYcOHbK6JNyFAwcOKDo6OsX7MjAwULVr1+Z9mQWtWrVKhQoVUvny5dWnTx+dOXPG6pJwCzExMZKkfPnySZKioqKUkJCQ4v1YoUIFlShRgvejC7t+HJN8++23KlCggCpXrqyRI0fq8uXLVpSH20hMTNSsWbN06dIlhYWF8T7MRDmsLiArOX36tBITExUUFJSiPSgoSH/99ZdFVcEZtWvX1rRp01S+fHkdP35c48aNU/369bVjxw7lzp3b6vJwB6KjoyXphu/LpGvIGlq2bKlOnTqpVKlS2r9/v1588UW1atVKGzZskKenp9Xl4Tp2u12DBg1S3bp1VblyZUnm+9Hb21t58uRJ0Zf3o+u60ThK0qOPPqqQkBAVKVJEf/75p0aMGKE9e/ZowYIFFlaL/9q+fbvCwsJ09epV+fv7a+HChQoNDdW2bdt4H2YSghSylVatWjkeV61aVbVr11ZISIjmzJmj3r17W1gZgIiICMfjKlWqqGrVqipTpoxWrVqlJk2aWFgZbqRv377asWMH95lmcTcbx6efftrxuEqVKipcuLCaNGmi/fv3q0yZMpldJm6gfPny2rZtm2JiYjRv3jz16NFDq1evtrqsbIWlfU4oUKCAPD09U+16cuLECQUHB1tUFe5Gnjx5VK5cOe3bt8/qUnCHkt57vC/dT+nSpVWgQAHeny6oX79+Wrx4sVauXKlixYo52oODgxUfH6/z58+n6M/70TXdbBxvpHbt2pLE+9GFeHt7q2zZsqpRo4YmTJigatWq6b333uN9mIkIUk7w9vZWjRo1tGLFCkeb3W7XihUrFBYWZmFluFMXL17U/v37VbhwYatLwR0qVaqUgoODU7wvY2NjtWnTJt6XWdyRI0d05swZ3p8uxDAM9evXTwsXLtQvv/yiUqVKpbheo0YNeXl5pXg/7tmzR4cOHeL96EJuN443sm3bNkni/ejC7Ha74uLieB9mIpb2OWnIkCHq0aOH7r//ftWqVUuTJ0/WpUuX1KtXL6tLQxoMHTpU7dq1U0hIiI4dO6YxY8bI09NTXbt2tbo03MLFixdT/BX0wIED2rZtm/Lly6cSJUpo0KBBevXVV3XPPfeoVKlSGj16tIoUKaIOHTpYVzRSudU45suXT+PGjVN4eLiCg4O1f/9+DR8+XGXLllWLFi0srBr/1bdvX82YMUOLFi1S7ty5HfdbBAYGKmfOnAoMDFTv3r01ZMgQ5cuXTwEBAerfv7/CwsJUp04di6tHktuN4/79+zVjxgy1bt1a+fPn159//qnBgwerQYMGqlq1qsXVQ5JGjhypVq1aqUSJErpw4YJmzJihVatW6eeff+Z9mJms3jYwK/rggw+MEiVKGN7e3katWrWMjRs3Wl0S0qhLly5G4cKFDW9vb6No0aJGly5djH379lldFm5j5cqVhqRUHz169DAMw9wCffTo0UZQUJDh4+NjNGnSxNizZ4+1RSOVW43j5cuXjebNmxsFCxY0vLy8jJCQEOOpp54yoqOjrS4b/3Gj8ZNkfPnll44+V65cMZ577jkjb968hp+fn9GxY0fj+PHj1hWNVG43jocOHTIaNGhg5MuXz/Dx8THKli1rDBs2zIiJibG2cDg88cQTRkhIiOHt7W0ULFjQaNKkibFs2TLHdd6HmcNmGIaRmcENAAAAALI67pECAAAAACcRpAAAAADASQQpAAAAAHASQQoAAAAAnESQAgAAAAAnEaQAAAAAwEkEKQAAAABwEkEKAAAAAJxEkAIAZFvTpk1Tnjx50v11x44dq+rVq6f76wIAXAdBCgBgqZ49e8pmszk+8ufPr5YtW+rPP/906nUyM7wsXLhQderUUWBgoHLnzq1KlSpp0KBBjutDhw7VihUrMqUWAIA1CFIAAMu1bNlSx48f1/Hjx7VixQrlyJFDbdu2tbqsG1qxYoW6dOmi8PBwbd68WVFRUXrttdeUkJDg6OPv76/8+fNbWCUAIKMRpAAAlvPx8VFwcLCCg4NVvXp1vfDCCzp8+LBOnTrl6DNixAiVK1dOfn5+Kl26tEaPHu0IL9OmTdO4ceP0xx9/OGa2pk2bJkk6f/68nnnmGQUFBcnX11eVK1fW4sWLU3z9n3/+WRUrVpS/v78j1N3MDz/8oLp162rYsGEqX768ypUrpw4dOmjKlCmOPtfPjv13xi3po2TJko7rO3bsUKtWreTv76+goCA9/vjjOn369F38RAEAGY0gBQBwKRcvXtQ333yjsmXLppjVyZ07t6ZNm6Zdu3bpvffe02effaZJkyZJkrp06aLnn39elSpVcsxsdenSRXa7Xa1atdK6dev0zTffaNeuXXrjjTfk6enpeN3Lly/r7bff1tdff601a9bo0KFDGjp06E3rCw4O1s6dO7Vjx440f09JNR0/flz79u1T2bJl1aBBA0lm0GvcuLHuvfde/f7771q6dKlOnDihzp07O/ujAwBkohxWFwAAwOLFi+Xv7y9JunTpkgoXLqzFixfLwyP5732jRo1yPC5ZsqSGDh2qWbNmafjw4cqZM6f8/f2VI0cOBQcHO/otW7ZMmzdv1u7du1WuXDlJUunSpVN87YSEBH3yyScqU6aMJKlfv34aP378TWvt37+/1q5dqypVqigkJER16tRR8+bN1a1bN/n4+Nzwc5JqMgxD4eHhCgwM1KeffipJ+vDDD3Xvvffq9ddfd/T/4osvVLx4cf3999+OugEAroUZKQCA5R588EFt27ZN27Zt0+bNm9WiRQu1atVK//77r6PP7NmzVbduXQUHB8vf31+jRo3SoUOHbvm627ZtU7FixW4ZRvz8/BwhSpIKFy6skydP3rR/rly59OOPP2rfvn0aNWqU/P399fzzz6tWrVq6fPnyLet58cUXtWHDBi1atEg5c+aUJP3xxx9auXKl/P39HR8VKlSQJO3fv/+WrwcAsA5BCgBguVy5cqls2bIqW7asatasqalTp+rSpUv67LPPJEkbNmxQt27d1Lp1ay1evFhbt27VSy+9pPj4+Fu+blJYuRUvL68Uz202mwzDuO3nlSlTRk8++aSmTp2qLVu2aNeuXZo9e/ZN+3/zzTeaNGmSFi5cqKJFizraL168qHbt2jmCZNLH3r17Hcv/AACuh6V9AACXY7PZ5OHhoStXrkiS1q9fr5CQEL300kuOPv+drZIkb29vJSYmpmirWrWqjhw5kuFL5EqWLCk/Pz9dunTphtc3bNigJ598Up9++qnq1KmT4tp9992n+fPnq2TJksqRg/8sA0BWwYwUAMBycXFxio6OVnR0tHbv3q3+/fs7Zmok6Z577tGhQ4c0a9Ys7d+/X++//74WLlyY4jVKliypAwcOaNu2bTp9+rTi4uLUsGFDNWjQQOHh4YqMjNSBAwf0008/aenSpXdc69ixYzV8+HCtWrVKBw4c0NatW/XEE08oISFBzZo1S9U/OjpaHTt2VEREhFq0aOH4PpN2JOzbt6/Onj2rrl276rffftP+/fv1888/q1evXqmCIQDAdRCkAACWW7p0qQoXLqzChQurdu3a+u233zR37lw1atRIkvTQQw9p8ODB6tevn6pXr67169dr9OjRKV4jPDxcLVu21IMPPqiCBQtq5syZkqT58+erZs2a6tq1q0JDQzV8+PC7CigNGzbUP//8o+7du6tChQpq1aqVoqOjtWzZMpUvXz5V/7/++ksnTpzQV1995fgeCxcurJo1a0qSihQponXr1ikxMVHNmzdXlSpVNGjQIOXJkyfFZhsAANdiM9KyEBwAAAAA4MCfugAAAADASQQpAAAAAHASQQoAAAAAnESQAgAAAAAnEaQAAAAAwEkEKQAAAABwEkEKAAAAAJxEkAIAAAAAJxGkAAAAAMBJBCkAAAAAcBJBCgAAAACc9H9YPbYreTmlnAAAAABJRU5ErkJggg==",
              "text/plain": [
               "<Figure size 1000x600 with 1 Axes>"
              ]
             },
             "metadata": {},
             "output_type": "display_data"
            }
           ],
           "source": [
            "# Test different batch sizes\n",
            "batch_sizes = [1, 2, 4, 8, 16, 32]\n",
            "throughputs = []\n",
            "\n",
            "for batch_size in batch_sizes:\n",
            "    test_batch = test_texts[:batch_size]\n",
            "    time_taken, _ = benchmark_inference(\n",
            "        model, tokenizer, test_batch, f\"Batch size {batch_size}\"\n",
            "    )\n",
            "    throughput = len(test_batch) / time_taken\n",
            "    throughputs.append(throughput)\n",
            "\n",
            "# Plot results\n",
            "plt.figure(figsize=(10, 6))\n",
            "plt.plot(batch_sizes, throughputs, 'b-o')\n",
            "plt.xlabel('Batch Size')\n",
            "plt.ylabel('Throughput (samples/sec)')\n",
            "plt.title('Throughput vs Batch Size')\n",
            "plt.grid(True)\n",
            "plt.show()"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## 4. Memory Profiling"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 5,
           "metadata": {},
           "outputs": [
            {
             "name": "stdout",
             "output_type": "stream",
             "text": [
              "Peak memory usage: 0.00MB\n"
             ]
            }
           ],
           "source": [
            "# Profile memory usage\n",
            "with MemoryTracker() as tracker:\n",
            "    inputs = tokenizer(\n",
            "        test_texts, \n",
            "        padding=True, \n",
            "        truncation=True,\n",
            "        return_tensors=\"pt\"\n",
            "    )\n",
            "    \n",
            "    if torch.cuda.is_available():\n",
            "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
            "        model = model.cuda()\n",
            "    \n",
            "    with torch.no_grad():\n",
            "        outputs = model(**inputs)\n",
            "\n",
            "print(f\"Peak memory usage: {tracker.get_memory_used():.2f}MB\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## 5. Optimization Summary"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 6,
           "metadata": {},
           "outputs": [
            {
             "data": {
              "image/png": "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",
              "text/plain": [
               "<Figure size 1000x600 with 1 Axes>"
              ]
             },
             "metadata": {},
             "output_type": "display_data"
            }
           ],
           "source": [
            "# Create comparison chart\n",
            "techniques = ['FP32', 'FP16/INT8', 'Batching']\n",
            "speedups = [1.0, 2.0, 5.0]  # Example values\n",
            "\n",
            "plt.figure(figsize=(10, 6))\n",
            "bars = plt.bar(techniques, speedups, color=['blue', 'green', 'orange'])\n",
            "plt.ylabel('Speedup Factor')\n",
            "plt.title('Optimization Technique Comparison')\n",
            "plt.ylim(0, 6)\n",
            "\n",
            "# Add value labels on bars\n",
            "for bar, speedup in zip(bars, speedups):\n",
            "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
            "             f'{speedup}x', ha='center', va='bottom')\n",
            "\n",
            "plt.show()"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": []
          }
         ],
         "metadata": {
          "kernelspec": {
           "display_name": "Python 3 (ipykernel)",
           "language": "python",
           "name": "python3"
          },
          "language_info": {
           "codemirror_mode": {
            "name": "ipython",
            "version": 3
           },
           "file_extension": ".py",
           "mimetype": "text/x-python",
           "name": "python",
           "nbconvert_exporter": "python",
           "pygments_lexer": "ipython3",
           "version": "3.12.9"
          }
         },
         "nbformat": 4,
         "nbformat_minor": 4
        }
      metadata:
        extension: .ipynb
        size_bytes: 92803
        language: ipynb
    notebooks/tutorial.ipynb:
      content: |
        {
         "cells": [
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "# Customizing Pipelines and Data Workflows: Advanced Models and Efficient Processing\n",
            "\n",
            "This notebook contains all examples from Chapter 8 with step-by-step explanations.\n",
            "\n",
            "## Table of Contents\n",
            "1. [Environment Setup](#environment-setup)\n",
            "2. [Pipeline Basics and Customization](#pipeline-basics)\n",
            "3. [Efficient Data Handling](#data-handling)\n",
            "4. [Optimization Techniques](#optimization)\n",
            "5. [Synthetic Data Generation](#synthetic-data)\n",
            "6. [Production Workflows](#production)\n",
            "\n",
            "---"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## Environment Setup <a id='environment-setup'></a>\n",
            "\n",
            "First, let's set up our environment with all necessary imports and configurations."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 2,
           "metadata": {},
           "outputs": [
            {
             "ename": "ImportError",
             "evalue": "cannot import name 'register_pipeline' from 'transformers.pipelines' (/Users/richardhightower/src/art_hug_08/.venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py)",
             "output_type": "error",
             "traceback": [
              "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
              "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
              "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Transformers imports\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     pipeline,\n\u001b[32m     17\u001b[39m     Pipeline,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     AutoModelForCausalLM\n\u001b[32m     22\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipelines\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_pipeline\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Datasets\u001b[39;00m\n",
              "\u001b[31mImportError\u001b[39m: cannot import name 'register_pipeline' from 'transformers.pipelines' (/Users/richardhightower/src/art_hug_08/.venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py)"
             ]
            }
           ],
           "source": [
            "# Core imports\n",
            "import os\n",
            "import sys\n",
            "import time\n",
            "import torch\n",
            "import numpy as np\n",
            "from pathlib import Path\n",
            "from typing import List, Dict, Any\n",
            "from contextlib import contextmanager\n",
            "\n",
            "# Add src to path for local imports\n",
            "sys.path.append('../src')\n",
            "\n",
            "# Transformers imports\n",
            "from transformers import (\n",
            "    pipeline,\n",
            "    Pipeline,\n",
            "    AutoModel,\n",
            "    AutoTokenizer,\n",
            "    AutoModelForSequenceClassification,\n",
            "    AutoModelForCausalLM\n",
            ")\n",
            "from transformers.pipelines import register_pipeline\n",
            "from transformers.utils import logging\n",
            "\n",
            "# Datasets\n",
            "from datasets import load_dataset, Dataset\n",
            "\n",
            "# For optimization examples\n",
            "from peft import LoraConfig, get_peft_model, TaskType\n",
            "\n",
            "# For synthetic data generation\n",
            "from diffusers import DiffusionPipeline\n",
            "\n",
            "print(\"Environment ready!\")\n",
            "print(f\"PyTorch version: {torch.__version__}\")\n",
            "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
            "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Device Configuration\n",
            "\n",
            "Let's implement the cross-platform device detection from the chapter."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 3,
           "metadata": {},
           "outputs": [
            {
             "name": "stdout",
             "output_type": "stream",
             "text": [
              "Using device: mps\n"
             ]
            }
           ],
           "source": [
            "def get_optimal_device() -> torch.device:\n",
            "    \"\"\"Automatically detect best available device.\"\"\"\n",
            "    if torch.cuda.is_available():\n",
            "        return torch.device(\"cuda\")\n",
            "    elif torch.backends.mps.is_available():  # Apple Silicon\n",
            "        return torch.device(\"mps\")\n",
            "    else:\n",
            "        return torch.device(\"cpu\")\n",
            "\n",
            "DEVICE = get_optimal_device()\n",
            "print(f\"Using device: {DEVICE}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## Pipeline Basics and Customization <a id='pipeline-basics'></a>\n",
            "\n",
            "### Quick Start: Modern Pipeline Usage\n",
            "\n",
            "Let's start with the basic pipeline example from the chapter."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 4,
           "metadata": {},
           "outputs": [
            {
             "name": "stderr",
             "output_type": "stream",
             "text": [
              "Device set to use cpu\n"
             ]
            },
            {
             "name": "stdout",
             "output_type": "stream",
             "text": [
              "[{'label': 'POSITIVE', 'score': 0.9998641014099121}]\n",
              "Text: 'I love this product!'\n",
              "  Sentiment: POSITIVE (confidence: 1.000)\n",
              "\n",
              "Text: 'This is terrible.'\n",
              "  Sentiment: NEGATIVE (confidence: 1.000)\n",
              "\n",
              "Text: 'Not bad, but could be better.'\n",
              "  Sentiment: NEGATIVE (confidence: 0.802)\n",
              "\n"
             ]
            }
           ],
           "source": [
            "# Modern quick-start with explicit model and device\n",
            "clf = pipeline(\n",
            "    'sentiment-analysis',\n",
            "    model='distilbert-base-uncased-finetuned-sst-2-english',\n",
            "    device=0 if DEVICE.type == \"cuda\" else -1  # 0 for CUDA GPU, -1 for CPU\n",
            ")\n",
            "\n",
            "# Run prediction on text\n",
            "result = clf('I love Hugging Face!')\n",
            "print(result)\n",
            "# Expected output: [{'label': 'POSITIVE', 'score': 0.9998}]\n",
            "\n",
            "# Let's try multiple examples\n",
            "texts = [\n",
            "    \"I love this product!\",\n",
            "    \"This is terrible.\",\n",
            "    \"Not bad, but could be better.\"\n",
            "]\n",
            "results = clf(texts)\n",
            "for text, result in zip(texts, results):\n",
            "    print(f\"Text: '{text}'\")\n",
            "    print(f\"  Sentiment: {result['label']} (confidence: {result['score']:.3f})\\n\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Custom Preprocessing\n",
            "\n",
            "Now let's add custom preprocessing to normalize text before inference."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 5,
           "metadata": {},
           "outputs": [
            {
             "name": "stdout",
             "output_type": "stream",
             "text": [
              "Original texts: ['Wow! Amazing product!!!', \"I don't like this...\"]\n",
              "Cleaned texts: ['wow amazing product', 'i dont like this']\n",
              "\n",
              "Results after preprocessing:\n",
              "Original: 'Wow! Amazing product!!!'\n",
              "Cleaned: 'wow amazing product'\n",
              "Result: {'label': 'POSITIVE', 'score': 0.9998600482940674}\n",
              "\n",
              "Original: 'I don't like this...'\n",
              "Cleaned: 'i dont like this'\n",
              "Result: {'label': 'NEGATIVE', 'score': 0.8758226037025452}\n",
              "\n"
             ]
            }
           ],
           "source": [
            "def custom_preprocess(text):\n",
            "    \"\"\"Normalize text for consistent predictions.\"\"\"\n",
            "    import string\n",
            "    text = text.lower()\n",
            "    return text.translate(str.maketrans('', '', string.punctuation))\n",
            "\n",
            "# Test preprocessing\n",
            "texts = [\"Wow! Amazing product!!!\", \"I don't like this...\"]\n",
            "print(\"Original texts:\", texts)\n",
            "\n",
            "# Clean then predict\n",
            "cleaned = [custom_preprocess(t) for t in texts]\n",
            "print(\"Cleaned texts:\", cleaned)\n",
            "\n",
            "# Batch processing for speed\n",
            "results = clf(cleaned, batch_size=16)\n",
            "print(\"\\nResults after preprocessing:\")\n",
            "for original, clean, result in zip(texts, cleaned, results):\n",
            "    print(f\"Original: '{original}'\")\n",
            "    print(f\"Cleaned: '{clean}'\")\n",
            "    print(f\"Result: {result}\\n\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Advanced: Pipeline Subclassing\n",
            "\n",
            "Create a reusable pipeline with built-in preprocessing and postprocessing."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 6,
           "metadata": {},
           "outputs": [
            {
             "name": "stdout",
             "output_type": "stream",
             "text": [
              "Custom pipeline concept demonstrated!\n"
             ]
            }
           ],
           "source": [
            "class CustomSentimentPipeline(Pipeline):\n",
            "    def preprocess(self, inputs):\n",
            "        \"\"\"Strip HTML, normalize text.\"\"\"\n",
            "        if isinstance(inputs, str):\n",
            "            text = inputs.lower()\n",
            "            import string\n",
            "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
            "            return super().preprocess(text)\n",
            "        return super().preprocess(inputs)\n",
            "    \n",
            "    def postprocess(self, outputs):\n",
            "        \"\"\"Add confidence thresholds.\"\"\"\n",
            "        results = super().postprocess(outputs)\n",
            "        for r in results:\n",
            "            r['confident'] = r['score'] > 0.95\n",
            "        return results\n",
            "\n",
            "# Note: In practice, you would register and use this custom pipeline\n",
            "# For now, let's demonstrate the concept with the standard pipeline\n",
            "print(\"Custom pipeline concept demonstrated!\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Inspecting Pipeline Components\n",
            "\n",
            "Let's peek under the hood to understand pipeline anatomy."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 7,
           "metadata": {},
           "outputs": [
            {
             "name": "stdout",
             "output_type": "stream",
             "text": [
              "Model: DistilBertForSequenceClassification\n",
              "Tokenizer: DistilBertTokenizerFast\n",
              "Processor: None\n",
              "Framework: pt\n",
              "Device: cpu\n",
              "\n",
              "Model architecture: distilbert\n",
              "Hidden size: 768\n",
              "Number of labels: 2\n",
              "\n",
              "Tokenizer vocab size: 30522\n",
              "Max length: 512\n"
             ]
            }
           ],
           "source": [
            "# Inspect pipeline components\n",
            "print('Model:', type(clf.model).__name__)\n",
            "print('Tokenizer:', type(clf.tokenizer).__name__)  \n",
            "print('Processor:', getattr(clf, 'processor', None))\n",
            "print('Framework:', clf.framework)\n",
            "print('Device:', clf.device)\n",
            "\n",
            "# Let's look at model details\n",
            "print(f\"\\nModel architecture: {clf.model.config.model_type}\")\n",
            "print(f\"Hidden size: {clf.model.config.hidden_size}\")\n",
            "print(f\"Number of labels: {clf.model.config.num_labels}\")\n",
            "\n",
            "# Tokenizer details\n",
            "print(f\"\\nTokenizer vocab size: {clf.tokenizer.vocab_size}\")\n",
            "print(f\"Max length: {clf.tokenizer.model_max_length}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Composing Multiple Pipelines\n",
            "\n",
            "Let's create a combined sentiment + NER pipeline."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 8,
           "metadata": {},
           "outputs": [
            {
             "name": "stderr",
             "output_type": "stream",
             "text": [
              "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
              "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
              "Device set to use mps:0\n",
              "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
              "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
              "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
              "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
              "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
              "Device set to use mps:0\n"
             ]
            },
            {
             "name": "stdout",
             "output_type": "stream",
             "text": [
              "Text: Apple Inc. makes amazing products! I love my iPhone.\n",
              "\n",
              "Sentiment: POSITIVE (1.000)\n",
              "\n",
              "Entities found:\n",
              "  - Apple: I-ORG\n",
              "  - Inc: I-ORG\n",
              "  - iPhone: I-MISC\n"
             ]
            }
           ],
           "source": [
            "# Load individual pipelines\n",
            "sentiment_pipe = pipeline('sentiment-analysis')\n",
            "ner_pipe = pipeline('ner')\n",
            "\n",
            "def combined_analysis(text):\n",
            "    \"\"\"Combine sentiment and NER analysis.\"\"\"\n",
            "    sentiment = sentiment_pipe(text)\n",
            "    entities = ner_pipe(text)\n",
            "    \n",
            "    return {\n",
            "        \"text\": text,\n",
            "        \"sentiment\": sentiment[0],\n",
            "        \"entities\": entities\n",
            "    }\n",
            "\n",
            "# Test combined analysis\n",
            "test_text = \"Apple Inc. makes amazing products! I love my iPhone.\"\n",
            "result = combined_analysis(test_text)\n",
            "\n",
            "print(f\"Text: {result['text']}\")\n",
            "print(f\"\\nSentiment: {result['sentiment']['label']} ({result['sentiment']['score']:.3f})\")\n",
            "print(\"\\nEntities found:\")\n",
            "for entity in result['entities']:\n",
            "    print(f\"  - {entity['word']}: {entity['entity_group'] if 'entity_group' in entity else entity['entity']}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Debugging Pipelines\n",
            "\n",
            "Enable verbose logging to debug pipeline issues."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 9,
           "metadata": {},
           "outputs": [
            {
             "ename": "NameError",
             "evalue": "name 'logging' is not defined",
             "output_type": "error",
             "traceback": [
              "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
              "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
              "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Enable debug logging\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mlogging\u001b[49m.set_verbosity_debug()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Now pipeline operations will show detailed logs\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDebug mode enabled. Running pipeline...\u001b[39m\u001b[33m\"\u001b[39m)\n",
              "\u001b[31mNameError\u001b[39m: name 'logging' is not defined"
             ]
            }
           ],
           "source": [
            "# Enable debug logging\n",
            "logging.set_verbosity_debug()\n",
            "\n",
            "# Now pipeline operations will show detailed logs\n",
            "print(\"Debug mode enabled. Running pipeline...\")\n",
            "result = clf(\"Debug me!\")\n",
            "print(f\"\\nResult: {result}\")\n",
            "\n",
            "# Reset logging to normal\n",
            "logging.set_verbosity_warning()\n",
            "print(\"\\nLogging reset to normal level.\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## Efficient Data Handling with ðŸ¤— Datasets <a id='data-handling'></a>\n",
            "\n",
            "### Loading and Transforming Datasets"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 11,
           "metadata": {},
           "outputs": [
            {
             "ename": "NameError",
             "evalue": "name 'load_dataset' is not defined",
             "output_type": "error",
             "traceback": [
              "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
              "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
              "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load a small dataset for demonstration\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mimdb\u001b[39m\u001b[33m'\u001b[39m, split=\u001b[33m'\u001b[39m\u001b[33mtrain[:1000]\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Load only first 1000 examples\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFirst example: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
              "\u001b[31mNameError\u001b[39m: name 'load_dataset' is not defined"
             ]
            }
           ],
           "source": [
            "# Load a small dataset for demonstration\n",
            "dataset = load_dataset('imdb', split='train[:1000]')  # Load only first 1000 examples\n",
            "print(f\"Dataset size: {len(dataset)}\")\n",
            "print(f\"First example: {dataset[0]}\")\n",
            "print(f\"\\nFeatures: {dataset.features}\")"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 12,
           "metadata": {},
           "outputs": [
            {
             "name": "stdout",
             "output_type": "stream",
             "text": [
              "Transforming dataset...\n"
             ]
            },
            {
             "ename": "NameError",
             "evalue": "name 'dataset' is not defined",
             "output_type": "error",
             "traceback": [
              "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
              "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
              "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTransforming dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m dataset = \u001b[43mdataset\u001b[49m.map(preprocess_batch, batched=\u001b[38;5;28;01mTrue\u001b[39;00m, num_proc=\u001b[32m4\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTransformation completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Filter short reviews\u001b[39;00m\n",
              "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
             ]
            }
           ],
           "source": [
            "# Define preprocessing function\n",
            "def preprocess_batch(batch):\n",
            "    \"\"\"Process entire batches at once.\"\"\"\n",
            "    batch['text'] = [text.lower() for text in batch['text']]\n",
            "    batch['length'] = [len(text.split()) for text in batch['text']]\n",
            "    return batch\n",
            "\n",
            "# Transform with parallel processing\n",
            "print(\"Transforming dataset...\")\n",
            "start_time = time.time()\n",
            "dataset = dataset.map(preprocess_batch, batched=True, num_proc=4)\n",
            "print(f\"Transformation completed in {time.time() - start_time:.2f} seconds\")\n",
            "\n",
            "# Filter short reviews\n",
            "print(f\"\\nDataset before filtering: {len(dataset)} examples\")\n",
            "dataset = dataset.filter(lambda x: x['length'] > 20)\n",
            "print(f\"Dataset after filtering: {len(dataset)} examples\")\n",
            "\n",
            "# Check the new features\n",
            "print(f\"\\nUpdated features: {dataset.features}\")\n",
            "print(f\"Example with new features: {dataset[0]}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Streaming Large Datasets\n",
            "\n",
            "For massive datasets, use streaming to avoid memory issues."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 13,
           "metadata": {},
           "outputs": [
            {
             "ename": "NameError",
             "evalue": "name 'load_dataset' is not defined",
             "output_type": "error",
             "traceback": [
              "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
              "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
              "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m     writer.writerows(sample_data)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Stream the dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m streaming_dataset = \u001b[43mload_dataset\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mcsv\u001b[39m\u001b[33m'\u001b[39m, data_files=csv_path, split=\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m, streaming=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Process in batches\u001b[39;00m\n\u001b[32m     22\u001b[39m batch_size = \u001b[32m32\u001b[39m\n",
              "\u001b[31mNameError\u001b[39m: name 'load_dataset' is not defined"
             ]
            }
           ],
           "source": [
            "# Create a sample CSV for demonstration\n",
            "import csv\n",
            "\n",
            "sample_data = [\n",
            "    {\"text\": \"This product is amazing!\", \"label\": \"positive\"},\n",
            "    {\"text\": \"Terrible experience.\", \"label\": \"negative\"},\n",
            "    {\"text\": \"Good value for money.\", \"label\": \"positive\"},\n",
            "    {\"text\": \"Not worth the price.\", \"label\": \"negative\"},\n",
            "    {\"text\": \"Excellent quality!\", \"label\": \"positive\"}\n",
            "] * 20  # Repeat for larger dataset\n",
            "\n",
            "csv_path = \"sample_reviews.csv\"\n",
            "with open(csv_path, 'w', newline='') as f:\n",
            "    writer = csv.DictWriter(f, fieldnames=['text', 'label'])\n",
            "    writer.writeheader()\n",
            "    writer.writerows(sample_data)\n",
            "\n",
            "# Stream the dataset\n",
            "streaming_dataset = load_dataset('csv', data_files=csv_path, split='train', streaming=True)\n",
            "\n",
            "# Process in batches\n",
            "batch_size = 32\n",
            "batch = []\n",
            "processed_count = 0\n",
            "\n",
            "print(\"Processing streaming dataset...\")\n",
            "for example in streaming_dataset:\n",
            "    batch.append(custom_preprocess(example['text']))\n",
            "    \n",
            "    if len(batch) == batch_size:\n",
            "        # Process batch\n",
            "        results = clf(batch, batch_size=batch_size)\n",
            "        processed_count += len(batch)\n",
            "        print(f\"Processed {processed_count} examples...\")\n",
            "        batch = []\n",
            "    \n",
            "    # Stop after processing 100 examples for demo\n",
            "    if processed_count >= 96:\n",
            "        break\n",
            "\n",
            "# Process remaining batch\n",
            "if batch:\n",
            "    results = clf(batch)\n",
            "    processed_count += len(batch)\n",
            "\n",
            "print(f\"\\nTotal processed: {processed_count} examples\")\n",
            "\n",
            "# Clean up\n",
            "os.remove(csv_path)"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Creating Custom Datasets"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 14,
           "metadata": {},
           "outputs": [
            {
             "ename": "NameError",
             "evalue": "name 'Dataset' is not defined",
             "output_type": "error",
             "traceback": [
              "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
              "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
              "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create a custom dataset from dictionaries\u001b[39;00m\n\u001b[32m      2\u001b[39m custom_data = {\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m      4\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe future of AI is bright and full of possibilities.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mfuture\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mml\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdl\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mnlp\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcv\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     11\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m custom_dataset = \u001b[43mDataset\u001b[49m.from_dict(custom_data)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCustom dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_dataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFirst example: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_dataset[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
              "\u001b[31mNameError\u001b[39m: name 'Dataset' is not defined"
             ]
            }
           ],
           "source": [
            "# Create a custom dataset from dictionaries\n",
            "custom_data = {\n",
            "    \"text\": [\n",
            "        \"The future of AI is bright and full of possibilities.\",\n",
            "        \"Machine learning transforms how we solve complex problems.\",\n",
            "        \"Deep learning models continue to improve rapidly.\",\n",
            "        \"Natural language processing enables better human-computer interaction.\",\n",
            "        \"Computer vision applications are becoming more sophisticated.\"\n",
            "    ],\n",
            "    \"category\": [\"future\", \"ml\", \"dl\", \"nlp\", \"cv\"]\n",
            "}\n",
            "\n",
            "custom_dataset = Dataset.from_dict(custom_data)\n",
            "print(f\"Custom dataset: {custom_dataset}\")\n",
            "print(f\"\\nFirst example: {custom_dataset[0]}\")\n",
            "\n",
            "# Apply transformations\n",
            "def add_metadata(example):\n",
            "    example['word_count'] = len(example['text'].split())\n",
            "    example['char_count'] = len(example['text'])\n",
            "    return example\n",
            "\n",
            "custom_dataset = custom_dataset.map(add_metadata)\n",
            "print(f\"\\nAfter transformation: {custom_dataset[0]}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## Optimization Techniques <a id='optimization'></a>\n",
            "\n",
            "### Batching for 10x Throughput"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 15,
           "metadata": {},
           "outputs": [
            {
             "name": "stdout",
             "output_type": "stream",
             "text": [
              "Method 1: Processing one by one...\n",
              "Time taken: 0.176 seconds\n",
              "Average per text: 22.0 ms\n",
              "\n",
              "Method 2: Batch processing...\n",
              "Time taken: 0.607 seconds\n",
              "Average per text: 19.0 ms\n",
              "\n",
              "Speedup: 1.2x faster with batching!\n"
             ]
            }
           ],
           "source": [
            "# Prepare test data\n",
            "test_texts = [\n",
            "    \"Review 1: This product exceeded my expectations.\",\n",
            "    \"Review 2: Not satisfied with the quality.\",\n",
            "    \"Review 3: Average product, nothing special.\",\n",
            "    \"Review 4: Absolutely love it!\",\n",
            "    \"Review 5: Waste of money.\",\n",
            "    \"Review 6: Good value for the price.\",\n",
            "    \"Review 7: Would recommend to friends.\",\n",
            "    \"Review 8: Poor customer service.\"\n",
            "] * 4  # Repeat for more examples\n",
            "\n",
            "# Method 1: One by one (slow)\n",
            "print(\"Method 1: Processing one by one...\")\n",
            "start_time = time.time()\n",
            "results_single = []\n",
            "for text in test_texts[:8]:  # Process only first 8 for demo\n",
            "    result = clf(text)\n",
            "    results_single.append(result)\n",
            "single_time = time.time() - start_time\n",
            "print(f\"Time taken: {single_time:.3f} seconds\")\n",
            "print(f\"Average per text: {single_time/8*1000:.1f} ms\")\n",
            "\n",
            "# Method 2: Batch processing (fast)\n",
            "print(\"\\nMethod 2: Batch processing...\")\n",
            "start_time = time.time()\n",
            "results_batch = clf(test_texts, \n",
            "                   padding=True,\n",
            "                   truncation=True,\n",
            "                   max_length=128)\n",
            "batch_time = time.time() - start_time\n",
            "print(f\"Time taken: {batch_time:.3f} seconds\")\n",
            "print(f\"Average per text: {batch_time/len(test_texts)*1000:.1f} ms\")\n",
            "\n",
            "# Calculate speedup\n",
            "speedup = (single_time/8*len(test_texts)) / batch_time\n",
            "print(f\"\\nSpeedup: {speedup:.1f}x faster with batching!\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Modern Quantization\n",
            "\n",
            "Demonstrate quantization for cost reduction (requires appropriate hardware)."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 16,
           "metadata": {},
           "outputs": [
            {
             "name": "stderr",
             "output_type": "stream",
             "text": [
              "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
              "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
             ]
            },
            {
             "name": "stdout",
             "output_type": "stream",
             "text": [
              "Loading standard model...\n",
              "Standard model size: 255.4 MB\n",
              "Total parameters: 66,955,010\n",
              "\n",
              "Quantization options:\n",
              "- INT8: ~75% size reduction, minimal accuracy loss\n",
              "- INT4: ~87.5% size reduction, may require fine-tuning\n",
              "- Dynamic quantization: Adapts to input ranges\n"
             ]
            }
           ],
           "source": [
            "# Load a small model for demonstration\n",
            "model_name = \"distilbert-base-uncased\"\n",
            "\n",
            "print(\"Loading standard model...\")\n",
            "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
            "\n",
            "# Calculate model size\n",
            "param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
            "buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
            "model_size = param_size + buffer_size\n",
            "print(f\"Standard model size: {model_size / 1024 / 1024:.1f} MB\")\n",
            "\n",
            "# Count parameters\n",
            "total_params = sum(p.numel() for p in model.parameters())\n",
            "print(f\"Total parameters: {total_params:,}\")\n",
            "\n",
            "# Note: Actual quantization requires bitsandbytes library and compatible GPU\n",
            "# This is a conceptual demonstration\n",
            "print(\"\\nQuantization options:\")\n",
            "print(\"- INT8: ~75% size reduction, minimal accuracy loss\")\n",
            "print(\"- INT4: ~87.5% size reduction, may require fine-tuning\")\n",
            "print(\"- Dynamic quantization: Adapts to input ranges\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Memory Tracking Utility\n",
            "\n",
            "Implement the memory tracking context manager from the chapter."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 17,
           "metadata": {},
           "outputs": [
            {
             "name": "stdout",
             "output_type": "stream",
             "text": [
              "Testing memory tracking...\n",
              "Memory tracking only available for CUDA devices\n"
             ]
            }
           ],
           "source": [
            "@contextmanager\n",
            "def track_memory(device: str = \"cuda\"):\n",
            "    \"\"\"Context manager for GPU memory profiling.\"\"\"\n",
            "    if device == \"cuda\" and torch.cuda.is_available():\n",
            "        torch.cuda.synchronize()\n",
            "        start_memory = torch.cuda.memory_allocated()\n",
            "        yield\n",
            "        torch.cuda.synchronize()\n",
            "        end_memory = torch.cuda.memory_allocated()\n",
            "        memory_used = end_memory - start_memory\n",
            "        print(f\"Memory used: {memory_used / 1024 / 1024:.2f} MB\")\n",
            "    else:\n",
            "        yield\n",
            "        print(\"Memory tracking only available for CUDA devices\")\n",
            "\n",
            "# Example usage\n",
            "print(\"Testing memory tracking...\")\n",
            "with track_memory(device=DEVICE.type):\n",
            "    # Run some inference\n",
            "    _ = clf(\"Test text for memory tracking\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### PEFT/LoRA Concept\n",
            "\n",
            "Demonstrate Parameter-Efficient Fine-Tuning concepts."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 18,
           "metadata": {},
           "outputs": [
            {
             "name": "stdout",
             "output_type": "stream",
             "text": [
              "LoRA Configuration:\n",
              "  Rank (r): 16\n",
              "  Alpha: 32\n",
              "  Dropout: 0.1\n",
              "  Target modules: {'query', 'value'}\n",
              "\n",
              "Parameter efficiency:\n",
              "  Original BERT-base: ~110M parameters\n",
              "  LoRA trainable: ~0.3M parameters\n",
              "  Reduction: 99.7% fewer trainable parameters!\n"
             ]
            }
           ],
           "source": [
            "# PEFT configuration example (conceptual)\n",
            "from peft import LoraConfig, TaskType\n",
            "\n",
            "# Define LoRA configuration\n",
            "peft_config = LoraConfig(\n",
            "    task_type=TaskType.SEQ_CLS,  # Sequence classification\n",
            "    r=16,  # LoRA rank\n",
            "    lora_alpha=32,\n",
            "    lora_dropout=0.1,\n",
            "    target_modules=[\"query\", \"value\"]  # Target attention layers\n",
            ")\n",
            "\n",
            "print(\"LoRA Configuration:\")\n",
            "print(f\"  Rank (r): {peft_config.r}\")\n",
            "print(f\"  Alpha: {peft_config.lora_alpha}\")\n",
            "print(f\"  Dropout: {peft_config.lora_dropout}\")\n",
            "print(f\"  Target modules: {peft_config.target_modules}\")\n",
            "\n",
            "# Calculate approximate trainable parameters\n",
            "# For BERT-base with r=16:\n",
            "# Original: ~110M parameters\n",
            "# LoRA trainable: ~0.3M parameters (0.3% of original)\n",
            "print(\"\\nParameter efficiency:\")\n",
            "print(\"  Original BERT-base: ~110M parameters\")\n",
            "print(\"  LoRA trainable: ~0.3M parameters\")\n",
            "print(\"  Reduction: 99.7% fewer trainable parameters!\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## Synthetic Data Generation <a id='synthetic-data'></a>\n",
            "\n",
            "### Text Generation with LLMs"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 19,
           "metadata": {},
           "outputs": [
            {
             "name": "stdout",
             "output_type": "stream",
             "text": [
              "Loading text generation model...\n"
             ]
            },
            {
             "name": "stderr",
             "output_type": "stream",
             "text": [
              "Device set to use cpu\n"
             ]
            },
            {
             "name": "stdout",
             "output_type": "stream",
             "text": [
              "Generating synthetic reviews...\n",
              "\n",
              "Prompt: 'This smartphone is'\n",
              "  Generated 1: This smartphone is still working, but it's going to be a challenge to figure out how to improve this new feature.\n",
              "\n",
              "Samsung's Galaxy Note 8 is the\n",
              "  Generated 2: This smartphone is an innovative system combining the unique features of the previous generation with a modern platform and a premium design,\" said J.P. Morgan analyst John F.\n",
              "\n",
              "Prompt: 'The laptop performance is'\n",
              "  Generated 1: The laptop performance is superb. I have an older version of the Macbook Air running OS X 10.5.5 (9.8.10).\n",
              "\n",
              "The\n",
              "  Generated 2: The laptop performance is very good and the only thing that I think about is the SSD capacity. For a laptop that is only 2TB, the performance is not as good\n",
              "\n",
              "Prompt: 'Customer service was'\n",
              "  Generated 1: Customer service was one of the most frustrating. I received my order from a local Walmart a day early. When I got home I was given my order at 8:\n",
              "  Generated 2: Customer service was not forthcoming.\n",
              "\n",
              "He also said that the company was unable to discuss the situation or what was happening on the ground.\n",
              "\n",
              "\"We need\n",
              "\n"
             ]
            }
           ],
           "source": [
            "# Load a small text generation model\n",
            "print(\"Loading text generation model...\")\n",
            "gen = pipeline(\n",
            "    'text-generation',\n",
            "    model='gpt2',  # Using smaller model for demo\n",
            "    device=0 if DEVICE.type == \"cuda\" else -1\n",
            ")\n",
            "\n",
            "# Generate product reviews\n",
            "prompts = [\n",
            "    \"This smartphone is\",\n",
            "    \"The laptop performance is\",\n",
            "    \"Customer service was\"\n",
            "]\n",
            "\n",
            "print(\"Generating synthetic reviews...\\n\")\n",
            "for prompt in prompts:\n",
            "    generated = gen(\n",
            "        prompt,\n",
            "        max_new_tokens=30,\n",
            "        num_return_sequences=2,\n",
            "        temperature=0.8,\n",
            "        pad_token_id=gen.tokenizer.eos_token_id\n",
            "    )\n",
            "    \n",
            "    print(f\"Prompt: '{prompt}'\")\n",
            "    for i, g in enumerate(generated):\n",
            "        print(f\"  Generated {i+1}: {g['generated_text']}\")\n",
            "    print()"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Synthetic Data Validation\n",
            "\n",
            "Implement quality checks for synthetic data."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": 20,
           "metadata": {},
           "outputs": [
            {
             "name": "stdout",
             "output_type": "stream",
             "text": [
              "Validation Results:\n",
              "  Total samples: 5\n",
              "  Valid samples: 2\n",
              "  Validity rate: 40.0%\n",
              "  Issues found: {'truncated', 'repetitive', 'too_short'}\n"
             ]
            }
           ],
           "source": [
            "def validate_synthetic_text(texts: List[str]) -> Dict[str, Any]:\n",
            "    \"\"\"Basic validation for synthetic text data.\"\"\"\n",
            "    results = {\n",
            "        \"total\": len(texts),\n",
            "        \"valid\": 0,\n",
            "        \"issues\": []\n",
            "    }\n",
            "    \n",
            "    for text in texts:\n",
            "        issues = []\n",
            "        \n",
            "        # Check length\n",
            "        if len(text.split()) < 5:\n",
            "            issues.append(\"too_short\")\n",
            "        elif len(text.split()) > 200:\n",
            "            issues.append(\"too_long\")\n",
            "        \n",
            "        # Check for repetition\n",
            "        words = text.lower().split()\n",
            "        if len(words) > 0 and len(set(words)) / len(words) < 0.5:\n",
            "            issues.append(\"repetitive\")\n",
            "        \n",
            "        # Check for truncation\n",
            "        if text.endswith(\"...\") or not text.endswith(('.', '!', '?')):\n",
            "            issues.append(\"truncated\")\n",
            "        \n",
            "        if not issues:\n",
            "            results[\"valid\"] += 1\n",
            "        else:\n",
            "            results[\"issues\"].extend(issues)\n",
            "    \n",
            "    results[\"validity_rate\"] = results[\"valid\"] / results[\"total\"]\n",
            "    return results\n",
            "\n",
            "# Test validation\n",
            "synthetic_samples = [\n",
            "    \"This product is amazing and works perfectly!\",\n",
            "    \"Good good good good good.\",\n",
            "    \"The laptop\",\n",
            "    \"Excellent quality and fast shipping. Would buy again.\",\n",
            "    \"This is a test that ends abruptly and\"\n",
            "]\n",
            "\n",
            "validation_results = validate_synthetic_text(synthetic_samples)\n",
            "print(\"Validation Results:\")\n",
            "print(f\"  Total samples: {validation_results['total']}\")\n",
            "print(f\"  Valid samples: {validation_results['valid']}\")\n",
            "print(f\"  Validity rate: {validation_results['validity_rate']:.1%}\")\n",
            "print(f\"  Issues found: {set(validation_results['issues'])}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## Production Workflows <a id='production'></a>\n",
            "\n",
            "### Complete Production Pipeline Example\n",
            "\n",
            "Let's implement a simplified version of the RetailReviewWorkflow from the chapter."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "class SimpleRetailWorkflow:\n",
            "    \"\"\"Simplified production workflow for retail review analysis.\"\"\"\n",
            "    \n",
            "    def __init__(self):\n",
            "        # Initialize pipelines\n",
            "        self.sentiment_pipeline = pipeline(\n",
            "            'sentiment-analysis',\n",
            "            model='distilbert-base-uncased-finetuned-sst-2-english'\n",
            "        )\n",
            "        \n",
            "        # Priority keywords for urgency detection\n",
            "        self.priority_keywords = {\n",
            "            \"urgent\": [\"broken\", \"damaged\", \"fraud\", \"stolen\", \"urgent\"],\n",
            "            \"high\": [\"terrible\", \"awful\", \"worst\", \"refund\", \"complaint\"],\n",
            "            \"medium\": [\"disappointed\", \"issue\", \"problem\", \"concern\"],\n",
            "            \"low\": [\"suggestion\", \"feedback\", \"minor\"]\n",
            "        }\n",
            "    \n",
            "    def analyze_priority(self, text: str) -> str:\n",
            "        \"\"\"Determine review priority based on keywords.\"\"\"\n",
            "        text_lower = text.lower()\n",
            "        \n",
            "        for priority, keywords in self.priority_keywords.items():\n",
            "            if any(keyword in text_lower for keyword in keywords):\n",
            "                return priority\n",
            "        \n",
            "        return \"normal\"\n",
            "    \n",
            "    def process_review(self, review: str) -> Dict[str, Any]:\n",
            "        \"\"\"Process a single review.\"\"\"\n",
            "        # Sentiment analysis\n",
            "        sentiment = self.sentiment_pipeline(review)[0]\n",
            "        \n",
            "        # Priority detection\n",
            "        priority = self.analyze_priority(review)\n",
            "        \n",
            "        return {\n",
            "            \"text\": review,\n",
            "            \"sentiment\": sentiment[\"label\"],\n",
            "            \"sentiment_score\": sentiment[\"score\"],\n",
            "            \"priority\": priority,\n",
            "            \"needs_attention\": priority in [\"urgent\", \"high\"]\n",
            "        }\n",
            "    \n",
            "    def process_batch(self, reviews: List[str]) -> Dict[str, Any]:\n",
            "        \"\"\"Process multiple reviews and generate insights.\"\"\"\n",
            "        results = [self.process_review(review) for review in reviews]\n",
            "        \n",
            "        # Generate insights\n",
            "        total = len(results)\n",
            "        urgent_count = sum(1 for r in results if r[\"priority\"] in [\"urgent\", \"high\"])\n",
            "        positive_count = sum(1 for r in results if r[\"sentiment\"] == \"POSITIVE\")\n",
            "        \n",
            "        insights = {\n",
            "            \"total_reviews\": total,\n",
            "            \"urgent_reviews\": urgent_count,\n",
            "            \"sentiment_distribution\": {\n",
            "                \"positive\": positive_count,\n",
            "                \"negative\": total - positive_count,\n",
            "                \"positive_rate\": positive_count / total if total > 0 else 0\n",
            "            },\n",
            "            \"results\": results\n",
            "        }\n",
            "        \n",
            "        return insights\n",
            "\n",
            "# Test the workflow\n",
            "workflow = SimpleRetailWorkflow()\n",
            "\n",
            "sample_reviews = [\n",
            "    \"This product is absolutely amazing! Fast shipping and great quality.\",\n",
            "    \"Terrible experience. The item arrived broken and customer service was unhelpful.\",\n",
            "    \"Good value for money, but packaging could be better.\",\n",
            "    \"URGENT: Received wrong item. Need immediate refund!\",\n",
            "    \"The product works as described. Delivery was on time.\"\n",
            "]\n",
            "\n",
            "# Process reviews\n",
            "print(\"Processing reviews...\\n\")\n",
            "start_time = time.time()\n",
            "insights = workflow.process_batch(sample_reviews)\n",
            "process_time = time.time() - start_time\n",
            "\n",
            "# Display results\n",
            "print(\"=== WORKFLOW RESULTS ===\")\n",
            "print(f\"Total reviews processed: {insights['total_reviews']}\")\n",
            "print(f\"Processing time: {process_time:.3f} seconds\")\n",
            "print(f\"\\nUrgent reviews requiring attention: {insights['urgent_reviews']}\")\n",
            "print(f\"Positive sentiment rate: {insights['sentiment_distribution']['positive_rate']:.1%}\")\n",
            "\n",
            "print(\"\\n=== DETAILED RESULTS ===\")\n",
            "for i, result in enumerate(insights['results']):\n",
            "    if result['needs_attention']:\n",
            "        print(f\"\\nâš ï¸  Review {i+1} (NEEDS ATTENTION):\")\n",
            "    else:\n",
            "        print(f\"\\nReview {i+1}:\")\n",
            "    print(f\"  Text: '{result['text'][:50]}...'\")\n",
            "    print(f\"  Sentiment: {result['sentiment']} ({result['sentiment_score']:.3f})\")\n",
            "    print(f\"  Priority: {result['priority'].upper()}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Configuration Management\n",
            "\n",
            "Implement a configuration system with environment variable support."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "class Config:\n",
            "    \"\"\"Centralized configuration with environment fallbacks.\"\"\"\n",
            "    \n",
            "    # Device configuration with automatic detection\n",
            "    DEVICE = get_optimal_device()\n",
            "    \n",
            "    # Model configurations with env overrides\n",
            "    DEFAULT_SENTIMENT_MODEL = os.getenv(\n",
            "        \"SENTIMENT_MODEL\", \n",
            "        \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
            "    )\n",
            "    \n",
            "    # Performance settings\n",
            "    BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"32\"))\n",
            "    MAX_LENGTH = int(os.getenv(\"MAX_LENGTH\", \"512\"))\n",
            "    ENABLE_FLASH_ATTENTION = os.getenv(\"ENABLE_FLASH_ATTENTION\", \"true\").lower() == \"true\"\n",
            "    \n",
            "    # Directory management with auto-creation\n",
            "    DATA_PATH = Path(os.getenv(\"DATA_PATH\", \"./data\"))\n",
            "    CACHE_DIR = Path(os.getenv(\"CACHE_DIR\", \"./cache\"))\n",
            "    \n",
            "    @classmethod\n",
            "    def display(cls):\n",
            "        \"\"\"Display current configuration.\"\"\"\n",
            "        print(\"Current Configuration:\")\n",
            "        print(f\"  Device: {cls.DEVICE}\")\n",
            "        print(f\"  Default Model: {cls.DEFAULT_SENTIMENT_MODEL}\")\n",
            "        print(f\"  Batch Size: {cls.BATCH_SIZE}\")\n",
            "        print(f\"  Max Length: {cls.MAX_LENGTH}\")\n",
            "        print(f\"  Flash Attention: {cls.ENABLE_FLASH_ATTENTION}\")\n",
            "        print(f\"  Data Path: {cls.DATA_PATH}\")\n",
            "        print(f\"  Cache Dir: {cls.CACHE_DIR}\")\n",
            "\n",
            "# Display configuration\n",
            "Config.display()"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Performance Benchmarking\n",
            "\n",
            "Create a simple benchmarking utility."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "def benchmark_pipeline(pipeline_func, inputs: List[str], name: str = \"Pipeline\") -> Dict[str, float]:\n",
            "    \"\"\"Benchmark a pipeline with various metrics.\"\"\"\n",
            "    print(f\"\\nBenchmarking {name}...\")\n",
            "    \n",
            "    # Warmup\n",
            "    _ = pipeline_func(inputs[0])\n",
            "    \n",
            "    # Single inference\n",
            "    start = time.time()\n",
            "    for inp in inputs[:10]:\n",
            "        _ = pipeline_func(inp)\n",
            "    single_time = time.time() - start\n",
            "    \n",
            "    # Batch inference\n",
            "    start = time.time()\n",
            "    _ = pipeline_func(inputs)\n",
            "    batch_time = time.time() - start\n",
            "    \n",
            "    metrics = {\n",
            "        \"single_latency_ms\": (single_time / 10) * 1000,\n",
            "        \"batch_latency_ms\": (batch_time / len(inputs)) * 1000,\n",
            "        \"throughput_single\": 10 / single_time,\n",
            "        \"throughput_batch\": len(inputs) / batch_time,\n",
            "        \"speedup\": (single_time / 10 * len(inputs)) / batch_time\n",
            "    }\n",
            "    \n",
            "    print(f\"  Single inference: {metrics['single_latency_ms']:.1f} ms/sample\")\n",
            "    print(f\"  Batch inference: {metrics['batch_latency_ms']:.1f} ms/sample\")\n",
            "    print(f\"  Throughput (single): {metrics['throughput_single']:.1f} samples/sec\")\n",
            "    print(f\"  Throughput (batch): {metrics['throughput_batch']:.1f} samples/sec\")\n",
            "    print(f\"  Batch speedup: {metrics['speedup']:.1f}x\")\n",
            "    \n",
            "    return metrics\n",
            "\n",
            "# Benchmark our sentiment pipeline\n",
            "test_inputs = [f\"Test review number {i}. This is a sample text.\" for i in range(50)]\n",
            "metrics = benchmark_pipeline(clf, test_inputs, \"Sentiment Analysis\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## Summary and Key Takeaways\n",
            "\n",
            "In this tutorial notebook, we've covered:\n",
            "\n",
            "1. **Pipeline Customization**: From basic usage to custom preprocessing and component composition\n",
            "2. **Efficient Data Handling**: Using ðŸ¤— Datasets for scalable data processing\n",
            "3. **Optimization Techniques**: Batching, quantization, and memory tracking\n",
            "4. **Synthetic Data Generation**: Creating and validating synthetic training data\n",
            "5. **Production Workflows**: Building robust, scalable systems for real-world deployment\n",
            "\n",
            "### Key Performance Gains:\n",
            "- **Batching**: 5-10x throughput improvement\n",
            "- **Quantization**: 75% model size reduction\n",
            "- **Streaming**: Handle datasets larger than memory\n",
            "- **PEFT/LoRA**: Train with 0.1% of parameters\n",
            "\n",
            "### Next Steps:\n",
            "1. Experiment with different models and batch sizes\n",
            "2. Implement quantization on compatible hardware\n",
            "3. Build custom pipelines for your specific use case\n",
            "4. Explore synthetic data generation for your domain\n",
            "\n",
            "**Remember**: Great AI isn't about using the fanciest models. It's about building robust, efficient workflows that solve real problems!"
           ]
          }
         ],
         "metadata": {
          "kernelspec": {
           "display_name": "Python 3 (ipykernel)",
           "language": "python",
           "name": "python3"
          },
          "language_info": {
           "codemirror_mode": {
            "name": "ipython",
            "version": 3
           },
           "file_extension": ".py",
           "mimetype": "text/x-python",
           "name": "python",
           "nbconvert_exporter": "python",
           "pygments_lexer": "ipython3",
           "version": "3.12.9"
          }
         },
         "nbformat": 4,
         "nbformat_minor": 4
        }
      metadata:
        extension: .ipynb
        size_bytes: 52996
        language: ipynb
    notebooks/.ipynb_checkpoints/pipeline_exploration-checkpoint.ipynb:
      content: |
        {
         "cells": [
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "# Pipeline Exploration Notebook\n",
            "\n",
            "This notebook provides interactive examples for exploring Hugging Face pipelines."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "import sys\n",
            "sys.path.append('..')\n",
            "\n",
            "from transformers import pipeline, logging\n",
            "import torch\n",
            "from src.config import get_device, DEFAULT_SENTIMENT_MODEL"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## 1. Basic Pipeline Usage"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Create a simple pipeline\n",
            "device = get_device()\n",
            "print(f\"Using device: {device}\")\n",
            "\n",
            "clf = pipeline(\n",
            "    'sentiment-analysis',\n",
            "    model=DEFAULT_SENTIMENT_MODEL,\n",
            "    device=0 if device == 'cuda' else -1\n",
            ")"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Test the pipeline\n",
            "texts = [\n",
            "    \"I love this product!\",\n",
            "    \"This is terrible.\",\n",
            "    \"It's okay, not great.\"\n",
            "]\n",
            "\n",
            "results = clf(texts)\n",
            "for text, result in zip(texts, results):\n",
            "    print(f\"{text}: {result}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## 2. Pipeline Internals"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Inspect pipeline components\n",
            "print(\"Model architecture:\")\n",
            "print(clf.model)\n",
            "\n",
            "print(\"\\nTokenizer info:\")\n",
            "print(f\"Vocab size: {clf.tokenizer.vocab_size}\")\n",
            "print(f\"Max length: {clf.tokenizer.model_max_length}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## 3. Custom Pipeline Creation"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "from src.custom_pipelines import CustomSentimentPipeline\n",
            "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
            "\n",
            "# Create custom pipeline\n",
            "model = AutoModelForSequenceClassification.from_pretrained(DEFAULT_SENTIMENT_MODEL)\n",
            "tokenizer = AutoTokenizer.from_pretrained(DEFAULT_SENTIMENT_MODEL)\n",
            "\n",
            "custom_pipe = CustomSentimentPipeline(\n",
            "    model=model,\n",
            "    tokenizer=tokenizer,\n",
            "    device=0 if device == 'cuda' else -1\n",
            ")\n",
            "\n",
            "# Test with messy input\n",
            "messy_texts = [\n",
            "    \"<p>AMAZING PRODUCT!!!</p>\",\n",
            "    \"terrible... just terrible!!!!!!\",\n",
            "    \"   Good value   \"\n",
            "]\n",
            "\n",
            "custom_results = custom_pipe(messy_texts)\n",
            "for text, result in zip(messy_texts, custom_results):\n",
            "    print(f\"\\nInput: {text}\")\n",
            "    print(f\"Result: {result}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## 4. Performance Comparison"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "import time\n",
            "\n",
            "# Generate test data\n",
            "test_texts = [\"This is a test sentence.\"] * 100\n",
            "\n",
            "# Test different batch sizes\n",
            "batch_sizes = [1, 8, 16, 32]\n",
            "\n",
            "for batch_size in batch_sizes:\n",
            "    start = time.time()\n",
            "    _ = clf(test_texts, batch_size=batch_size)\n",
            "    end = time.time()\n",
            "    \n",
            "    throughput = len(test_texts) / (end - start)\n",
            "    print(f\"Batch size {batch_size}: {throughput:.1f} samples/sec\")"
           ]
          }
         ],
         "metadata": {
          "kernelspec": {
           "display_name": "Python 3",
           "language": "python",
           "name": "python3"
          }
         },
         "nbformat": 4,
         "nbformat_minor": 4
        }
      metadata:
        extension: .ipynb
        size_bytes: 4077
        language: ipynb
    notebooks/.ipynb_checkpoints/optimization_benchmarks-checkpoint.ipynb:
      content: |
        {
         "cells": [
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "# Model Optimization Benchmarks\n",
            "\n",
            "This notebook demonstrates various optimization techniques and their impact."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "import sys\n",
            "sys.path.append('..')\n",
            "\n",
            "import torch\n",
            "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
            "from src.optimization import benchmark_inference\n",
            "from src.utils import calculate_model_size, MemoryTracker\n",
            "import matplotlib.pyplot as plt\n",
            "import numpy as np"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## 1. Load Models"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Load base model\n",
            "model_name = \"bert-base-uncased\"\n",
            "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
            "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
            "\n",
            "# Model info\n",
            "model_info = calculate_model_size(model)\n",
            "print(f\"Model: {model_name}\")\n",
            "print(f\"Size: {model_info['total_size_mb']:.1f}MB\")\n",
            "print(f\"Parameters: {model_info['total_params']:,}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## 2. Quantization Comparison"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Test texts\n",
            "test_texts = [\n",
            "    \"This is a positive review.\",\n",
            "    \"This is a negative review.\",\n",
            "    \"This is a neutral statement.\"\n",
            "] * 10\n",
            "\n",
            "# Benchmark FP32\n",
            "fp32_time, fp32_mem = benchmark_inference(\n",
            "    model, tokenizer, test_texts, \"FP32 Model\"\n",
            ")\n",
            "\n",
            "# Quantize and benchmark\n",
            "if torch.cuda.is_available():\n",
            "    # FP16\n",
            "    model_fp16 = model.half()\n",
            "    fp16_time, fp16_mem = benchmark_inference(\n",
            "        model_fp16, tokenizer, test_texts, \"FP16 Model\"\n",
            "    )\n",
            "else:\n",
            "    # INT8 for CPU\n",
            "    model_int8 = torch.quantization.quantize_dynamic(\n",
            "        model, {torch.nn.Linear}, dtype=torch.qint8\n",
            "    )\n",
            "    int8_time, int8_mem = benchmark_inference(\n",
            "        model_int8, tokenizer, test_texts, \"INT8 Model\"\n",
            "    )"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## 3. Batch Size Impact"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Test different batch sizes\n",
            "batch_sizes = [1, 2, 4, 8, 16, 32]\n",
            "throughputs = []\n",
            "\n",
            "for batch_size in batch_sizes:\n",
            "    test_batch = test_texts[:batch_size]\n",
            "    time_taken, _ = benchmark_inference(\n",
            "        model, tokenizer, test_batch, f\"Batch size {batch_size}\"\n",
            "    )\n",
            "    throughput = len(test_batch) / time_taken\n",
            "    throughputs.append(throughput)\n",
            "\n",
            "# Plot results\n",
            "plt.figure(figsize=(10, 6))\n",
            "plt.plot(batch_sizes, throughputs, 'b-o')\n",
            "plt.xlabel('Batch Size')\n",
            "plt.ylabel('Throughput (samples/sec)')\n",
            "plt.title('Throughput vs Batch Size')\n",
            "plt.grid(True)\n",
            "plt.show()"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## 4. Memory Profiling"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Profile memory usage\n",
            "with MemoryTracker() as tracker:\n",
            "    inputs = tokenizer(\n",
            "        test_texts, \n",
            "        padding=True, \n",
            "        truncation=True,\n",
            "        return_tensors=\"pt\"\n",
            "    )\n",
            "    \n",
            "    if torch.cuda.is_available():\n",
            "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
            "        model = model.cuda()\n",
            "    \n",
            "    with torch.no_grad():\n",
            "        outputs = model(**inputs)\n",
            "\n",
            "print(f\"Peak memory usage: {tracker.get_memory_used():.2f}MB\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## 5. Optimization Summary"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Create comparison chart\n",
            "techniques = ['FP32', 'FP16/INT8', 'Batching']\n",
            "speedups = [1.0, 2.0, 5.0]  # Example values\n",
            "\n",
            "plt.figure(figsize=(10, 6))\n",
            "bars = plt.bar(techniques, speedups, color=['blue', 'green', 'orange'])\n",
            "plt.ylabel('Speedup Factor')\n",
            "plt.title('Optimization Technique Comparison')\n",
            "plt.ylim(0, 6)\n",
            "\n",
            "# Add value labels on bars\n",
            "for bar, speedup in zip(bars, speedups):\n",
            "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
            "             f'{speedup}x', ha='center', va='bottom')\n",
            "\n",
            "plt.show()"
           ]
          }
         ],
         "metadata": {
          "kernelspec": {
           "display_name": "Python 3",
           "language": "python",
           "name": "python3"
          }
         },
         "nbformat": 4,
         "nbformat_minor": 4
        }
      metadata:
        extension: .ipynb
        size_bytes: 5383
        language: ipynb
    notebooks/.ipynb_checkpoints/tutorial-checkpoint.ipynb:
      content: |-
        {
         "cells": [
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "# Customizing Pipelines and Data Workflows: Advanced Models and Efficient Processing\n",
            "\n",
            "This notebook contains all examples from Chapter 8 with step-by-step explanations.\n",
            "\n",
            "## Table of Contents\n",
            "1. [Environment Setup](#environment-setup)\n",
            "2. [Pipeline Basics and Customization](#pipeline-basics)\n",
            "3. [Efficient Data Handling](#data-handling)\n",
            "4. [Optimization Techniques](#optimization)\n",
            "5. [Synthetic Data Generation](#synthetic-data)\n",
            "6. [Production Workflows](#production)\n",
            "\n",
            "---"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## Environment Setup <a id='environment-setup'></a>\n",
            "\n",
            "First, let's set up our environment with all necessary imports and configurations."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Core imports\n",
            "import os\n",
            "import sys\n",
            "import time\n",
            "import torch\n",
            "import numpy as np\n",
            "from pathlib import Path\n",
            "from typing import List, Dict, Any\n",
            "from contextlib import contextmanager\n",
            "\n",
            "# Add src to path for local imports\n",
            "sys.path.append('../src')\n",
            "\n",
            "# Transformers imports\n",
            "from transformers import (\n",
            "    pipeline,\n",
            "    Pipeline,\n",
            "    AutoModel,\n",
            "    AutoTokenizer,\n",
            "    AutoModelForSequenceClassification,\n",
            "    AutoModelForCausalLM\n",
            ")\n",
            "from transformers.pipelines import register_pipeline\n",
            "from transformers.utils import logging\n",
            "\n",
            "# Datasets\n",
            "from datasets import load_dataset, Dataset\n",
            "\n",
            "# For optimization examples\n",
            "from peft import LoraConfig, get_peft_model, TaskType\n",
            "\n",
            "# For synthetic data generation\n",
            "from diffusers import DiffusionPipeline\n",
            "\n",
            "print(\"Environment ready!\")\n",
            "print(f\"PyTorch version: {torch.__version__}\")\n",
            "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
            "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Device Configuration\n",
            "\n",
            "Let's implement the cross-platform device detection from the chapter."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "def get_optimal_device() -> torch.device:\n",
            "    \"\"\"Automatically detect best available device.\"\"\"\n",
            "    if torch.cuda.is_available():\n",
            "        return torch.device(\"cuda\")\n",
            "    elif torch.backends.mps.is_available():  # Apple Silicon\n",
            "        return torch.device(\"mps\")\n",
            "    else:\n",
            "        return torch.device(\"cpu\")\n",
            "\n",
            "DEVICE = get_optimal_device()\n",
            "print(f\"Using device: {DEVICE}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## Pipeline Basics and Customization <a id='pipeline-basics'></a>\n",
            "\n",
            "### Quick Start: Modern Pipeline Usage\n",
            "\n",
            "Let's start with the basic pipeline example from the chapter."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Modern quick-start with explicit model and device\n",
            "clf = pipeline(\n",
            "    'sentiment-analysis',\n",
            "    model='distilbert-base-uncased-finetuned-sst-2-english',\n",
            "    device=0 if DEVICE.type == \"cuda\" else -1  # 0 for CUDA GPU, -1 for CPU\n",
            ")\n",
            "\n",
            "# Run prediction on text\n",
            "result = clf('I love Hugging Face!')\n",
            "print(result)\n",
            "# Expected output: [{'label': 'POSITIVE', 'score': 0.9998}]\n",
            "\n",
            "# Let's try multiple examples\n",
            "texts = [\n",
            "    \"I love this product!\",\n",
            "    \"This is terrible.\",\n",
            "    \"Not bad, but could be better.\"\n",
            "]\n",
            "results = clf(texts)\n",
            "for text, result in zip(texts, results):\n",
            "    print(f\"Text: '{text}'\")\n",
            "    print(f\"  Sentiment: {result['label']} (confidence: {result['score']:.3f})\\n\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Custom Preprocessing\n",
            "\n",
            "Now let's add custom preprocessing to normalize text before inference."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "def custom_preprocess(text):\n",
            "    \"\"\"Normalize text for consistent predictions.\"\"\"\n",
            "    import string\n",
            "    text = text.lower()\n",
            "    return text.translate(str.maketrans('', '', string.punctuation))\n",
            "\n",
            "# Test preprocessing\n",
            "texts = [\"Wow! Amazing product!!!\", \"I don't like this...\"]\n",
            "print(\"Original texts:\", texts)\n",
            "\n",
            "# Clean then predict\n",
            "cleaned = [custom_preprocess(t) for t in texts]\n",
            "print(\"Cleaned texts:\", cleaned)\n",
            "\n",
            "# Batch processing for speed\n",
            "results = clf(cleaned, batch_size=16)\n",
            "print(\"\\nResults after preprocessing:\")\n",
            "for original, clean, result in zip(texts, cleaned, results):\n",
            "    print(f\"Original: '{original}'\")\n",
            "    print(f\"Cleaned: '{clean}'\")\n",
            "    print(f\"Result: {result}\\n\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Advanced: Pipeline Subclassing\n",
            "\n",
            "Create a reusable pipeline with built-in preprocessing and postprocessing."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "class CustomSentimentPipeline(Pipeline):\n",
            "    def preprocess(self, inputs):\n",
            "        \"\"\"Strip HTML, normalize text.\"\"\"\n",
            "        if isinstance(inputs, str):\n",
            "            text = inputs.lower()\n",
            "            import string\n",
            "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
            "            return super().preprocess(text)\n",
            "        return super().preprocess(inputs)\n",
            "    \n",
            "    def postprocess(self, outputs):\n",
            "        \"\"\"Add confidence thresholds.\"\"\"\n",
            "        results = super().postprocess(outputs)\n",
            "        for r in results:\n",
            "            r['confident'] = r['score'] > 0.95\n",
            "        return results\n",
            "\n",
            "# Note: In practice, you would register and use this custom pipeline\n",
            "# For now, let's demonstrate the concept with the standard pipeline\n",
            "print(\"Custom pipeline concept demonstrated!\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Inspecting Pipeline Components\n",
            "\n",
            "Let's peek under the hood to understand pipeline anatomy."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Inspect pipeline components\n",
            "print('Model:', type(clf.model).__name__)\n",
            "print('Tokenizer:', type(clf.tokenizer).__name__)  \n",
            "print('Processor:', getattr(clf, 'processor', None))\n",
            "print('Framework:', clf.framework)\n",
            "print('Device:', clf.device)\n",
            "\n",
            "# Let's look at model details\n",
            "print(f\"\\nModel architecture: {clf.model.config.model_type}\")\n",
            "print(f\"Hidden size: {clf.model.config.hidden_size}\")\n",
            "print(f\"Number of labels: {clf.model.config.num_labels}\")\n",
            "\n",
            "# Tokenizer details\n",
            "print(f\"\\nTokenizer vocab size: {clf.tokenizer.vocab_size}\")\n",
            "print(f\"Max length: {clf.tokenizer.model_max_length}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Composing Multiple Pipelines\n",
            "\n",
            "Let's create a combined sentiment + NER pipeline."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Load individual pipelines\n",
            "sentiment_pipe = pipeline('sentiment-analysis')\n",
            "ner_pipe = pipeline('ner')\n",
            "\n",
            "def combined_analysis(text):\n",
            "    \"\"\"Combine sentiment and NER analysis.\"\"\"\n",
            "    sentiment = sentiment_pipe(text)\n",
            "    entities = ner_pipe(text)\n",
            "    \n",
            "    return {\n",
            "        \"text\": text,\n",
            "        \"sentiment\": sentiment[0],\n",
            "        \"entities\": entities\n",
            "    }\n",
            "\n",
            "# Test combined analysis\n",
            "test_text = \"Apple Inc. makes amazing products! I love my iPhone.\"\n",
            "result = combined_analysis(test_text)\n",
            "\n",
            "print(f\"Text: {result['text']}\")\n",
            "print(f\"\\nSentiment: {result['sentiment']['label']} ({result['sentiment']['score']:.3f})\")\n",
            "print(\"\\nEntities found:\")\n",
            "for entity in result['entities']:\n",
            "    print(f\"  - {entity['word']}: {entity['entity_group'] if 'entity_group' in entity else entity['entity']}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Debugging Pipelines\n",
            "\n",
            "Enable verbose logging to debug pipeline issues."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Enable debug logging\n",
            "logging.set_verbosity_debug()\n",
            "\n",
            "# Now pipeline operations will show detailed logs\n",
            "print(\"Debug mode enabled. Running pipeline...\")\n",
            "result = clf(\"Debug me!\")\n",
            "print(f\"\\nResult: {result}\")\n",
            "\n",
            "# Reset logging to normal\n",
            "logging.set_verbosity_warning()\n",
            "print(\"\\nLogging reset to normal level.\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## Efficient Data Handling with ðŸ¤— Datasets <a id='data-handling'></a>\n",
            "\n",
            "### Loading and Transforming Datasets"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Load a small dataset for demonstration\n",
            "dataset = load_dataset('imdb', split='train[:1000]')  # Load only first 1000 examples\n",
            "print(f\"Dataset size: {len(dataset)}\")\n",
            "print(f\"First example: {dataset[0]}\")\n",
            "print(f\"\\nFeatures: {dataset.features}\")"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Define preprocessing function\n",
            "def preprocess_batch(batch):\n",
            "    \"\"\"Process entire batches at once.\"\"\"\n",
            "    batch['text'] = [text.lower() for text in batch['text']]\n",
            "    batch['length'] = [len(text.split()) for text in batch['text']]\n",
            "    return batch\n",
            "\n",
            "# Transform with parallel processing\n",
            "print(\"Transforming dataset...\")\n",
            "start_time = time.time()\n",
            "dataset = dataset.map(preprocess_batch, batched=True, num_proc=4)\n",
            "print(f\"Transformation completed in {time.time() - start_time:.2f} seconds\")\n",
            "\n",
            "# Filter short reviews\n",
            "print(f\"\\nDataset before filtering: {len(dataset)} examples\")\n",
            "dataset = dataset.filter(lambda x: x['length'] > 20)\n",
            "print(f\"Dataset after filtering: {len(dataset)} examples\")\n",
            "\n",
            "# Check the new features\n",
            "print(f\"\\nUpdated features: {dataset.features}\")\n",
            "print(f\"Example with new features: {dataset[0]}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Streaming Large Datasets\n",
            "\n",
            "For massive datasets, use streaming to avoid memory issues."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Create a sample CSV for demonstration\n",
            "import csv\n",
            "\n",
            "sample_data = [\n",
            "    {\"text\": \"This product is amazing!\", \"label\": \"positive\"},\n",
            "    {\"text\": \"Terrible experience.\", \"label\": \"negative\"},\n",
            "    {\"text\": \"Good value for money.\", \"label\": \"positive\"},\n",
            "    {\"text\": \"Not worth the price.\", \"label\": \"negative\"},\n",
            "    {\"text\": \"Excellent quality!\", \"label\": \"positive\"}\n",
            "] * 20  # Repeat for larger dataset\n",
            "\n",
            "csv_path = \"sample_reviews.csv\"\n",
            "with open(csv_path, 'w', newline='') as f:\n",
            "    writer = csv.DictWriter(f, fieldnames=['text', 'label'])\n",
            "    writer.writeheader()\n",
            "    writer.writerows(sample_data)\n",
            "\n",
            "# Stream the dataset\n",
            "streaming_dataset = load_dataset('csv', data_files=csv_path, split='train', streaming=True)\n",
            "\n",
            "# Process in batches\n",
            "batch_size = 32\n",
            "batch = []\n",
            "processed_count = 0\n",
            "\n",
            "print(\"Processing streaming dataset...\")\n",
            "for example in streaming_dataset:\n",
            "    batch.append(custom_preprocess(example['text']))\n",
            "    \n",
            "    if len(batch) == batch_size:\n",
            "        # Process batch\n",
            "        results = clf(batch, batch_size=batch_size)\n",
            "        processed_count += len(batch)\n",
            "        print(f\"Processed {processed_count} examples...\")\n",
            "        batch = []\n",
            "    \n",
            "    # Stop after processing 100 examples for demo\n",
            "    if processed_count >= 96:\n",
            "        break\n",
            "\n",
            "# Process remaining batch\n",
            "if batch:\n",
            "    results = clf(batch)\n",
            "    processed_count += len(batch)\n",
            "\n",
            "print(f\"\\nTotal processed: {processed_count} examples\")\n",
            "\n",
            "# Clean up\n",
            "os.remove(csv_path)"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Creating Custom Datasets"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Create a custom dataset from dictionaries\n",
            "custom_data = {\n",
            "    \"text\": [\n",
            "        \"The future of AI is bright and full of possibilities.\",\n",
            "        \"Machine learning transforms how we solve complex problems.\",\n",
            "        \"Deep learning models continue to improve rapidly.\",\n",
            "        \"Natural language processing enables better human-computer interaction.\",\n",
            "        \"Computer vision applications are becoming more sophisticated.\"\n",
            "    ],\n",
            "    \"category\": [\"future\", \"ml\", \"dl\", \"nlp\", \"cv\"]\n",
            "}\n",
            "\n",
            "custom_dataset = Dataset.from_dict(custom_data)\n",
            "print(f\"Custom dataset: {custom_dataset}\")\n",
            "print(f\"\\nFirst example: {custom_dataset[0]}\")\n",
            "\n",
            "# Apply transformations\n",
            "def add_metadata(example):\n",
            "    example['word_count'] = len(example['text'].split())\n",
            "    example['char_count'] = len(example['text'])\n",
            "    return example\n",
            "\n",
            "custom_dataset = custom_dataset.map(add_metadata)\n",
            "print(f\"\\nAfter transformation: {custom_dataset[0]}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## Optimization Techniques <a id='optimization'></a>\n",
            "\n",
            "### Batching for 10x Throughput"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Prepare test data\n",
            "test_texts = [\n",
            "    \"Review 1: This product exceeded my expectations.\",\n",
            "    \"Review 2: Not satisfied with the quality.\",\n",
            "    \"Review 3: Average product, nothing special.\",\n",
            "    \"Review 4: Absolutely love it!\",\n",
            "    \"Review 5: Waste of money.\",\n",
            "    \"Review 6: Good value for the price.\",\n",
            "    \"Review 7: Would recommend to friends.\",\n",
            "    \"Review 8: Poor customer service.\"\n",
            "] * 4  # Repeat for more examples\n",
            "\n",
            "# Method 1: One by one (slow)\n",
            "print(\"Method 1: Processing one by one...\")\n",
            "start_time = time.time()\n",
            "results_single = []\n",
            "for text in test_texts[:8]:  # Process only first 8 for demo\n",
            "    result = clf(text)\n",
            "    results_single.append(result)\n",
            "single_time = time.time() - start_time\n",
            "print(f\"Time taken: {single_time:.3f} seconds\")\n",
            "print(f\"Average per text: {single_time/8*1000:.1f} ms\")\n",
            "\n",
            "# Method 2: Batch processing (fast)\n",
            "print(\"\\nMethod 2: Batch processing...\")\n",
            "start_time = time.time()\n",
            "results_batch = clf(test_texts, \n",
            "                   padding=True,\n",
            "                   truncation=True,\n",
            "                   max_length=128)\n",
            "batch_time = time.time() - start_time\n",
            "print(f\"Time taken: {batch_time:.3f} seconds\")\n",
            "print(f\"Average per text: {batch_time/len(test_texts)*1000:.1f} ms\")\n",
            "\n",
            "# Calculate speedup\n",
            "speedup = (single_time/8*len(test_texts)) / batch_time\n",
            "print(f\"\\nSpeedup: {speedup:.1f}x faster with batching!\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Modern Quantization\n",
            "\n",
            "Demonstrate quantization for cost reduction (requires appropriate hardware)."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Load a small model for demonstration\n",
            "model_name = \"distilbert-base-uncased\"\n",
            "\n",
            "print(\"Loading standard model...\")\n",
            "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
            "\n",
            "# Calculate model size\n",
            "param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
            "buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
            "model_size = param_size + buffer_size\n",
            "print(f\"Standard model size: {model_size / 1024 / 1024:.1f} MB\")\n",
            "\n",
            "# Count parameters\n",
            "total_params = sum(p.numel() for p in model.parameters())\n",
            "print(f\"Total parameters: {total_params:,}\")\n",
            "\n",
            "# Note: Actual quantization requires bitsandbytes library and compatible GPU\n",
            "# This is a conceptual demonstration\n",
            "print(\"\\nQuantization options:\")\n",
            "print(\"- INT8: ~75% size reduction, minimal accuracy loss\")\n",
            "print(\"- INT4: ~87.5% size reduction, may require fine-tuning\")\n",
            "print(\"- Dynamic quantization: Adapts to input ranges\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Memory Tracking Utility\n",
            "\n",
            "Implement the memory tracking context manager from the chapter."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "@contextmanager\n",
            "def track_memory(device: str = \"cuda\"):\n",
            "    \"\"\"Context manager for GPU memory profiling.\"\"\"\n",
            "    if device == \"cuda\" and torch.cuda.is_available():\n",
            "        torch.cuda.synchronize()\n",
            "        start_memory = torch.cuda.memory_allocated()\n",
            "        yield\n",
            "        torch.cuda.synchronize()\n",
            "        end_memory = torch.cuda.memory_allocated()\n",
            "        memory_used = end_memory - start_memory\n",
            "        print(f\"Memory used: {memory_used / 1024 / 1024:.2f} MB\")\n",
            "    else:\n",
            "        yield\n",
            "        print(\"Memory tracking only available for CUDA devices\")\n",
            "\n",
            "# Example usage\n",
            "print(\"Testing memory tracking...\")\n",
            "with track_memory(device=DEVICE.type):\n",
            "    # Run some inference\n",
            "    _ = clf(\"Test text for memory tracking\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### PEFT/LoRA Concept\n",
            "\n",
            "Demonstrate Parameter-Efficient Fine-Tuning concepts."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# PEFT configuration example (conceptual)\n",
            "from peft import LoraConfig, TaskType\n",
            "\n",
            "# Define LoRA configuration\n",
            "peft_config = LoraConfig(\n",
            "    task_type=TaskType.SEQ_CLS,  # Sequence classification\n",
            "    r=16,  # LoRA rank\n",
            "    lora_alpha=32,\n",
            "    lora_dropout=0.1,\n",
            "    target_modules=[\"query\", \"value\"]  # Target attention layers\n",
            ")\n",
            "\n",
            "print(\"LoRA Configuration:\")\n",
            "print(f\"  Rank (r): {peft_config.r}\")\n",
            "print(f\"  Alpha: {peft_config.lora_alpha}\")\n",
            "print(f\"  Dropout: {peft_config.lora_dropout}\")\n",
            "print(f\"  Target modules: {peft_config.target_modules}\")\n",
            "\n",
            "# Calculate approximate trainable parameters\n",
            "# For BERT-base with r=16:\n",
            "# Original: ~110M parameters\n",
            "# LoRA trainable: ~0.3M parameters (0.3% of original)\n",
            "print(\"\\nParameter efficiency:\")\n",
            "print(\"  Original BERT-base: ~110M parameters\")\n",
            "print(\"  LoRA trainable: ~0.3M parameters\")\n",
            "print(\"  Reduction: 99.7% fewer trainable parameters!\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## Synthetic Data Generation <a id='synthetic-data'></a>\n",
            "\n",
            "### Text Generation with LLMs"
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "# Load a small text generation model\n",
            "print(\"Loading text generation model...\")\n",
            "gen = pipeline(\n",
            "    'text-generation',\n",
            "    model='gpt2',  # Using smaller model for demo\n",
            "    device=0 if DEVICE.type == \"cuda\" else -1\n",
            ")\n",
            "\n",
            "# Generate product reviews\n",
            "prompts = [\n",
            "    \"This smartphone is\",\n",
            "    \"The laptop performance is\",\n",
            "    \"Customer service was\"\n",
            "]\n",
            "\n",
            "print(\"Generating synthetic reviews...\\n\")\n",
            "for prompt in prompts:\n",
            "    generated = gen(\n",
            "        prompt,\n",
            "        max_new_tokens=30,\n",
            "        num_return_sequences=2,\n",
            "        temperature=0.8,\n",
            "        pad_token_id=gen.tokenizer.eos_token_id\n",
            "    )\n",
            "    \n",
            "    print(f\"Prompt: '{prompt}'\")\n",
            "    for i, g in enumerate(generated):\n",
            "        print(f\"  Generated {i+1}: {g['generated_text']}\")\n",
            "    print()"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Synthetic Data Validation\n",
            "\n",
            "Implement quality checks for synthetic data."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "def validate_synthetic_text(texts: List[str]) -> Dict[str, Any]:\n",
            "    \"\"\"Basic validation for synthetic text data.\"\"\"\n",
            "    results = {\n",
            "        \"total\": len(texts),\n",
            "        \"valid\": 0,\n",
            "        \"issues\": []\n",
            "    }\n",
            "    \n",
            "    for text in texts:\n",
            "        issues = []\n",
            "        \n",
            "        # Check length\n",
            "        if len(text.split()) < 5:\n",
            "            issues.append(\"too_short\")\n",
            "        elif len(text.split()) > 200:\n",
            "            issues.append(\"too_long\")\n",
            "        \n",
            "        # Check for repetition\n",
            "        words = text.lower().split()\n",
            "        if len(words) > 0 and len(set(words)) / len(words) < 0.5:\n",
            "            issues.append(\"repetitive\")\n",
            "        \n",
            "        # Check for truncation\n",
            "        if text.endswith(\"...\") or not text.endswith(('.', '!', '?')):\n",
            "            issues.append(\"truncated\")\n",
            "        \n",
            "        if not issues:\n",
            "            results[\"valid\"] += 1\n",
            "        else:\n",
            "            results[\"issues\"].extend(issues)\n",
            "    \n",
            "    results[\"validity_rate\"] = results[\"valid\"] / results[\"total\"]\n",
            "    return results\n",
            "\n",
            "# Test validation\n",
            "synthetic_samples = [\n",
            "    \"This product is amazing and works perfectly!\",\n",
            "    \"Good good good good good.\",\n",
            "    \"The laptop\",\n",
            "    \"Excellent quality and fast shipping. Would buy again.\",\n",
            "    \"This is a test that ends abruptly and\"\n",
            "]\n",
            "\n",
            "validation_results = validate_synthetic_text(synthetic_samples)\n",
            "print(\"Validation Results:\")\n",
            "print(f\"  Total samples: {validation_results['total']}\")\n",
            "print(f\"  Valid samples: {validation_results['valid']}\")\n",
            "print(f\"  Validity rate: {validation_results['validity_rate']:.1%}\")\n",
            "print(f\"  Issues found: {set(validation_results['issues'])}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## Production Workflows <a id='production'></a>\n",
            "\n",
            "### Complete Production Pipeline Example\n",
            "\n",
            "Let's implement a simplified version of the RetailReviewWorkflow from the chapter."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "class SimpleRetailWorkflow:\n",
            "    \"\"\"Simplified production workflow for retail review analysis.\"\"\"\n",
            "    \n",
            "    def __init__(self):\n",
            "        # Initialize pipelines\n",
            "        self.sentiment_pipeline = pipeline(\n",
            "            'sentiment-analysis',\n",
            "            model='distilbert-base-uncased-finetuned-sst-2-english'\n",
            "        )\n",
            "        \n",
            "        # Priority keywords for urgency detection\n",
            "        self.priority_keywords = {\n",
            "            \"urgent\": [\"broken\", \"damaged\", \"fraud\", \"stolen\", \"urgent\"],\n",
            "            \"high\": [\"terrible\", \"awful\", \"worst\", \"refund\", \"complaint\"],\n",
            "            \"medium\": [\"disappointed\", \"issue\", \"problem\", \"concern\"],\n",
            "            \"low\": [\"suggestion\", \"feedback\", \"minor\"]\n",
            "        }\n",
            "    \n",
            "    def analyze_priority(self, text: str) -> str:\n",
            "        \"\"\"Determine review priority based on keywords.\"\"\"\n",
            "        text_lower = text.lower()\n",
            "        \n",
            "        for priority, keywords in self.priority_keywords.items():\n",
            "            if any(keyword in text_lower for keyword in keywords):\n",
            "                return priority\n",
            "        \n",
            "        return \"normal\"\n",
            "    \n",
            "    def process_review(self, review: str) -> Dict[str, Any]:\n",
            "        \"\"\"Process a single review.\"\"\"\n",
            "        # Sentiment analysis\n",
            "        sentiment = self.sentiment_pipeline(review)[0]\n",
            "        \n",
            "        # Priority detection\n",
            "        priority = self.analyze_priority(review)\n",
            "        \n",
            "        return {\n",
            "            \"text\": review,\n",
            "            \"sentiment\": sentiment[\"label\"],\n",
            "            \"sentiment_score\": sentiment[\"score\"],\n",
            "            \"priority\": priority,\n",
            "            \"needs_attention\": priority in [\"urgent\", \"high\"]\n",
            "        }\n",
            "    \n",
            "    def process_batch(self, reviews: List[str]) -> Dict[str, Any]:\n",
            "        \"\"\"Process multiple reviews and generate insights.\"\"\"\n",
            "        results = [self.process_review(review) for review in reviews]\n",
            "        \n",
            "        # Generate insights\n",
            "        total = len(results)\n",
            "        urgent_count = sum(1 for r in results if r[\"priority\"] in [\"urgent\", \"high\"])\n",
            "        positive_count = sum(1 for r in results if r[\"sentiment\"] == \"POSITIVE\")\n",
            "        \n",
            "        insights = {\n",
            "            \"total_reviews\": total,\n",
            "            \"urgent_reviews\": urgent_count,\n",
            "            \"sentiment_distribution\": {\n",
            "                \"positive\": positive_count,\n",
            "                \"negative\": total - positive_count,\n",
            "                \"positive_rate\": positive_count / total if total > 0 else 0\n",
            "            },\n",
            "            \"results\": results\n",
            "        }\n",
            "        \n",
            "        return insights\n",
            "\n",
            "# Test the workflow\n",
            "workflow = SimpleRetailWorkflow()\n",
            "\n",
            "sample_reviews = [\n",
            "    \"This product is absolutely amazing! Fast shipping and great quality.\",\n",
            "    \"Terrible experience. The item arrived broken and customer service was unhelpful.\",\n",
            "    \"Good value for money, but packaging could be better.\",\n",
            "    \"URGENT: Received wrong item. Need immediate refund!\",\n",
            "    \"The product works as described. Delivery was on time.\"\n",
            "]\n",
            "\n",
            "# Process reviews\n",
            "print(\"Processing reviews...\\n\")\n",
            "start_time = time.time()\n",
            "insights = workflow.process_batch(sample_reviews)\n",
            "process_time = time.time() - start_time\n",
            "\n",
            "# Display results\n",
            "print(\"=== WORKFLOW RESULTS ===\")\n",
            "print(f\"Total reviews processed: {insights['total_reviews']}\")\n",
            "print(f\"Processing time: {process_time:.3f} seconds\")\n",
            "print(f\"\\nUrgent reviews requiring attention: {insights['urgent_reviews']}\")\n",
            "print(f\"Positive sentiment rate: {insights['sentiment_distribution']['positive_rate']:.1%}\")\n",
            "\n",
            "print(\"\\n=== DETAILED RESULTS ===\")\n",
            "for i, result in enumerate(insights['results']):\n",
            "    if result['needs_attention']:\n",
            "        print(f\"\\nâš ï¸  Review {i+1} (NEEDS ATTENTION):\")\n",
            "    else:\n",
            "        print(f\"\\nReview {i+1}:\")\n",
            "    print(f\"  Text: '{result['text'][:50]}...'\")\n",
            "    print(f\"  Sentiment: {result['sentiment']} ({result['sentiment_score']:.3f})\")\n",
            "    print(f\"  Priority: {result['priority'].upper()}\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Configuration Management\n",
            "\n",
            "Implement a configuration system with environment variable support."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "class Config:\n",
            "    \"\"\"Centralized configuration with environment fallbacks.\"\"\"\n",
            "    \n",
            "    # Device configuration with automatic detection\n",
            "    DEVICE = get_optimal_device()\n",
            "    \n",
            "    # Model configurations with env overrides\n",
            "    DEFAULT_SENTIMENT_MODEL = os.getenv(\n",
            "        \"SENTIMENT_MODEL\", \n",
            "        \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
            "    )\n",
            "    \n",
            "    # Performance settings\n",
            "    BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"32\"))\n",
            "    MAX_LENGTH = int(os.getenv(\"MAX_LENGTH\", \"512\"))\n",
            "    ENABLE_FLASH_ATTENTION = os.getenv(\"ENABLE_FLASH_ATTENTION\", \"true\").lower() == \"true\"\n",
            "    \n",
            "    # Directory management with auto-creation\n",
            "    DATA_PATH = Path(os.getenv(\"DATA_PATH\", \"./data\"))\n",
            "    CACHE_DIR = Path(os.getenv(\"CACHE_DIR\", \"./cache\"))\n",
            "    \n",
            "    @classmethod\n",
            "    def display(cls):\n",
            "        \"\"\"Display current configuration.\"\"\"\n",
            "        print(\"Current Configuration:\")\n",
            "        print(f\"  Device: {cls.DEVICE}\")\n",
            "        print(f\"  Default Model: {cls.DEFAULT_SENTIMENT_MODEL}\")\n",
            "        print(f\"  Batch Size: {cls.BATCH_SIZE}\")\n",
            "        print(f\"  Max Length: {cls.MAX_LENGTH}\")\n",
            "        print(f\"  Flash Attention: {cls.ENABLE_FLASH_ATTENTION}\")\n",
            "        print(f\"  Data Path: {cls.DATA_PATH}\")\n",
            "        print(f\"  Cache Dir: {cls.CACHE_DIR}\")\n",
            "\n",
            "# Display configuration\n",
            "Config.display()"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "### Performance Benchmarking\n",
            "\n",
            "Create a simple benchmarking utility."
           ]
          },
          {
           "cell_type": "code",
           "execution_count": null,
           "metadata": {},
           "outputs": [],
           "source": [
            "def benchmark_pipeline(pipeline_func, inputs: List[str], name: str = \"Pipeline\") -> Dict[str, float]:\n",
            "    \"\"\"Benchmark a pipeline with various metrics.\"\"\"\n",
            "    print(f\"\\nBenchmarking {name}...\")\n",
            "    \n",
            "    # Warmup\n",
            "    _ = pipeline_func(inputs[0])\n",
            "    \n",
            "    # Single inference\n",
            "    start = time.time()\n",
            "    for inp in inputs[:10]:\n",
            "        _ = pipeline_func(inp)\n",
            "    single_time = time.time() - start\n",
            "    \n",
            "    # Batch inference\n",
            "    start = time.time()\n",
            "    _ = pipeline_func(inputs)\n",
            "    batch_time = time.time() - start\n",
            "    \n",
            "    metrics = {\n",
            "        \"single_latency_ms\": (single_time / 10) * 1000,\n",
            "        \"batch_latency_ms\": (batch_time / len(inputs)) * 1000,\n",
            "        \"throughput_single\": 10 / single_time,\n",
            "        \"throughput_batch\": len(inputs) / batch_time,\n",
            "        \"speedup\": (single_time / 10 * len(inputs)) / batch_time\n",
            "    }\n",
            "    \n",
            "    print(f\"  Single inference: {metrics['single_latency_ms']:.1f} ms/sample\")\n",
            "    print(f\"  Batch inference: {metrics['batch_latency_ms']:.1f} ms/sample\")\n",
            "    print(f\"  Throughput (single): {metrics['throughput_single']:.1f} samples/sec\")\n",
            "    print(f\"  Throughput (batch): {metrics['throughput_batch']:.1f} samples/sec\")\n",
            "    print(f\"  Batch speedup: {metrics['speedup']:.1f}x\")\n",
            "    \n",
            "    return metrics\n",
            "\n",
            "# Benchmark our sentiment pipeline\n",
            "test_inputs = [f\"Test review number {i}. This is a sample text.\" for i in range(50)]\n",
            "metrics = benchmark_pipeline(clf, test_inputs, \"Sentiment Analysis\")"
           ]
          },
          {
           "cell_type": "markdown",
           "metadata": {},
           "source": [
            "## Summary and Key Takeaways\n",
            "\n",
            "In this tutorial notebook, we've covered:\n",
            "\n",
            "1. **Pipeline Customization**: From basic usage to custom preprocessing and component composition\n",
            "2. **Efficient Data Handling**: Using ðŸ¤— Datasets for scalable data processing\n",
            "3. **Optimization Techniques**: Batching, quantization, and memory tracking\n",
            "4. **Synthetic Data Generation**: Creating and validating synthetic training data\n",
            "5. **Production Workflows**: Building robust, scalable systems for real-world deployment\n",
            "\n",
            "### Key Performance Gains:\n",
            "- **Batching**: 5-10x throughput improvement\n",
            "- **Quantization**: 75% model size reduction\n",
            "- **Streaming**: Handle datasets larger than memory\n",
            "- **PEFT/LoRA**: Train with 0.1% of parameters\n",
            "\n",
            "### Next Steps:\n",
            "1. Experiment with different models and batch sizes\n",
            "2. Implement quantization on compatible hardware\n",
            "3. Build custom pipelines for your specific use case\n",
            "4. Explore synthetic data generation for your domain\n",
            "\n",
            "**Remember**: Great AI isn't about using the fanciest models. It's about building robust, efficient workflows that solve real problems!"
           ]
          }
         ],
         "metadata": {
          "kernelspec": {
           "display_name": "Python 3",
           "language": "python",
           "name": "python3"
          },
          "language_info": {
           "codemirror_mode": {
            "name": "ipython",
            "version": 3
           },
           "file_extension": ".py",
           "mimetype": "text/x-python",
           "name": "python",
           "nbconvert_exporter": "python",
           "pygments_lexer": "ipython3",
           "version": "3.12.9"
          }
         },
         "nbformat": 4,
         "nbformat_minor": 4
        }
      metadata:
        extension: .ipynb
        size_bytes: 36133
        language: ipynb
    src/diffusion_generation.py:
      content: |
        """
        Diffusion pipeline examples for synthetic image generation.

        This module demonstrates how to use Stable Diffusion and other diffusion models
        for generating synthetic training data and augmenting datasets.
        """

        import time
        from pathlib import Path
        from typing import Any

        import numpy as np
        import torch
        from diffusers import (
            DPMSolverMultistepScheduler,
            StableDiffusionImg2ImgPipeline,
            StableDiffusionPipeline,
        )
        from PIL import Image

        from src.config import Config
        from src.utils import format_size, timer_decorator


        class DiffusionGenerator:
            """Generate synthetic images using diffusion models."""

            def __init__(
                self,
                model_name: str = "runwayml/stable-diffusion-v1-5",
                device: torch.device = None,
            ):
                """Initialize diffusion generator."""
                self.model_name = model_name
                self.device = device or Config.DEVICE
                self.pipelines = {}

            def load_text2img_pipeline(
                self, enable_attention_slicing: bool = True, enable_cpu_offload: bool = False
            ) -> StableDiffusionPipeline:
                """
                Load text-to-image generation pipeline.

                Args:
                    enable_attention_slicing: Enable memory-efficient attention
                    enable_cpu_offload: Enable model CPU offloading for low VRAM

                Returns:
                    Loaded pipeline
                """
                if "text2img" in self.pipelines:
                    return self.pipelines["text2img"]

                print(f"\nðŸŽ¨ Loading text-to-image pipeline: {self.model_name}")

                # Load pipeline with optimizations
                pipe = StableDiffusionPipeline.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
                    safety_checker=None,  # Disable for performance in controlled environments
                    requires_safety_checker=False,
                )

                # Optimize for memory/speed
                if enable_attention_slicing:
                    pipe.enable_attention_slicing()
                    print("âœ… Attention slicing enabled (reduces memory)")

                if enable_cpu_offload and self.device.type == "cuda":
                    pipe.enable_model_cpu_offload()
                    print("âœ… CPU offloading enabled (for low VRAM)")
                else:
                    pipe = pipe.to(self.device)

                # Use faster scheduler
                pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)

                self.pipelines["text2img"] = pipe
                return pipe

            @timer_decorator
            def generate_images(
                self,
                prompts: list[str],
                negative_prompt: str = "",
                num_images_per_prompt: int = 1,
                num_inference_steps: int = 25,
                guidance_scale: float = 7.5,
                width: int = 512,
                height: int = 512,
                seed: int | None = None,
            ) -> list[Image.Image]:
                """
                Generate images from text prompts.

                Args:
                    prompts: List of text prompts
                    negative_prompt: What to avoid in generation
                    num_images_per_prompt: Images per prompt
                    num_inference_steps: Denoising steps
                    guidance_scale: Classifier-free guidance strength
                    width: Image width
                    height: Image height
                    seed: Random seed for reproducibility

                Returns:
                    List of generated images
                """
                pipe = self.load_text2img_pipeline()

                if seed is not None:
                    generator = torch.Generator(device=self.device).manual_seed(seed)
                else:
                    generator = None

                all_images = []

                print(f"\nðŸ–¼ï¸ Generating {len(prompts) * num_images_per_prompt} images...")

                for i, prompt in enumerate(prompts):
                    print(f"\n  Prompt {i+1}/{len(prompts)}: '{prompt[:50]}...'")

                    # Generate images
                    images = pipe(
                        prompt=prompt,
                        negative_prompt=negative_prompt,
                        num_images_per_prompt=num_images_per_prompt,
                        num_inference_steps=num_inference_steps,
                        guidance_scale=guidance_scale,
                        width=width,
                        height=height,
                        generator=generator,
                    ).images

                    all_images.extend(images)

                return all_images

            def generate_dataset_augmentation(
                self,
                categories: dict[str, list[str]],
                images_per_prompt: int = 5,
                output_dir: str = "data/synthetic_images",
            ) -> dict[str, Any]:
                """
                Generate synthetic images for dataset augmentation.

                Args:
                    categories: Dict mapping category names to prompt templates
                    images_per_prompt: Number of images per prompt
                    output_dir: Directory to save images

                Returns:
                    Generation statistics
                """
                output_path = Path(output_dir)
                output_path.mkdir(parents=True, exist_ok=True)

                print(f"\nðŸ“Š Generating synthetic dataset with {len(categories)} categories")

                stats = {"total_images": 0, "categories": {}, "generation_time": 0}

                start_time = time.time()

                for category, prompts in categories.items():
                    category_path = output_path / category
                    category_path.mkdir(exist_ok=True)

                    print(f"\nðŸ“ Category: {category}")

                    # Generate images for this category
                    images = self.generate_images(
                        prompts=prompts,
                        num_images_per_prompt=images_per_prompt,
                        num_inference_steps=20,  # Faster for bulk generation
                        guidance_scale=7.5,
                    )

                    # Save images
                    for i, img in enumerate(images):
                        img_path = category_path / f"{category}_{i:04d}.png"
                        img.save(img_path)

                    stats["categories"][category] = len(images)
                    stats["total_images"] += len(images)

                stats["generation_time"] = time.time() - start_time

                print("\nâœ… Dataset generation complete:")
                print(f"  Total images: {stats['total_images']}")
                print(f"  Time: {stats['generation_time']:.1f}s")
                print(f"  Output: {output_path}")

                return stats

            def demonstrate_advanced_techniques(self) -> dict[str, Any]:
                """Demonstrate advanced diffusion techniques."""
                print("\nðŸ”¬ Advanced Diffusion Techniques")

                results = {}

                # 1. Prompt engineering for quality
                print("\n1ï¸âƒ£ Prompt Engineering")
                quality_prompts = [
                    "professional product photo of a smartphone, studio lighting, "
                    "white background, high quality, 8k",
                    "damaged laptop with cracked screen, realistic, detailed, "
                    "product documentation photo",
                    "elegant watch on display stand, luxury product photography, "
                    "bokeh background",
                ]

                quality_images = self.generate_images(
                    prompts=quality_prompts,
                    negative_prompt="blurry, low quality, distorted, amateur",
                    num_inference_steps=30,
                    guidance_scale=8.5,
                    width=768,
                    height=768,
                )

                results["quality_generation"] = {
                    "num_images": len(quality_images),
                    "resolution": "768x768",
                    "technique": "prompt engineering + negative prompts",
                }

                # 2. Style consistency with seeds
                print("\n2ï¸âƒ£ Style Consistency")
                consistent_prompts = [
                    "smartphone in minimalist style",
                    "laptop in minimalist style",
                    "headphones in minimalist style",
                ]

                # Use same seed for consistency
                consistent_images = self.generate_images(
                    prompts=consistent_prompts, num_inference_steps=25, seed=42
                )

                results["style_consistency"] = {
                    "num_images": len(consistent_images),
                    "technique": "fixed seed + style prompt",
                }

                # 3. Memory optimization comparison
                print("\n3ï¸âƒ£ Memory Optimization")

                # Test with attention slicing
                pipe = self.load_text2img_pipeline(enable_attention_slicing=True)

                if self.device.type == "cuda":
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    mem_before = torch.cuda.memory_allocated()

                _ = pipe("test image", num_inference_steps=10, width=512, height=512).images[0]

                if self.device.type == "cuda":
                    torch.cuda.synchronize()
                    mem_with_slicing = torch.cuda.memory_allocated() - mem_before
                else:
                    mem_with_slicing = 0

                results["memory_optimization"] = {
                    "attention_slicing_memory": format_size(mem_with_slicing),
                    "optimization_enabled": True,
                }

                return results, quality_images + consistent_images

            def create_image_variations(
                self,
                base_image: Image.Image,
                variation_prompts: list[str],
                strength: float = 0.75,
            ) -> list[Image.Image]:
                """
                Create variations of an existing image using img2img.

                Args:
                    base_image: Source image
                    variation_prompts: Prompts for variations
                    strength: How much to change (0=no change, 1=complete change)

                Returns:
                    List of image variations
                """
                print("\nðŸ”„ Creating image variations...")

                # Load img2img pipeline
                if "img2img" not in self.pipelines:
                    self.pipelines["img2img"] = StableDiffusionImg2ImgPipeline.from_pretrained(
                        self.model_name,
                        torch_dtype=(
                            torch.float16 if self.device.type == "cuda" else torch.float32
                        ),
                        safety_checker=None,
                        requires_safety_checker=False,
                    ).to(self.device)

                    self.pipelines["img2img"].scheduler = (
                        DPMSolverMultistepScheduler.from_config(
                            self.pipelines["img2img"].scheduler.config
                        )
                    )

                pipe = self.pipelines["img2img"]
                variations = []

                for prompt in variation_prompts:
                    print(f"  Creating variation: '{prompt[:50]}...'")

                    image = pipe(
                        prompt=prompt,
                        image=base_image,
                        strength=strength,
                        num_inference_steps=30,
                        guidance_scale=7.5,
                    ).images[0]

                    variations.append(image)

                return variations

            def quality_filter_images(
                self, images: list[Image.Image], min_entropy: float = 5.0
            ) -> tuple[list[Image.Image], dict[str, Any]]:
                """
                Filter generated images by quality metrics.

                Args:
                    images: List of generated images
                    min_entropy: Minimum entropy threshold

                Returns:
                    Filtered images and statistics
                """
                print(f"\nðŸ” Filtering {len(images)} images by quality...")

                filtered = []
                stats = {
                    "total": len(images),
                    "passed": 0,
                    "filtered_low_entropy": 0,
                    "filtered_uniform": 0,
                }

                for img in images:
                    # Convert to numpy
                    img_array = np.array(img)

                    # Calculate entropy (measure of information content)
                    hist, _ = np.histogram(img_array, bins=256, range=(0, 256))
                    hist = hist / hist.sum()
                    entropy = -np.sum(hist[hist > 0] * np.log2(hist[hist > 0]))

                    # Check for uniform color (low variance)
                    variance = np.var(img_array)

                    if entropy < min_entropy:
                        stats["filtered_low_entropy"] += 1
                    elif variance < 100:  # Too uniform
                        stats["filtered_uniform"] += 1
                    else:
                        filtered.append(img)
                        stats["passed"] += 1

                print("\nâœ… Quality filtering complete:")
                print(f"  Passed: {stats['passed']}/{stats['total']}")
                print(f"  Low entropy: {stats['filtered_low_entropy']}")
                print(f"  Too uniform: {stats['filtered_uniform']}")

                return filtered, stats


        def demonstrate_diffusion_generation():
            """Demonstrate diffusion pipeline for synthetic data generation."""
            print("=" * 80)
            print("ðŸŽ¨ DIFFUSION GENERATION DEMONSTRATION")
            print("=" * 80)

            generator = DiffusionGenerator()

            # 1. Basic generation
            print("\n1ï¸âƒ£ Basic Text-to-Image Generation")
            basic_prompts = [
                "a professional photo of a modern smartphone on white background",
                "a laptop computer with visible keyboard, product photography",
            ]

            basic_images = generator.generate_images(
                prompts=basic_prompts, num_inference_steps=25, seed=42  # For reproducibility
            )

            # Save examples
            output_dir = Path("output/diffusion_examples")
            output_dir.mkdir(parents=True, exist_ok=True)

            for i, img in enumerate(basic_images):
                img.save(output_dir / f"basic_{i}.png")

            # 2. Dataset augmentation
            print("\n2ï¸âƒ£ Synthetic Dataset Generation")
            categories = {
                "electronics": [
                    "smartphone with cracked screen, repair documentation",
                    "pristine laptop, product catalog photo",
                    "wireless earbuds in charging case, white background",
                ],
                "furniture": [
                    "modern office chair, studio lighting",
                    "wooden desk with drawers, product photo",
                    "bookshelf filled with books, interior design",
                ],
            }

            dataset_stats = generator.generate_dataset_augmentation(
                categories=categories, images_per_prompt=2
            )

            # 3. Advanced techniques
            print("\n3ï¸âƒ£ Advanced Generation Techniques")
            advanced_results, advanced_images = generator.demonstrate_advanced_techniques()

            # 4. Quality filtering
            print("\n4ï¸âƒ£ Quality Filtering")
            all_generated = basic_images + advanced_images
            filtered_images, filter_stats = generator.quality_filter_images(all_generated)

            # 5. Image variations (if we have images)
            if filtered_images:
                print("\n5ï¸âƒ£ Creating Variations")
                variations = generator.create_image_variations(
                    base_image=filtered_images[0],
                    variation_prompts=[
                        "the same device but with a blue color scheme",
                        "the same device in a futuristic style",
                    ],
                    strength=0.7,
                )

                for i, img in enumerate(variations):
                    img.save(output_dir / f"variation_{i}.png")

            print("\n" + "=" * 80)
            print("ðŸ“Š DIFFUSION GENERATION SUMMARY")
            print("=" * 80)

            print(f"\nâœ… Generated {dataset_stats['total_images']} synthetic images")
            print(
                f"âœ… Quality filtered: {filter_stats['passed']}/{filter_stats['total']} passed"
            )
            print(f"âœ… Output saved to: {output_dir}")

            print("\nðŸ’¡ Key Applications:")
            print("   - Dataset augmentation for rare classes")
            print("   - Privacy-preserving synthetic data")
            print("   - Rapid prototyping of visual concepts")
            print("   - Style transfer and variations")

            print("\nâš¡ Performance Tips:")
            print("   - Use attention slicing for large images")
            print("   - Reduce inference steps for faster generation")
            print("   - Use CPU offloading for limited VRAM")
            print("   - Batch similar prompts for efficiency")


        if __name__ == "__main__":
            demonstrate_diffusion_generation()
      metadata:
        extension: .py
        size_bytes: 15420
        language: python
    src/edge_deployment.py:
      content: |
        """
        Edge deployment utilities for exporting models to ONNX and other formats.

        This module demonstrates how to export HuggingFace models for edge deployment,
        including ONNX export, optimization, and quantization for mobile/edge devices.
        """

        import os
        import time
        from pathlib import Path
        from typing import Any

        import torch
        from optimum.onnxruntime import ORTOptimizer
        from optimum.onnxruntime.configuration import OptimizationConfig
        from transformers import AutoModelForSequenceClassification, AutoTokenizer

        from src.config import Config
        from src.utils import format_size, timer_decorator


        class EdgeDeploymentPipeline:
            """Pipeline for exporting and optimizing models for edge deployment."""

            def __init__(self, model_name: str = None):
                """Initialize with a model for edge deployment."""
                self.model_name = model_name or Config.DEFAULT_SENTIMENT_MODEL
                self.device = torch.device("cpu")  # Edge deployment typically on CPU

            def export_to_onnx(
                self, output_dir: str = "models/onnx", optimize: bool = True
            ) -> dict[str, Any]:
                """
                Export model to ONNX format with optimization.

                Args:
                    output_dir: Directory to save ONNX model
                    optimize: Whether to apply ONNX optimizations

                Returns:
                    Dictionary with export metrics
                """
                output_path = Path(output_dir)
                output_path.mkdir(parents=True, exist_ok=True)

                print(f"\nðŸ”§ Exporting {self.model_name} to ONNX...")

                # Load model and tokenizer
                model = AutoModelForSequenceClassification.from_pretrained(self.model_name)
                tokenizer = AutoTokenizer.from_pretrained(self.model_name)

                # Get model size before export
                original_size = sum(p.numel() * p.element_size() for p in model.parameters())

                # Create dummy input for tracing
                dummy_text = "This is a sample text for ONNX export"
                inputs = tokenizer(dummy_text, return_tensors="pt")

                # Export to ONNX
                start_time = time.time()

                onnx_path = output_path / "model.onnx"
                torch.onnx.export(
                    model,
                    (inputs["input_ids"], inputs["attention_mask"]),
                    onnx_path,
                    export_params=True,
                    opset_version=14,
                    do_constant_folding=True,
                    input_names=["input_ids", "attention_mask"],
                    output_names=["logits"],
                    dynamic_axes={
                        "input_ids": {0: "batch_size", 1: "sequence"},
                        "attention_mask": {0: "batch_size", 1: "sequence"},
                        "logits": {0: "batch_size"},
                    },
                )

                export_time = time.time() - start_time

                # Optimize if requested
                if optimize:
                    print("ðŸ“Š Applying ONNX optimizations...")
                    optimizer = ORTOptimizer.from_pretrained(self.model_name)

                    optimization_config = OptimizationConfig(
                        optimization_level=2,
                        optimize_for_gpu=False,
                        fp16=False,  # Keep FP32 for CPU
                        enable_transformers_specific_optimizations=True,
                    )

                    optimizer.optimize(
                        optimization_config=optimization_config, save_dir=output_path
                    )

                # Get final size
                onnx_size = os.path.getsize(onnx_path)

                metrics = {
                    "original_size": format_size(original_size),
                    "onnx_size": format_size(onnx_size),
                    "size_reduction": f"{(1 - onnx_size/original_size)*100:.1f}%",
                    "export_time": f"{export_time:.2f}s",
                    "output_path": str(onnx_path),
                }

                print("\nâœ… ONNX Export Complete:")
                for key, value in metrics.items():
                    print(f"  {key}: {value}")

                return metrics

            def export_for_mobile(
                self, output_dir: str = "models/mobile", quantize: bool = True
            ) -> dict[str, Any]:
                """
                Export model optimized for mobile deployment.

                Args:
                    output_dir: Directory to save mobile-optimized model
                    quantize: Whether to apply dynamic quantization

                Returns:
                    Dictionary with mobile optimization metrics
                """
                output_path = Path(output_dir)
                output_path.mkdir(parents=True, exist_ok=True)

                print(f"\nðŸ“± Optimizing {self.model_name} for mobile...")

                # Load model
                model = AutoModelForSequenceClassification.from_pretrained(self.model_name)
                model.eval()

                # Apply dynamic quantization
                if quantize:
                    print("ðŸ”¢ Applying dynamic quantization...")
                    quantized_model = torch.quantization.quantize_dynamic(
                        model, {torch.nn.Linear}, dtype=torch.qint8
                    )
                else:
                    quantized_model = model

                # Save the model
                torch.save(quantized_model, output_path / "mobile_model.pt")

                # Also save in TorchScript format for mobile
                print("ðŸ“ Creating TorchScript model...")
                tokenizer = AutoTokenizer.from_pretrained(self.model_name)
                dummy_input = tokenizer("Sample text", return_tensors="pt")

                traced_model = torch.jit.trace(
                    quantized_model, (dummy_input["input_ids"], dummy_input["attention_mask"])
                )

                traced_model.save(output_path / "mobile_model_traced.pt")

                # Calculate sizes
                original_size = sum(p.numel() * p.element_size() for p in model.parameters())
                mobile_size = os.path.getsize(output_path / "mobile_model.pt")
                traced_size = os.path.getsize(output_path / "mobile_model_traced.pt")

                metrics = {
                    "original_size": format_size(original_size),
                    "mobile_size": format_size(mobile_size),
                    "traced_size": format_size(traced_size),
                    "size_reduction": f"{(1 - mobile_size/original_size)*100:.1f}%",
                    "quantization": "INT8" if quantize else "FP32",
                }

                print("\nâœ… Mobile Optimization Complete:")
                for key, value in metrics.items():
                    print(f"  {key}: {value}")

                return metrics

            @timer_decorator
            def benchmark_edge_inference(
                self, model_path: str, num_samples: int = 100
            ) -> dict[str, Any]:
                """
                Benchmark inference performance on edge device.

                Args:
                    model_path: Path to the optimized model
                    num_samples: Number of samples to benchmark

                Returns:
                    Performance metrics
                """
                print(f"\nâš¡ Benchmarking edge inference with {num_samples} samples...")

                # Load optimized model
                if model_path.endswith(".onnx"):
                    # ONNX Runtime inference
                    import onnxruntime as ort

                    session = ort.InferenceSession(model_path)
                    tokenizer = AutoTokenizer.from_pretrained(self.model_name)

                    # Warm up
                    dummy_input = tokenizer("Warm up", return_tensors="np")
                    _ = session.run(
                        None,
                        {
                            "input_ids": dummy_input["input_ids"],
                            "attention_mask": dummy_input["attention_mask"],
                        },
                    )

                    # Benchmark
                    texts = [f"Sample text {i}" for i in range(num_samples)]
                    start_time = time.time()

                    for text in texts:
                        inputs = tokenizer(
                            text,
                            return_tensors="np",
                            padding=True,
                            truncation=True,
                            max_length=128,
                        )
                        _ = session.run(
                            None,
                            {
                                "input_ids": inputs["input_ids"],
                                "attention_mask": inputs["attention_mask"],
                            },
                        )

                    total_time = time.time() - start_time

                else:
                    # PyTorch inference
                    model = torch.load(model_path)
                    model.eval()
                    tokenizer = AutoTokenizer.from_pretrained(self.model_name)

                    # Warm up
                    with torch.no_grad():
                        dummy_input = tokenizer("Warm up", return_tensors="pt")
                        _ = model(**dummy_input)

                    # Benchmark
                    texts = [f"Sample text {i}" for i in range(num_samples)]
                    start_time = time.time()

                    with torch.no_grad():
                        for text in texts:
                            inputs = tokenizer(
                                text,
                                return_tensors="pt",
                                padding=True,
                                truncation=True,
                                max_length=128,
                            )
                            _ = model(**inputs)

                    total_time = time.time() - start_time

                avg_latency = (total_time / num_samples) * 1000  # Convert to ms
                throughput = num_samples / total_time

                metrics = {
                    "total_samples": num_samples,
                    "total_time": f"{total_time:.2f}s",
                    "avg_latency": f"{avg_latency:.2f}ms",
                    "throughput": f"{throughput:.1f} samples/sec",
                    "model_type": "ONNX" if model_path.endswith(".onnx") else "PyTorch",
                }

                print("\nðŸ“Š Edge Inference Benchmarks:")
                for key, value in metrics.items():
                    print(f"  {key}: {value}")

                return metrics


        def demonstrate_edge_deployment():
            """Demonstrate complete edge deployment workflow."""
            print("=" * 80)
            print("ðŸš€ EDGE DEPLOYMENT DEMONSTRATION")
            print("=" * 80)

            pipeline = EdgeDeploymentPipeline()

            # 1. Export to ONNX
            print("\n1ï¸âƒ£ ONNX Export")
            onnx_metrics = pipeline.export_to_onnx()

            # 2. Mobile optimization
            print("\n2ï¸âƒ£ Mobile Optimization")
            mobile_metrics = pipeline.export_for_mobile()

            # 3. Benchmark performance
            print("\n3ï¸âƒ£ Performance Benchmarking")

            # Benchmark ONNX
            onnx_perf = pipeline.benchmark_edge_inference(
                "models/onnx/model.onnx", num_samples=50
            )

            # Benchmark mobile model
            mobile_perf = pipeline.benchmark_edge_inference(
                "models/mobile/mobile_model.pt", num_samples=50
            )

            print("\n" + "=" * 80)
            print("ðŸ“ˆ DEPLOYMENT SUMMARY")
            print("=" * 80)
            print("\nONNX Model:")
            print(
                f"  Size: {onnx_metrics['onnx_size']} ({onnx_metrics['size_reduction']} smaller)"
            )
            print(f"  Latency: {onnx_perf['avg_latency']}")
            print(f"  Throughput: {onnx_perf['throughput']}")

            print("\nMobile Model:")
            print(
                f"  Size: {mobile_metrics['mobile_size']} ({mobile_metrics['size_reduction']} smaller)"
            )
            print(f"  Latency: {mobile_perf['avg_latency']}")
            print(f"  Throughput: {mobile_perf['throughput']}")


        if __name__ == "__main__":
            demonstrate_edge_deployment()
      metadata:
        extension: .py
        size_bytes: 10786
        language: python
    src/config.py:
      content: |
        """Configuration management for workflow examples."""

        import os
        from pathlib import Path

        import torch
        from dotenv import load_dotenv

        # Load environment variables
        load_dotenv()


        # Device configuration
        def get_device():
            """Determine the best available device."""
            if torch.cuda.is_available():
                return "cuda"
            elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                return "mps"
            else:
                return "cpu"


        # Environment settings
        DEVICE = os.getenv("DEFAULT_DEVICE", get_device())
        BATCH_SIZE = int(os.getenv("BATCH_SIZE", "32"))
        MAX_LENGTH = int(os.getenv("MAX_LENGTH", "512"))

        # Model settings
        DEFAULT_SENTIMENT_MODEL = "distilbert-base-uncased-finetuned-sst-2-english"
        DEFAULT_NER_MODEL = "dbmdz/bert-large-cased-finetuned-conll03-english"
        DEFAULT_CLASSIFICATION_MODEL = "facebook/bart-large-mnli"
        DEFAULT_GENERATION_MODEL = "mistralai/Mistral-7B-Instruct-v0.2"

        # Data settings
        DATA_PATH = Path(os.getenv("DATA_PATH", "./data"))
        CACHE_DIR = Path(os.getenv("CACHE_DIR", "./cache"))
        NUM_WORKERS = int(os.getenv("NUM_WORKERS", "4"))

        # Optimization settings
        ENABLE_QUANTIZATION = os.getenv("ENABLE_QUANTIZATION", "true").lower() == "true"
        QUANTIZATION_BITS = int(os.getenv("QUANTIZATION_BITS", "8"))
        ENABLE_FLASH_ATTENTION = os.getenv("ENABLE_FLASH_ATTENTION", "true").lower() == "true"

        # Synthetic data settings
        SYNTHETIC_DATA_PATH = Path(os.getenv("SYNTHETIC_DATA_PATH", "./synthetic"))
        VALIDATION_THRESHOLD = float(os.getenv("VALIDATION_THRESHOLD", "0.85"))

        # Create directories
        DATA_PATH.mkdir(exist_ok=True)
        CACHE_DIR.mkdir(exist_ok=True)
        SYNTHETIC_DATA_PATH.mkdir(exist_ok=True)

        # Hugging Face settings
        HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")
        if HF_TOKEN:
            os.environ["HF_TOKEN"] = HF_TOKEN


        class Config:
            """Centralized configuration class for easy access."""

            # Device configuration
            DEVICE = torch.device(DEVICE)

            # Batch and model settings
            BATCH_SIZE = BATCH_SIZE
            MAX_LENGTH = MAX_LENGTH

            # Model names
            DEFAULT_SENTIMENT_MODEL = DEFAULT_SENTIMENT_MODEL
            DEFAULT_NER_MODEL = DEFAULT_NER_MODEL
            DEFAULT_CLASSIFICATION_MODEL = DEFAULT_CLASSIFICATION_MODEL
            DEFAULT_GENERATION_MODEL = DEFAULT_GENERATION_MODEL

            # Paths
            DATA_PATH = DATA_PATH
            CACHE_DIR = CACHE_DIR
            SYNTHETIC_DATA_PATH = SYNTHETIC_DATA_PATH

            # Other settings
            NUM_WORKERS = NUM_WORKERS
            ENABLE_QUANTIZATION = ENABLE_QUANTIZATION
            QUANTIZATION_BITS = QUANTIZATION_BITS
            ENABLE_FLASH_ATTENTION = ENABLE_FLASH_ATTENTION
            VALIDATION_THRESHOLD = VALIDATION_THRESHOLD
      metadata:
        extension: .py
        size_bytes: 2568
        language: python
    src/peft_lora.py:
      content: |
        """
        PEFT (Parameter-Efficient Fine-Tuning) and LoRA integration examples.

        This module demonstrates how to use LoRA and other PEFT methods to efficiently
        fine-tune large language models with minimal compute and memory requirements.
        """

        import os
        import time
        from typing import Any

        import torch
        from datasets import Dataset
        from peft import (
            LoraConfig,
            PeftModel,
            TaskType,
            get_peft_model,
            prepare_model_for_kbit_training,
        )
        from transformers import (
            AutoModelForCausalLM,
            AutoTokenizer,
            DataCollatorForLanguageModeling,
            Trainer,
            TrainingArguments,
        )

        from src.config import Config
        from src.utils import format_size, timer_decorator


        class PEFTTrainer:
            """Trainer class for PEFT/LoRA fine-tuning."""

            def __init__(
                self,
                model_name: str = "gpt2",  # Start with smaller model for demo
                task_type: TaskType = TaskType.CAUSAL_LM,
            ):
                """Initialize PEFT trainer with model and task type."""
                self.model_name = model_name
                self.task_type = task_type
                self.device = Config.DEVICE

            def setup_lora_model(
                self,
                r: int = 16,
                lora_alpha: int = 32,
                lora_dropout: float = 0.1,
                target_modules: list[str] = None,
            ) -> tuple:
                """
                Setup model with LoRA configuration.

                Args:
                    r: LoRA rank
                    lora_alpha: LoRA scaling parameter
                    lora_dropout: Dropout for LoRA layers
                    target_modules: Modules to apply LoRA to

                Returns:
                    Tuple of (peft_model, tokenizer)
                """
                print(f"\nðŸ”§ Setting up LoRA for {self.model_name}...")

                # Load base model and tokenizer
                model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    device_map="auto",
                    torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
                )

                tokenizer = AutoTokenizer.from_pretrained(self.model_name)
                tokenizer.pad_token = tokenizer.eos_token

                # Print original model stats
                original_params = sum(p.numel() for p in model.parameters())
                print(f"ðŸ“Š Original model parameters: {original_params:,}")

                # Configure LoRA
                if target_modules is None:
                    # Default modules for different model types
                    if "gpt2" in self.model_name.lower():
                        target_modules = ["c_attn", "c_proj"]
                    elif "llama" in self.model_name.lower():
                        target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
                    else:
                        target_modules = ["query", "value"]

                lora_config = LoraConfig(
                    r=r,
                    lora_alpha=lora_alpha,
                    lora_dropout=lora_dropout,
                    bias="none",
                    task_type=self.task_type,
                    target_modules=target_modules,
                )

                # Create PEFT model
                model = prepare_model_for_kbit_training(model)
                peft_model = get_peft_model(model, lora_config)

                # Print LoRA stats
                peft_model.print_trainable_parameters()

                return peft_model, tokenizer

            @timer_decorator
            def fine_tune_with_lora(
                self,
                peft_model,
                tokenizer,
                dataset: Dataset,
                output_dir: str = "models/lora",
                num_epochs: int = 3,
                batch_size: int = 4,
            ) -> dict[str, Any]:
                """
                Fine-tune model using LoRA.

                Args:
                    peft_model: PEFT model with LoRA
                    tokenizer: Tokenizer
                    dataset: Training dataset
                    output_dir: Directory to save fine-tuned model
                    num_epochs: Number of training epochs
                    batch_size: Training batch size

                Returns:
                    Training metrics
                """
                print("\nðŸš€ Starting LoRA fine-tuning...")

                # Prepare dataset
                def tokenize_function(examples):
                    return tokenizer(
                        examples["text"], truncation=True, padding="max_length", max_length=128
                    )

                tokenized_dataset = dataset.map(tokenize_function, batched=True)

                # Setup training arguments
                training_args = TrainingArguments(
                    output_dir=output_dir,
                    num_train_epochs=num_epochs,
                    per_device_train_batch_size=batch_size,
                    gradient_accumulation_steps=1,
                    warmup_steps=100,
                    logging_steps=10,
                    save_strategy="epoch",
                    evaluation_strategy="no",
                    learning_rate=3e-4,
                    fp16=self.device.type == "cuda",
                    push_to_hub=False,
                )

                # Create trainer
                data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

                trainer = Trainer(
                    model=peft_model,
                    args=training_args,
                    train_dataset=tokenized_dataset,
                    data_collator=data_collator,
                )

                # Train
                start_time = time.time()
                trainer.train()
                training_time = time.time() - start_time

                # Save LoRA weights
                peft_model.save_pretrained(output_dir)

                # Calculate adapter size
                adapter_size = sum(
                    os.path.getsize(os.path.join(output_dir, f))
                    for f in os.listdir(output_dir)
                    if f.endswith(".bin") or f.endswith(".safetensors")
                )

                metrics = {
                    "training_time": f"{training_time:.2f}s",
                    "adapter_size": format_size(adapter_size),
                    "trainable_params": sum(
                        p.numel() for p in peft_model.parameters() if p.requires_grad
                    ),
                    "total_params": sum(p.numel() for p in peft_model.parameters()),
                    "efficiency": f"{(sum(p.numel() for p in peft_model.parameters() if p.requires_grad) / sum(p.numel() for p in peft_model.parameters()) * 100):.2f}%",
                }

                print("\nâœ… LoRA Training Complete:")
                for key, value in metrics.items():
                    print(f"  {key}: {value}")

                return metrics

            def inference_with_lora(
                self,
                base_model_name: str,
                lora_weights_path: str,
                prompt: str,
                max_length: int = 100,
            ) -> str:
                """
                Run inference with LoRA adapter.

                Args:
                    base_model_name: Name of base model
                    lora_weights_path: Path to LoRA weights
                    prompt: Input prompt
                    max_length: Maximum generation length

                Returns:
                    Generated text
                """
                print("\nðŸ’­ Running inference with LoRA adapter...")

                # Load base model
                model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    device_map="auto",
                    torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
                )

                # Load LoRA weights
                model = PeftModel.from_pretrained(model, lora_weights_path)
                tokenizer = AutoTokenizer.from_pretrained(base_model_name)

                # Generate
                inputs = tokenizer(prompt, return_tensors="pt").to(self.device)

                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_length=max_length,
                        num_beams=1,
                        do_sample=True,
                        temperature=0.7,
                    )

                generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
                return generated_text

            def compare_lora_configs(self, dataset: Dataset) -> dict[str, Any]:
                """
                Compare different LoRA configurations.

                Args:
                    dataset: Training dataset

                Returns:
                    Comparison metrics
                """
                print("\nðŸ”¬ Comparing LoRA Configurations...")

                configs = [
                    {"r": 8, "lora_alpha": 16, "name": "Small (r=8)"},
                    {"r": 16, "lora_alpha": 32, "name": "Medium (r=16)"},
                    {"r": 32, "lora_alpha": 64, "name": "Large (r=32)"},
                ]

                results = []

                for config in configs:
                    print(f"\nðŸ“Š Testing {config['name']}...")

                    # Setup model with config
                    peft_model, tokenizer = self.setup_lora_model(
                        r=config["r"], lora_alpha=config["lora_alpha"]
                    )

                    # Count trainable parameters
                    trainable_params = sum(
                        p.numel() for p in peft_model.parameters() if p.requires_grad
                    )
                    total_params = sum(p.numel() for p in peft_model.parameters())

                    # Measure memory
                    if self.device.type == "cuda":
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                        memory_before = torch.cuda.memory_allocated()

                        # Do a forward pass
                        dummy_input = tokenizer("Test", return_tensors="pt").to(self.device)
                        _ = peft_model(**dummy_input)

                        torch.cuda.synchronize()
                        memory_after = torch.cuda.memory_allocated()
                        memory_used = memory_after - memory_before
                    else:
                        memory_used = 0

                    results.append(
                        {
                            "config": config["name"],
                            "r": config["r"],
                            "trainable_params": f"{trainable_params:,}",
                            "total_params": f"{total_params:,}",
                            "efficiency": f"{(trainable_params/total_params*100):.3f}%",
                            "memory_used": (
                                format_size(memory_used) if memory_used > 0 else "N/A"
                            ),
                        }
                    )

                    # Clean up
                    del peft_model
                    if self.device.type == "cuda":
                        torch.cuda.empty_cache()

                print("\nðŸ“Š LoRA Configuration Comparison:")
                print(
                    f"{'Config':<15} {'r':<5} {'Trainable':<15} {'Total':<15} "
                    f"{'Efficiency':<10} {'Memory':<10}"
                )
                print("-" * 80)
                for result in results:
                    print(
                        f"{result['config']:<15} {result['r']:<5} "
                        f"{result['trainable_params']:<15} "
                        f"{result['total_params']:<15} {result['efficiency']:<10} "
                        f"{result['memory_used']:<10}"
                    )

                return {"comparison": results}


        def demonstrate_peft_lora():
            """Demonstrate PEFT/LoRA workflow."""
            print("=" * 80)
            print("ðŸŽ¯ PEFT/LoRA DEMONSTRATION")
            print("=" * 80)

            # Create sample dataset
            texts = [
                "The future of AI is bright and full of possibilities.",
                "Machine learning transforms how we solve complex problems.",
                "Deep learning models continue to improve rapidly.",
                "Natural language processing enables better human-computer interaction.",
                "Computer vision applications are becoming more sophisticated.",
            ] * 20  # Repeat for larger dataset

            dataset = Dataset.from_dict({"text": texts})

            # Initialize trainer
            trainer = PEFTTrainer(model_name="gpt2")

            # 1. Setup LoRA model
            print("\n1ï¸âƒ£ Setting up LoRA Model")
            peft_model, tokenizer = trainer.setup_lora_model(r=16, lora_alpha=32)

            # 2. Fine-tune with LoRA
            print("\n2ï¸âƒ£ Fine-tuning with LoRA")
            metrics = trainer.fine_tune_with_lora(
                peft_model, tokenizer, dataset, num_epochs=1  # Quick demo
            )

            # 3. Test inference
            print("\n3ï¸âƒ£ Testing Inference")
            generated = trainer.inference_with_lora(
                "gpt2", "models/lora", "The future of AI", max_length=50
            )
            print(f"Generated: {generated}")

            # 4. Compare configurations
            print("\n4ï¸âƒ£ Comparing Configurations")
            comparison = trainer.compare_lora_configs(dataset)

            print("\n" + "=" * 80)
            print("âœ… PEFT/LoRA demonstration complete!")
            print("=" * 80)


        if __name__ == "__main__":
            demonstrate_peft_lora()
      metadata:
        extension: .py
        size_bytes: 11800
        language: python
    src/optimization.py:
      content: |
        """Model optimization techniques for efficient deployment."""

        import gc
        import time

        import torch
        from transformers import AutoModelForSequenceClassification, AutoTokenizer

        from src.config import DEFAULT_SENTIMENT_MODEL, DEVICE


        def benchmark_inference(model, tokenizer, texts, description=""):
            """Benchmark model inference speed and memory."""
            if torch.cuda.is_available():
                torch.cuda.synchronize()
                torch.cuda.reset_peak_memory_stats()
                start_mem = torch.cuda.memory_allocated()

            start_time = time.time()

            # Run inference
            inputs = tokenizer(
                texts, padding=True, truncation=True, max_length=128, return_tensors="pt"
            ).to(model.device)

            with torch.no_grad():
                outputs = model(**inputs)

            if torch.cuda.is_available():
                torch.cuda.synchronize()

            end_time = time.time()

            if torch.cuda.is_available():
                end_mem = torch.cuda.memory_allocated()
                peak_mem = torch.cuda.max_memory_allocated()
                memory_used = (peak_mem - start_mem) / 1024 / 1024  # MB
            else:
                memory_used = 0

            inference_time = end_time - start_time

            print(f"{description}")
            print(
                f"  Time: {inference_time:.3f}s ({inference_time/len(texts)*1000:.1f}ms per sample)"
            )
            print(f"  Memory: {memory_used:.1f}MB")
            print(f"  Throughput: {len(texts)/inference_time:.1f} samples/sec")

            return inference_time, memory_used


        def demonstrate_optimization():
            """Demonstrate various optimization techniques."""

            print("Loading test data...")
            test_texts = [
                "This product is absolutely amazing!",
                "Terrible experience, would not recommend.",
                "Average quality, nothing special.",
                "Exceeded all my expectations!",
                "Complete waste of money.",
                "Pretty good for the price.",
                "Outstanding service and quality!",
                "Disappointing purchase.",
            ] * 4  # 32 samples

            print(f"Test set: {len(test_texts)} samples\n")

            print("1. Baseline Model Performance")
            print("-" * 40)

            # Load model and tokenizer
            tokenizer = AutoTokenizer.from_pretrained(DEFAULT_SENTIMENT_MODEL)
            model = AutoModelForSequenceClassification.from_pretrained(DEFAULT_SENTIMENT_MODEL)

            if DEVICE == "cuda":
                model = model.cuda()

            # Baseline performance
            base_time, base_mem = benchmark_inference(
                model, tokenizer, test_texts, "FP32 Model"
            )

            # Model size
            param_size = (
                sum(p.numel() * p.element_size() for p in model.parameters()) / 1024 / 1024
            )
            print(f"  Model size: {param_size:.1f}MB")

            print("\n2. Dynamic Quantization (INT8)")
            print("-" * 40)

            # Quantize model
            if DEVICE == "cpu":
                quantized_model = torch.quantization.quantize_dynamic(
                    model.cpu(), {torch.nn.Linear}, dtype=torch.qint8
                )

                q_time, q_mem = benchmark_inference(
                    quantized_model, tokenizer, test_texts, "INT8 Quantized"
                )

                print(f"  Speedup: {base_time/q_time:.2f}x")
            else:
                print("  Skipping (CPU only)")

            print("\n3. Batching Optimization")
            print("-" * 40)

            # Single sample inference
            single_times = []
            for text in test_texts[:8]:
                start = time.time()
                inputs = tokenizer(text, return_tensors="pt").to(model.device)
                with torch.no_grad():
                    _ = model(**inputs)
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                single_times.append(time.time() - start)

            single_avg = sum(single_times) / len(single_times)
            print(f"Single inference: {single_avg*1000:.1f}ms per sample")

            # Batch inference
            batch_sizes = [1, 4, 8, 16, 32]
            for bs in batch_sizes:
                if bs > len(test_texts):
                    continue

                batch_texts = test_texts[:bs]
                start = time.time()

                inputs = tokenizer(
                    batch_texts,
                    padding=True,
                    truncation=True,
                    max_length=128,
                    return_tensors="pt",
                ).to(model.device)

                with torch.no_grad():
                    _ = model(**inputs)

                if torch.cuda.is_available():
                    torch.cuda.synchronize()

                batch_time = time.time() - start
                per_sample = batch_time / bs * 1000

                print(
                    f"Batch size {bs}: {per_sample:.1f}ms per sample (speedup: {single_avg*1000/per_sample:.1f}x)"
                )

            print("\n4. Half Precision (FP16)")
            print("-" * 40)

            if DEVICE == "cuda":
                model_fp16 = model.half()

                fp16_time, fp16_mem = benchmark_inference(
                    model_fp16, tokenizer, test_texts, "FP16 Model"
                )

                print(f"  Speedup: {base_time/fp16_time:.2f}x")
                print(f"  Memory reduction: {(base_mem-fp16_mem)/base_mem*100:.1f}%")
            else:
                print("  Skipping (GPU only)")

            print("\n5. Model Pruning Simulation")
            print("-" * 40)

            # Simulate pruning by using a smaller model
            small_model_name = "distilbert-base-uncased"
            print(f"Loading smaller model: {small_model_name}")

            small_tokenizer = AutoTokenizer.from_pretrained(small_model_name)
            small_model = AutoModelForSequenceClassification.from_pretrained(
                small_model_name, num_labels=2
            )

            if DEVICE == "cuda":
                small_model = small_model.cuda()

            small_time, small_mem = benchmark_inference(
                small_model, small_tokenizer, test_texts, "DistilBERT (smaller)"
            )

            small_param_size = (
                sum(p.numel() * p.element_size() for p in small_model.parameters())
                / 1024
                / 1024
            )

            print(
                f"  Model size: {small_param_size:.1f}MB (reduction: {(param_size-small_param_size)/param_size*100:.1f}%)"
            )
            print(f"  Speedup: {base_time/small_time:.2f}x")

            print("\n6. Optimization Summary")
            print("-" * 40)
            print("| Technique | Speedup | Size Reduction | Use Case |")
            print("|-----------|---------|----------------|----------|")
            print("| Batching (32) | ~8x | 0% | GPU servers |")
            if DEVICE == "cpu":
                print(f"| INT8 Quant | {base_time/q_time:.1f}x | 75% | CPU/Edge |")
            if DEVICE == "cuda":
                print(f"| FP16 | {base_time/fp16_time:.1f}x | 50% | Modern GPUs |")
            print(
                f"| DistilBERT | {base_time/small_time:.1f}x | {(param_size-small_param_size)/param_size*100:.0f}% | General use |"
            )

            # Cleanup
            del model
            if "quantized_model" in locals():
                del quantized_model
            if "model_fp16" in locals():
                del model_fp16
            del small_model
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()


        if __name__ == "__main__":
            demonstrate_optimization()
      metadata:
        extension: .py
        size_bytes: 6646
        language: python
    src/__init__.py:
      content: |
        """Custom pipelines and data workflows for production-ready transformers."""

        __version__ = "0.1.0"
      metadata:
        extension: .py
        size_bytes: 100
        language: python
    src/advanced_quantization.py:
      content: |
        """
        Advanced quantization techniques including INT4 for large language models.

        This module demonstrates INT4, INT8, and other quantization methods for
        deploying large models with minimal memory footprint and maximum performance.
        """

        import time
        from pathlib import Path
        from typing import Any

        import torch
        from transformers import (
            AutoModelForCausalLM,
            AutoTokenizer,
            BitsAndBytesConfig,
        )

        from src.config import Config
        from src.utils import format_size, timer_decorator


        class AdvancedQuantization:
            """Advanced quantization techniques for large models."""

            def __init__(self):
                """Initialize quantization demo."""
                self.device = Config.DEVICE
                self._check_dependencies()

            def _check_dependencies(self):
                """Check if required dependencies are available."""
                try:
                    import bitsandbytes as bnb

                    print("âœ… bitsandbytes library found")
                except ImportError:
                    print(
                        "âš ï¸  bitsandbytes not installed. Install with: pip install bitsandbytes"
                    )

            @timer_decorator
            def quantize_model_int4(
                self,
                model_name: str = "gpt2",
                compute_dtype: torch.dtype = torch.float16,
                quant_type: str = "nf4",
            ) -> dict[str, Any]:
                """
                Quantize model to INT4 using bitsandbytes.

                Args:
                    model_name: Model to quantize
                    compute_dtype: Compute dtype for forward pass
                    quant_type: Quantization type (nf4 or fp4)

                Returns:
                    Quantization metrics
                """
                print(f"\nðŸ”¢ Quantizing {model_name} to INT4 ({quant_type})...")

                # Configure INT4 quantization
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type=quant_type,  # nf4 or fp4
                    bnb_4bit_compute_dtype=compute_dtype,
                    bnb_4bit_use_double_quant=True,  # Nested quantization
                )

                # Track memory before loading
                if self.device.type == "cuda":
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    mem_before = torch.cuda.memory_allocated()

                # Load model with INT4 quantization
                start_time = time.time()
                model_int4 = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    quantization_config=bnb_config,
                    device_map="auto",
                    trust_remote_code=True,
                )
                load_time = time.time() - start_time

                tokenizer = AutoTokenizer.from_pretrained(model_name)
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token

                # Track memory after loading
                if self.device.type == "cuda":
                    torch.cuda.synchronize()
                    mem_after = torch.cuda.memory_allocated()
                    memory_used = mem_after - mem_before
                else:
                    memory_used = 0

                # Test inference
                test_text = "The future of AI is"
                inputs = tokenizer(test_text, return_tensors="pt").to(self.device)

                with torch.no_grad():
                    start_time = time.time()
                    outputs = model_int4.generate(**inputs, max_new_tokens=20)
                    inference_time = time.time() - start_time

                generated = tokenizer.decode(outputs[0], skip_special_tokens=True)

                # Calculate model size (approximate)
                param_count = sum(p.numel() for p in model_int4.parameters())
                int4_size = param_count * 0.5  # 4 bits = 0.5 bytes per parameter
                fp32_size = param_count * 4  # 32 bits = 4 bytes per parameter

                metrics = {
                    "quantization": f"INT4 ({quant_type})",
                    "load_time": f"{load_time:.2f}s",
                    "memory_used": format_size(memory_used),
                    "inference_time": f"{inference_time:.3f}s",
                    "approx_model_size": format_size(int4_size),
                    "size_reduction": f"{(1 - int4_size/fp32_size)*100:.1f}%",
                    "generated_text": generated,
                }

                print("\nâœ… INT4 Quantization Complete:")
                for key, value in metrics.items():
                    if key != "generated_text":
                        print(f"  {key}: {value}")

                return metrics, model_int4

            def compare_quantization_methods(
                self, model_name: str = "gpt2", test_sequences: int = 10
            ) -> dict[str, Any]:
                """
                Compare different quantization methods.

                Args:
                    model_name: Model to test
                    test_sequences: Number of sequences for benchmarking

                Returns:
                    Comparison results
                """
                print(f"\nðŸ”¬ Comparing Quantization Methods for {model_name}")

                tokenizer = AutoTokenizer.from_pretrained(model_name)
                if tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token

                # Test inputs
                test_texts = [
                    f"Sample text {i} for quantization testing." for i in range(test_sequences)
                ]

                results = {}

                # 1. FP32 Baseline
                print("\nðŸ“Š Testing FP32 (Baseline)...")
                if self.device.type == "cuda":
                    torch.cuda.empty_cache()

                model_fp32 = AutoModelForCausalLM.from_pretrained(
                    model_name, torch_dtype=torch.float32, device_map="auto"
                )

                results["FP32"] = self._benchmark_model(model_fp32, tokenizer, test_texts)
                del model_fp32

                # 2. FP16
                print("\nðŸ“Š Testing FP16...")
                if self.device.type == "cuda":
                    torch.cuda.empty_cache()

                    model_fp16 = AutoModelForCausalLM.from_pretrained(
                        model_name, torch_dtype=torch.float16, device_map="auto"
                    )

                    results["FP16"] = self._benchmark_model(model_fp16, tokenizer, test_texts)
                    del model_fp16

                # 3. INT8
                print("\nðŸ“Š Testing INT8...")
                if self.device.type == "cuda":
                    torch.cuda.empty_cache()

                bnb_config_int8 = BitsAndBytesConfig(
                    load_in_8bit=True,
                    bnb_8bit_compute_dtype=(
                        torch.float16 if self.device.type == "cuda" else torch.float32
                    ),
                )

                model_int8 = AutoModelForCausalLM.from_pretrained(
                    model_name, quantization_config=bnb_config_int8, device_map="auto"
                )

                results["INT8"] = self._benchmark_model(model_int8, tokenizer, test_texts)
                del model_int8

                # 4. INT4 (NF4)
                print("\nðŸ“Š Testing INT4 (NF4)...")
                if self.device.type == "cuda":
                    torch.cuda.empty_cache()

                bnb_config_nf4 = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=(
                        torch.float16 if self.device.type == "cuda" else torch.float32
                    ),
                    bnb_4bit_use_double_quant=True,
                )

                model_nf4 = AutoModelForCausalLM.from_pretrained(
                    model_name, quantization_config=bnb_config_nf4, device_map="auto"
                )

                results["INT4-NF4"] = self._benchmark_model(model_nf4, tokenizer, test_texts)
                del model_nf4

                # Print comparison
                print("\nðŸ“Š Quantization Method Comparison:")
                print(
                    f"{'Method':<12} {'Memory':<12} {'Avg Latency':<15} "
                    f"{'Throughput':<20} {'Size Reduction':<15}"
                )
                print("-" * 85)

                fp32_memory = results.get("FP32", {}).get("memory", 0)
                for method, metrics in results.items():
                    size_reduction = (
                        f"{(1 - metrics['memory']/fp32_memory)*100:.1f}%"
                        if fp32_memory > 0
                        else "N/A"
                    )
                    print(
                        f"{method:<12} {format_size(metrics['memory']):<12} "
                        f"{metrics['avg_latency']:<15} {metrics['throughput']:<20} "
                        f"{size_reduction:<15}"
                    )

                return results

            def _benchmark_model(
                self, model, tokenizer, test_texts: list[str]
            ) -> dict[str, Any]:
                """Benchmark a quantized model."""
                # Memory usage
                if self.device.type == "cuda":
                    torch.cuda.synchronize()
                    memory = torch.cuda.memory_allocated()
                else:
                    memory = 0

                # Warm up
                inputs = tokenizer(
                    test_texts[0], return_tensors="pt", truncation=True, max_length=128
                ).to(self.device)
                with torch.no_grad():
                    _ = model.generate(**inputs, max_new_tokens=20)

                # Benchmark
                total_time = 0
                for text in test_texts:
                    inputs = tokenizer(
                        text, return_tensors="pt", truncation=True, max_length=128
                    ).to(self.device)

                    start_time = time.time()
                    with torch.no_grad():
                        _ = model.generate(**inputs, max_new_tokens=20)
                    total_time += time.time() - start_time

                avg_latency = total_time / len(test_texts)
                throughput = len(test_texts) / total_time

                return {
                    "memory": memory,
                    "avg_latency": f"{avg_latency*1000:.1f}ms",
                    "throughput": f"{throughput:.2f} seq/s",
                    "total_time": total_time,
                }

            def demonstrate_qlora_optimization(
                self, model_name: str = "gpt2"
            ) -> dict[str, Any]:
                """
                Demonstrate QLoRA (Quantized LoRA) for memory-efficient fine-tuning.

                Args:
                    model_name: Model to demonstrate with

                Returns:
                    QLoRA optimization metrics
                """
                print(f"\nðŸŽ¯ Demonstrating QLoRA (INT4 + LoRA) for {model_name}")

                # Load model with INT4 quantization
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=(
                        torch.float16 if self.device.type == "cuda" else torch.float32
                    ),
                    bnb_4bit_use_double_quant=True,
                )

                model = AutoModelForCausalLM.from_pretrained(
                    model_name, quantization_config=bnb_config, device_map="auto"
                )

                # Prepare for LoRA
                from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

                model = prepare_model_for_kbit_training(model)

                # Add LoRA adapters
                lora_config = LoraConfig(
                    r=8,
                    lora_alpha=16,
                    target_modules=["c_attn"] if "gpt2" in model_name else ["q_proj", "v_proj"],
                    lora_dropout=0.1,
                    bias="none",
                    task_type="CAUSAL_LM",
                )

                model = get_peft_model(model, lora_config)

                # Print statistics
                model.print_trainable_parameters()

                # Calculate memory footprint
                total_params = sum(p.numel() for p in model.parameters())
                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

                # Approximate memory usage
                base_model_memory = total_params * 0.5  # INT4 = 0.5 bytes per param
                lora_memory = trainable_params * 2  # FP16 = 2 bytes per param
                total_memory = base_model_memory + lora_memory

                metrics = {
                    "base_model": model_name,
                    "quantization": "INT4 (NF4)",
                    "lora_rank": 8,
                    "total_params": f"{total_params:,}",
                    "trainable_params": f"{trainable_params:,}",
                    "trainable_percentage": f"{(trainable_params/total_params)*100:.3f}%",
                    "approx_memory": format_size(total_memory),
                    "base_model_memory": format_size(base_model_memory),
                    "lora_adapter_memory": format_size(lora_memory),
                }

                print("\nâœ… QLoRA Configuration:")
                for key, value in metrics.items():
                    print(f"  {key}: {value}")

                return metrics

            def export_quantized_model(
                self, model, output_dir: str = "models/quantized"
            ) -> dict[str, Any]:
                """
                Export quantized model for deployment.

                Args:
                    model: Quantized model to export
                    output_dir: Directory to save model

                Returns:
                    Export metrics
                """
                output_path = Path(output_dir)
                output_path.mkdir(parents=True, exist_ok=True)

                print(f"\nðŸ’¾ Exporting quantized model to {output_dir}...")

                # Save model
                model.save_pretrained(output_path)

                # Calculate exported size
                total_size = 0
                for file in output_path.rglob("*"):
                    if file.is_file():
                        total_size += file.stat().st_size

                metrics = {
                    "output_dir": str(output_path),
                    "exported_size": format_size(total_size),
                    "files_created": len(list(output_path.rglob("*"))),
                }

                print("\nâœ… Export complete:")
                for key, value in metrics.items():
                    print(f"  {key}: {value}")

                return metrics


        def demonstrate_advanced_quantization():
            """Demonstrate advanced quantization techniques."""
            print("=" * 80)
            print("ðŸ”¢ ADVANCED QUANTIZATION DEMONSTRATION")
            print("=" * 80)

            quant = AdvancedQuantization()

            # 1. INT4 Quantization
            print("\n1ï¸âƒ£ INT4 Quantization")
            int4_metrics, int4_model = quant.quantize_model_int4()

            # 2. Compare quantization methods
            print("\n2ï¸âƒ£ Quantization Method Comparison")
            comparison = quant.compare_quantization_methods(test_sequences=5)

            # 3. QLoRA demonstration
            print("\n3ï¸âƒ£ QLoRA (Quantized LoRA)")
            qlora_metrics = quant.demonstrate_qlora_optimization()

            # 4. Export quantized model
            print("\n4ï¸âƒ£ Export Quantized Model")
            export_metrics = quant.export_quantized_model(int4_model)

            print("\n" + "=" * 80)
            print("ðŸ“Š QUANTIZATION SUMMARY")
            print("=" * 80)

            print("\nâœ… Key Benefits Demonstrated:")
            print("   - INT4: 87.5% size reduction vs FP32")
            print("   - INT8: 75% size reduction with minimal accuracy loss")
            print("   - QLoRA: Fine-tune large models on consumer GPUs")
            print("   - Double quantization: Extra memory savings")

            print("\nðŸ’¡ Deployment Recommendations:")
            print("   - Edge devices: INT8 dynamic quantization")
            print("   - GPU servers: INT4 with FP16 compute")
            print("   - Fine-tuning: QLoRA for memory efficiency")
            print("   - Mobile: Export to ONNX after quantization")


        if __name__ == "__main__":
            demonstrate_advanced_quantization()
      metadata:
        extension: .py
        size_bytes: 14331
        language: python
    src/production_workflows.py:
      content: |
        """End-to-end production workflow example."""

        import json
        import time
        from pathlib import Path

        from transformers import pipeline

        from src.config import BATCH_SIZE, DEVICE


        class RetailReviewWorkflow:
            """Production workflow for retail review analysis."""

            def __init__(self):
                # Initialize pipelines
                self.sentiment_pipeline = pipeline(
                    "sentiment-analysis",
                    model="distilbert-base-uncased-finetuned-sst-2-english",
                    device=0 if DEVICE == "cuda" else -1,
                )

                self.classification_pipeline = pipeline(
                    "zero-shot-classification",
                    model="facebook/bart-large-mnli",
                    device=0 if DEVICE == "cuda" else -1,
                )

                self.categories = [
                    "product quality",
                    "shipping",
                    "customer service",
                    "pricing",
                    "packaging",
                ]

                self.priority_keywords = {
                    "urgent": ["broken", "damaged", "fraud", "stolen", "urgent"],
                    "high": ["terrible", "awful", "worst", "refund", "complaint"],
                    "medium": ["disappointed", "issue", "problem", "concern"],
                    "low": ["suggestion", "feedback", "minor"],
                }

            def preprocess(self, text):
                """Clean and normalize review text."""
                import re

                # Remove URLs
                text = re.sub(r"http\S+|www.\S+", "", text)

                # Remove excessive whitespace
                text = " ".join(text.split())

                # Truncate very long reviews
                words = text.split()
                if len(words) > 200:
                    text = " ".join(words[:200]) + "..."

                return text

            def analyze_priority(self, text):
                """Determine review priority based on keywords."""
                text_lower = text.lower()

                for priority, keywords in self.priority_keywords.items():
                    if any(keyword in text_lower for keyword in keywords):
                        return priority

                return "normal"

            def process_batch(self, reviews):
                """Process a batch of reviews."""
                # Preprocess
                cleaned_reviews = [self.preprocess(review) for review in reviews]

                # Sentiment analysis
                sentiments = self.sentiment_pipeline(cleaned_reviews, batch_size=BATCH_SIZE)

                # Category classification
                categories = []
                for review in cleaned_reviews:
                    result = self.classification_pipeline(
                        review, self.categories, multi_label=True
                    )
                    categories.append(
                        {
                            "labels": result["labels"][:2],  # Top 2 categories
                            "scores": result["scores"][:2],
                        }
                    )

                # Combine results
                results = []
                for i, review in enumerate(reviews):
                    results.append(
                        {
                            "original_text": review,
                            "cleaned_text": cleaned_reviews[i],
                            "sentiment": sentiments[i]["label"],
                            "sentiment_score": sentiments[i]["score"],
                            "categories": categories[i]["labels"],
                            "category_scores": categories[i]["scores"],
                            "priority": self.analyze_priority(review),
                            "timestamp": time.time(),
                        }
                    )

                return results

            def generate_insights(self, results):
                """Generate business insights from processed reviews."""
                total = len(results)

                # Sentiment distribution
                sentiment_counts = {}
                for r in results:
                    sentiment = r["sentiment"]
                    sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1

                # Category distribution
                category_counts = {}
                for r in results:
                    for category in r["categories"]:
                        category_counts[category] = category_counts.get(category, 0) + 1

                # Priority distribution
                priority_counts = {}
                for r in results:
                    priority = r["priority"]
                    priority_counts[priority] = priority_counts.get(priority, 0) + 1

                insights = {
                    "total_reviews": total,
                    "sentiment_distribution": {
                        k: {"count": v, "percentage": v / total * 100}
                        for k, v in sentiment_counts.items()
                    },
                    "top_categories": sorted(
                        category_counts.items(), key=lambda x: x[1], reverse=True
                    )[:5],
                    "priority_distribution": priority_counts,
                    "urgent_reviews": [
                        r for r in results if r["priority"] in ["urgent", "high"]
                    ][:5],
                }

                return insights


        def demonstrate_production_workflow():
            """Demonstrate a complete production workflow."""

            print("=== Retail Review Analysis Workflow ===\n")

            # Initialize workflow
            workflow = RetailReviewWorkflow()

            # Sample reviews (in production, these would come from a database/stream)
            sample_reviews = [
                "This product is absolutely amazing! Fast shipping and great quality.",
                "Terrible experience. The item arrived broken and customer service "
                "was unhelpful.",
                "Good value for money, but packaging could be better.",
                "URGENT: Received wrong item. Need immediate refund!",
                "The product works as described. Delivery was on time.",
                "Worst purchase ever! Complete waste of money. Want my refund NOW!",
                "Nice product but a bit overpriced compared to competitors.",
                "Package was damaged during shipping but product was fine.",
                "Excellent customer service! They resolved my issue quickly.",
                "Product quality is okay but not worth the premium price.",
                "Love it! Exceeded my expectations in every way.",
                "Shipping took forever and no tracking updates provided.",
            ]

            print(f"Processing {len(sample_reviews)} reviews...\n")

            # Process reviews
            start_time = time.time()
            results = workflow.process_batch(sample_reviews)
            process_time = time.time() - start_time

            print(
                f"Processed in {process_time:.2f}s "
                f"({len(sample_reviews)/process_time:.1f} reviews/sec)\n"
            )

            # Generate insights
            insights = workflow.generate_insights(results)

            print("=== Business Insights ===\n")

            print("1. Sentiment Distribution:")
            for sentiment, data in insights["sentiment_distribution"].items():
                print(f"   {sentiment}: {data['count']} ({data['percentage']:.1f}%)")

            print("\n2. Top Categories:")
            for category, count in insights["top_categories"]:
                print(f"   {category}: {count} mentions")

            print("\n3. Priority Distribution:")
            for priority, count in insights["priority_distribution"].items():
                print(f"   {priority}: {count} reviews")

            print("\n4. Urgent Reviews Requiring Attention:")
            for review in insights["urgent_reviews"]:
                print(f"\n   Priority: {review['priority'].upper()}")
                print(f"   Text: \"{review['original_text'][:100]}...\"")
                print(f"   Sentiment: {review['sentiment']} ({review['sentiment_score']:.2f})")
                print(f"   Categories: {', '.join(review['categories'])}")

            print("\n=== Workflow Performance ===")
            print(f"Total processing time: {process_time:.2f}s")
            print(f"Average time per review: {process_time/len(sample_reviews)*1000:.1f}ms")
            print(f"Throughput: {len(sample_reviews)/process_time:.1f} reviews/second")

            # Save results
            output_dir = Path("output")
            output_dir.mkdir(exist_ok=True)

            # Save detailed results
            with open(output_dir / "review_analysis_results.json", "w") as f:
                json.dump(results, f, indent=2)

            # Save insights
            with open(output_dir / "business_insights.json", "w") as f:
                json.dump(insights, f, indent=2)

            print(f"\nResults saved to {output_dir}/")

            print("\n=== Next Steps ===")
            print("1. Connect to real-time data stream (Kafka, SQS)")
            print("2. Set up automated alerts for urgent reviews")
            print("3. Create dashboard for insights visualization")
            print("4. Implement model monitoring and retraining")
            print("5. Add A/B testing for model improvements")


        if __name__ == "__main__":
            demonstrate_production_workflow()
      metadata:
        extension: .py
        size_bytes: 8234
        language: python
    src/synthetic_data.py:
      content: |
        """Synthetic data generation for training augmentation."""

        import json
        import random
        from collections import Counter

        import numpy as np
        import torch
        from transformers import pipeline

        from src.config import SYNTHETIC_DATA_PATH, VALIDATION_THRESHOLD


        def generate_text_variations(base_prompt, model="gpt2", num_samples=5):
            """Generate text variations using language models."""
            generator = pipeline(
                "text-generation", model=model, device=0 if torch.cuda.is_available() else -1
            )

            variations = []
            temperatures = [0.7, 0.8, 0.9, 1.0, 1.1]

            for i in range(num_samples):
                temp = temperatures[i % len(temperatures)]

                result = generator(
                    base_prompt,
                    max_new_tokens=50,
                    temperature=temp,
                    num_return_sequences=1,
                    pad_token_id=50256,  # GPT2 eos token
                    do_sample=True,
                )[0]["generated_text"]

                # Extract only the generated part
                if base_prompt in result:
                    generated = result[len(base_prompt) :].strip()
                else:
                    generated = result

                variations.append(
                    {
                        "prompt": base_prompt,
                        "generated": generated,
                        "temperature": temp,
                        "full_text": result,
                    }
                )

            return variations


        def validate_synthetic_text(synthetic_samples, real_samples):
            """Validate synthetic text quality."""
            # Calculate basic statistics
            real_lengths = [len(s.split()) for s in real_samples]
            synth_lengths = [len(s["full_text"].split()) for s in synthetic_samples]

            real_mean = np.mean(real_lengths)
            synth_mean = np.mean(synth_lengths)

            # Length similarity
            length_similarity = 1 - abs(real_mean - synth_mean) / real_mean

            # Vocabulary overlap
            real_words = set(" ".join(real_samples).lower().split())
            synth_words = set(
                " ".join([s["full_text"] for s in synthetic_samples]).lower().split()
            )
            vocab_overlap = len(real_words & synth_words) / len(real_words)

            # Diversity check
            synth_texts = [s["full_text"] for s in synthetic_samples]
            diversity = len(set(synth_texts)) / len(synth_texts)

            validation_score = (length_similarity + vocab_overlap + diversity) / 3

            return {
                "score": validation_score,
                "length_similarity": length_similarity,
                "vocab_overlap": vocab_overlap,
                "diversity": diversity,
                "passed": validation_score >= VALIDATION_THRESHOLD,
            }


        def generate_classification_data(categories, samples_per_category=10):
            """Generate synthetic classification data."""
            prompts = {
                "positive": [
                    "Write a positive product review about",
                    "Express satisfaction with",
                    "Describe why you love",
                ],
                "negative": [
                    "Write a negative review about",
                    "Express disappointment with",
                    "Explain problems with",
                ],
                "neutral": [
                    "Write a balanced review of",
                    "Give a fair assessment of",
                    "Describe the pros and cons of",
                ],
            }

            products = [
                "wireless headphones",
                "a smartphone",
                "a laptop",
                "running shoes",
                "a coffee maker",
                "a backpack",
            ]

            synthetic_data = []

            for category in categories:
                if category not in prompts:
                    continue

                for _ in range(samples_per_category):
                    prompt_template = random.choice(prompts[category])
                    product = random.choice(products)
                    full_prompt = f"{prompt_template} {product}: "

                    variations = generate_text_variations(full_prompt, num_samples=1)

                    if variations:
                        synthetic_data.append(
                            {
                                "text": variations[0]["full_text"],
                                "label": category,
                                "is_synthetic": True,
                                "prompt_used": full_prompt,
                            }
                        )

            return synthetic_data


        def demonstrate_synthetic_data():
            """Demonstrate synthetic data generation techniques."""

            print("1. Text Generation with GPT-2")
            print("-" * 40)

            # Generate product reviews
            review_prompts = [
                "This smartphone has amazing",
                "The battery life on this device",
                "I've been using this laptop for",
                "The customer service was",
            ]

            all_variations = []
            for prompt in review_prompts:
                print(f"\nPrompt: '{prompt}'")
                variations = generate_text_variations(prompt, num_samples=3)
                all_variations.extend(variations)

                for i, var in enumerate(variations):
                    print(f"  Variation {i+1} (temp={var['temperature']}):")
                    print(f"    {var['generated'][:100]}...")

            print("\n2. Synthetic Data Validation")
            print("-" * 40)

            # Mock real samples for comparison
            real_samples = [
                "This smartphone has amazing features and great battery life.",
                "The battery life on this device lasts all day with heavy use.",
                "I've been using this laptop for work and it's been reliable.",
                "The customer service was helpful and resolved my issue quickly.",
            ]

            validation_results = validate_synthetic_text(all_variations, real_samples)

            print("Validation Results:")
            print(f"  Overall Score: {validation_results['score']:.3f}")
            print(f"  Length Similarity: {validation_results['length_similarity']:.3f}")
            print(f"  Vocabulary Overlap: {validation_results['vocab_overlap']:.3f}")
            print(f"  Diversity: {validation_results['diversity']:.3f}")
            print(f"  Passed: {'âœ“' if validation_results['passed'] else 'âœ—'}")

            print("\n3. Classification Data Generation")
            print("-" * 40)

            synthetic_classification = generate_classification_data(
                ["positive", "negative", "neutral"], samples_per_category=3
            )

            print(f"Generated {len(synthetic_classification)} classification samples:")
            for sample in synthetic_classification[:5]:
                print(f"\nLabel: {sample['label']}")
                print(f"Text: {sample['text'][:150]}...")

            print("\n4. Data Augmentation Strategy")
            print("-" * 40)

            # Simulate class imbalance
            original_distribution = {"positive": 1000, "negative": 200, "neutral": 300}

            print("Original distribution:")
            for label, count in original_distribution.items():
                print(f"  {label}: {count} samples")

            # Calculate how many synthetic samples needed
            max_count = max(original_distribution.values())
            augmentation_needed = {}

            for label, count in original_distribution.items():
                if count < max_count * 0.8:  # Augment if less than 80% of max
                    needed = int(max_count * 0.8 - count)
                    augmentation_needed[label] = needed

            print("\nAugmentation plan:")
            for label, needed in augmentation_needed.items():
                print(f"  Generate {needed} synthetic {label} samples")

            print("\n5. Quality Control Pipeline")
            print("-" * 40)

            def quality_filter(samples):
                """Filter out low-quality synthetic samples."""
                filtered = []
                reasons = Counter()

                for sample in samples:
                    text = sample["text"]

                    # Length check
                    if len(text.split()) < 5:
                        reasons["too_short"] += 1
                        continue

                    # Repetition check
                    words = text.lower().split()
                    if len(words) > 0 and len(set(words)) / len(words) < 0.5:
                        reasons["repetitive"] += 1
                        continue

                    # Truncation check
                    if text.endswith("...") or text.count("...") > 2:
                        reasons["truncated"] += 1
                        continue

                    filtered.append(sample)

                return filtered, reasons

            filtered_samples, filter_reasons = quality_filter(synthetic_classification)

            print("Quality filtering results:")
            print(f"  Input samples: {len(synthetic_classification)}")
            print(f"  Output samples: {len(filtered_samples)}")
            print(f"  Filtered out: {len(synthetic_classification) - len(filtered_samples)}")

            if filter_reasons:
                print("  Reasons:")
                for reason, count in filter_reasons.items():
                    print(f"    {reason}: {count}")

            # Save synthetic data
            output_file = SYNTHETIC_DATA_PATH / "synthetic_samples.json"
            with open(output_file, "w") as f:
                json.dump(filtered_samples, f, indent=2)

            print(f"\nSaved {len(filtered_samples)} samples to {output_file}")


        if __name__ == "__main__":
            demonstrate_synthetic_data()
      metadata:
        extension: .py
        size_bytes: 8614
        language: python
    src/custom_pipelines.py:
      content: |
        """Custom pipeline creation and composition examples."""

        import time

        from transformers import (
            AutoModelForSequenceClassification,
            AutoTokenizer,
            Pipeline,
            pipeline,
        )

        # Note: register_pipeline API has changed in newer versions
        # For demo purposes, we'll show the concept without actual registration

        from src.config import BATCH_SIZE, DEFAULT_NER_MODEL, DEFAULT_SENTIMENT_MODEL, DEVICE


        class CustomSentimentPipeline(Pipeline):
            """Custom sentiment pipeline with preprocessing and business logic."""

            def preprocess(self, inputs):
                """Clean and normalize text before processing."""

                if isinstance(inputs, str):
                    inputs = [inputs]

                cleaned = []
                for text in inputs:
                    # Remove HTML tags
                    import re

                    text = re.sub(r"<.*?>", "", text)

                    # Normalize
                    text = text.lower().strip()

                    # Remove excessive punctuation
                    text = re.sub(r"[!?]{2,}", "!", text)

                    cleaned.append(text)

                return super().preprocess(cleaned)

            def postprocess(self, outputs):
                """Add business logic to outputs."""
                results = super().postprocess(outputs)

                for result in results:
                    # Add confidence level
                    score = result["score"]
                    if score > 0.95:
                        result["confidence"] = "high"
                    elif score > 0.8:
                        result["confidence"] = "medium"
                    else:
                        result["confidence"] = "low"

                    # Add action recommendation
                    if result["label"] == "NEGATIVE" and score > 0.9:
                        result["action"] = "urgent_review"
                    elif result["label"] == "NEGATIVE":
                        result["action"] = "review"
                    else:
                        result["action"] = "none"

                return results


        class SentimentNERPipeline(Pipeline):
            """Composite pipeline for sentiment + entity recognition."""

            def __init__(self, sentiment_pipeline, ner_pipeline, **kwargs):
                self.sentiment_pipeline = sentiment_pipeline
                self.ner_pipeline = ner_pipeline
                super().__init__(
                    model=sentiment_pipeline.model,
                    tokenizer=sentiment_pipeline.tokenizer,
                    **kwargs,
                )

            def _forward(self, model_inputs):
                """Run both pipelines and combine results."""
                # Get the original text
                texts = model_inputs.get("texts", [])

                # Run sentiment analysis
                sentiment_results = self.sentiment_pipeline(texts)

                # Run NER
                ner_results = self.ner_pipeline(texts)

                # Combine results
                combined = []
                for i, text in enumerate(texts):
                    combined.append(
                        {
                            "text": text,
                            "sentiment": (
                                sentiment_results[i] if i < len(sentiment_results) else None
                            ),
                            "entities": ner_results[i] if i < len(ner_results) else [],
                        }
                    )

                return combined

            def preprocess(self, inputs):
                """Store original text for later use."""
                if isinstance(inputs, str):
                    inputs = [inputs]
                return {"texts": inputs}

            def postprocess(self, outputs):
                """Return combined results."""
                return outputs


        def demonstrate_custom_pipelines():
            """Demonstrate custom pipeline creation and usage."""

            print("1. Standard Pipeline Baseline")
            print("-" * 40)

            # Standard pipeline
            standard_pipe = pipeline(
                "sentiment-analysis",
                model=DEFAULT_SENTIMENT_MODEL,
                device=0 if DEVICE == "cuda" else -1,
            )

            test_texts = [
                "This product is absolutely amazing! Best purchase ever!!!",
                "Terrible quality... Broke after 2 days :(",
                "<p>Good product overall.</p> Would recommend.",
                "The service was okay, nothing special.",
            ]

            start = time.time()
            standard_results = standard_pipe(test_texts, batch_size=BATCH_SIZE)
            standard_time = time.time() - start

            print(f"Standard pipeline results ({standard_time:.3f}s):")
            for text, result in zip(test_texts, standard_results, strict=False):
                print(f"  '{text[:50]}...' -> {result}")

            print("\n2. Custom Sentiment Pipeline")
            print("-" * 40)

            # Create custom pipeline
            model = AutoModelForSequenceClassification.from_pretrained(DEFAULT_SENTIMENT_MODEL)
            tokenizer = AutoTokenizer.from_pretrained(DEFAULT_SENTIMENT_MODEL)

            custom_pipe = CustomSentimentPipeline(
                model=model, tokenizer=tokenizer, device=0 if DEVICE == "cuda" else -1
            )

            start = time.time()
            custom_results = custom_pipe(test_texts, batch_size=BATCH_SIZE)
            custom_time = time.time() - start

            print(f"Custom pipeline results ({custom_time:.3f}s):")
            for text, result in zip(test_texts, custom_results, strict=False):
                print(f"  '{text[:30]}...' ->")
                print(f"    Label: {result['label']}, Score: {result['score']:.3f}")
                print(f"    Confidence: {result['confidence']}, Action: {result['action']}")

            print("\n3. Composite Pipeline (Sentiment + NER)")
            print("-" * 40)

            # Note: In newer versions of transformers, the registration API has changed
            # For demonstration, we'll create the pipeline directly
            # register_pipeline(
            #     task="sentiment-ner", pipeline_class=SentimentNERPipeline, pt_model=True
            # )

            # Create component pipelines
            sentiment_pipe = pipeline(
                "sentiment-analysis",
                model=DEFAULT_SENTIMENT_MODEL,
                device=0 if DEVICE == "cuda" else -1,
            )

            ner_pipe = pipeline(
                "ner", model=DEFAULT_NER_MODEL, device=0 if DEVICE == "cuda" else -1
            )

            # Create composite pipeline
            composite_pipe = SentimentNERPipeline(
                sentiment_pipeline=sentiment_pipe, ner_pipeline=ner_pipe
            )

            business_texts = [
                "Apple Inc. makes amazing products! Tim Cook is a visionary.",
                "Amazon's delivery was terrible. Jeff Bezos should fix this.",
                "Microsoft Teams worked well for our meeting in Seattle.",
            ]

            start = time.time()
            composite_results = composite_pipe(business_texts)
            composite_time = time.time() - start

            print(f"Composite pipeline results ({composite_time:.3f}s):")
            for result in composite_results:
                print(f"\nText: '{result['text'][:60]}...'")
                print(f"Sentiment: {result['sentiment']}")
                if result["entities"]:
                    print("Entities found:")
                    for entity in result["entities"]:
                        print(f"  - {entity['word']} ({entity['entity']})")

            print("\n4. Performance Comparison")
            print("-" * 40)
            print(f"Standard pipeline: {standard_time:.3f}s")
            print(f"Custom pipeline: {custom_time:.3f}s")
            print(f"Composite pipeline: {composite_time:.3f}s")
            print(
                f"\nCustom preprocessing overhead: {((custom_time/standard_time)-1)*100:.1f}%"
            )


        if __name__ == "__main__":
            demonstrate_custom_pipelines()
      metadata:
        extension: .py
        size_bytes: 7013
        language: python
    src/utils.py:
      content: |
        """Utility functions for workflow examples."""

        import time
        from functools import wraps

        import GPUtil
        import psutil
        import torch


        def timer(func):
            """Decorator to time function execution."""

            @wraps(func)
            def wrapper(*args, **kwargs):
                start = time.time()
                result = func(*args, **kwargs)
                end = time.time()
                print(f"{func.__name__} took {end - start:.3f}s")
                return result

            return wrapper


        # Alias for compatibility
        timer_decorator = timer


        def format_size(size_bytes):
            """Format byte size into human readable string."""
            for unit in ["B", "KB", "MB", "GB", "TB"]:
                if size_bytes < 1024.0:
                    return f"{size_bytes:.2f} {unit}"
                size_bytes /= 1024.0
            return f"{size_bytes:.2f} PB"


        def get_system_info():
            """Get current system resource usage."""
            info = {
                "cpu_percent": psutil.cpu_percent(interval=1),
                "memory_percent": psutil.virtual_memory().percent,
                "memory_used_gb": psutil.virtual_memory().used / (1024**3),
            }

            if torch.cuda.is_available():
                gpu = GPUtil.getGPUs()[0]
                info.update(
                    {
                        "gpu_name": gpu.name,
                        "gpu_memory_percent": gpu.memoryUtil * 100,
                        "gpu_memory_used_mb": gpu.memoryUsed,
                        "gpu_temperature": gpu.temperature,
                    }
                )

            return info


        def format_number(num):
            """Format large numbers with K/M/B suffixes."""
            if num < 1000:
                return str(num)
            elif num < 1_000_000:
                return f"{num/1000:.1f}K"
            elif num < 1_000_000_000:
                return f"{num/1_000_000:.1f}M"
            else:
                return f"{num/1_000_000_000:.1f}B"


        def calculate_model_size(model):
            """Calculate model size in MB."""
            param_size = 0
            buffer_size = 0

            for param in model.parameters():
                param_size += param.numel() * param.element_size()

            for buffer in model.buffers():
                buffer_size += buffer.numel() * buffer.element_size()

            total_size = (param_size + buffer_size) / 1024 / 1024  # MB

            return {
                "total_size_mb": total_size,
                "param_size_mb": param_size / 1024 / 1024,
                "buffer_size_mb": buffer_size / 1024 / 1024,
                "total_params": sum(p.numel() for p in model.parameters()),
            }


        def ensure_reproducibility(seed=42):
            """Set random seeds for reproducibility."""
            import random

            import numpy as np

            random.seed(seed)
            np.random.seed(seed)
            torch.manual_seed(seed)

            if torch.cuda.is_available():
                torch.cuda.manual_seed(seed)
                torch.cuda.manual_seed_all(seed)
                torch.backends.cudnn.deterministic = True
                torch.backends.cudnn.benchmark = False


        class MemoryTracker:
            """Context manager to track memory usage."""

            def __init__(self, device="cuda"):
                self.device = device
                self.start_memory = 0
                self.peak_memory = 0

            def __enter__(self):
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.reset_peak_memory_stats()
                    torch.cuda.synchronize()
                    self.start_memory = torch.cuda.memory_allocated()
                return self

            def __exit__(self, *args):
                if self.device == "cuda" and torch.cuda.is_available():
                    torch.cuda.synchronize()
                    self.peak_memory = torch.cuda.max_memory_allocated()

            def get_memory_used(self):
                """Get memory used in MB."""
                return (self.peak_memory - self.start_memory) / 1024 / 1024


        def create_model_card(model_info):
            """Create a model card with key information."""
            card = f"""
        # Model Card

        ## Model Details
        - **Name**: {model_info.get('name', 'Unknown')}
        - **Type**: {model_info.get('type', 'Unknown')}
        - **Size**: {model_info.get('size_mb', 0):.1f}MB
        - **Parameters**: {format_number(model_info.get('parameters', 0))}

        ## Performance
        - **Inference Speed**: {model_info.get('inference_ms', 0):.1f}ms
        - **Throughput**: {model_info.get('throughput', 0):.1f} samples/sec
        - **Memory Usage**: {model_info.get('memory_mb', 0):.1f}MB

        ## Training
        - **Dataset**: {model_info.get('dataset', 'Unknown')}
        - **Epochs**: {model_info.get('epochs', 'Unknown')}
        - **Batch Size**: {model_info.get('batch_size', 'Unknown')}

        ## Usage
        ```python
        from transformers import pipeline

        pipe = pipeline(
            '{model_info.get('task', 'text-classification')}',
            model='{model_info.get('name', 'model-name')}'
        )
        result = pipe("Your text here")
        ```
        """
            return card.strip()


        if __name__ == "__main__":
            # Test utilities
            print("System Info:")
            print(get_system_info())

            print("\nMemory Tracking Test:")
            with MemoryTracker() as tracker:
                # Simulate some GPU operations
                if torch.cuda.is_available():
                    x = torch.randn(1000, 1000).cuda()
                    y = torch.randn(1000, 1000).cuda()
                    z = torch.matmul(x, y)

            print(f"Memory used: {tracker.get_memory_used():.2f}MB")
      metadata:
        extension: .py
        size_bytes: 4927
        language: python
    src/flash_attention.py:
      content: |
        """
        Flash Attention demonstration for efficient transformer inference.

        This module shows how to use Flash Attention and other attention optimizations
        to significantly speed up transformer model inference on GPUs.
        """

        import time
        from typing import Any

        import torch
        from transformers import (
            AutoModelForCausalLM,
            AutoModelForSequenceClassification,
            AutoTokenizer,
        )

        from src.config import Config
        from src.utils import format_size, timer_decorator


        class FlashAttentionDemo:
            """Demonstrates Flash Attention and attention optimization techniques."""

            def __init__(self):
                """Initialize Flash Attention demo."""
                self.device = Config.DEVICE
                self.has_flash_attn = self._check_flash_attention()

            def _check_flash_attention(self) -> bool:
                """Check if Flash Attention is available."""
                try:
                    # Check for flash_attn package
                    import flash_attn

                    print("âœ… Flash Attention package found")

                    # Check CUDA capability
                    if torch.cuda.is_available():
                        capability = torch.cuda.get_device_capability()
                        if capability[0] >= 8:  # Ampere or newer
                            print(
                                f"âœ… GPU supports Flash Attention (compute capability {capability[0]}.{capability[1]})"
                            )
                            return True
                        else:
                            print(
                                f"âš ï¸  GPU compute capability {capability[0]}.{capability[1]} < 8.0 (Ampere)"
                            )
                    return False
                except ImportError:
                    print(
                        "âš ï¸  Flash Attention not installed. Install with: pip install flash-attn"
                    )
                    return False

            @timer_decorator
            def compare_attention_methods(
                self, model_name: str = "gpt2", sequence_length: int = 512, batch_size: int = 8
            ) -> dict[str, Any]:
                """
                Compare different attention implementations.

                Args:
                    model_name: Model to test
                    sequence_length: Input sequence length
                    batch_size: Batch size for testing

                Returns:
                    Comparison metrics
                """
                print(f"\nðŸ”¬ Comparing Attention Methods for {model_name}")
                print(f"   Sequence length: {sequence_length}, Batch size: {batch_size}")

                tokenizer = AutoTokenizer.from_pretrained(model_name)
                tokenizer.pad_token = tokenizer.eos_token

                # Create dummy input
                dummy_text = " ".join(["Sample text"] * (sequence_length // 10))
                inputs = tokenizer(
                    [dummy_text] * batch_size,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=sequence_length,
                ).to(self.device)

                results = {}

                # 1. Standard Attention
                print("\nðŸ“Š Testing Standard Attention...")
                model_standard = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
                    device_map="auto",
                )

                # Warm up
                with torch.no_grad():
                    _ = model_standard(**inputs)

                # Benchmark
                if self.device.type == "cuda":
                    torch.cuda.synchronize()
                    start_mem = torch.cuda.memory_allocated()

                start_time = time.time()
                with torch.no_grad():
                    for _ in range(10):
                        _ = model_standard(**inputs)

                if self.device.type == "cuda":
                    torch.cuda.synchronize()

                standard_time = time.time() - start_time
                standard_mem = (
                    torch.cuda.memory_allocated() - start_mem
                    if self.device.type == "cuda"
                    else 0
                )

                results["standard"] = {
                    "time": f"{standard_time:.3f}s",
                    "memory": format_size(standard_mem),
                    "throughput": f"{(10 * batch_size) / standard_time:.1f} samples/s",
                }

                del model_standard
                if self.device.type == "cuda":
                    torch.cuda.empty_cache()

                # 2. Flash Attention (if available)
                if self.has_flash_attn and self.device.type == "cuda":
                    print("\nâš¡ Testing Flash Attention...")

                    # Load model with Flash Attention
                    model_flash = AutoModelForCausalLM.from_pretrained(
                        model_name,
                        torch_dtype=torch.float16,
                        device_map="auto",
                        use_flash_attention_2=True,  # Enable Flash Attention 2
                    )

                    # Warm up
                    with torch.no_grad():
                        _ = model_flash(**inputs)

                    # Benchmark
                    torch.cuda.synchronize()
                    start_mem = torch.cuda.memory_allocated()

                    start_time = time.time()
                    with torch.no_grad():
                        for _ in range(10):
                            _ = model_flash(**inputs)

                    torch.cuda.synchronize()
                    flash_time = time.time() - start_time
                    flash_mem = torch.cuda.memory_allocated() - start_mem

                    results["flash_attention"] = {
                        "time": f"{flash_time:.3f}s",
                        "memory": format_size(flash_mem),
                        "throughput": f"{(10 * batch_size) / flash_time:.1f} samples/s",
                        "speedup": f"{standard_time / flash_time:.2f}x",
                    }

                    del model_flash
                    torch.cuda.empty_cache()

                # 3. Scaled Dot Product Attention (SDPA) - PyTorch 2.0+
                if torch.__version__ >= "2.0.0":
                    print("\nðŸ”§ Testing Scaled Dot Product Attention (SDPA)...")

                    model_sdpa = AutoModelForCausalLM.from_pretrained(
                        model_name,
                        torch_dtype=(
                            torch.float16 if self.device.type == "cuda" else torch.float32
                        ),
                        device_map="auto",
                        attn_implementation="sdpa",  # Use PyTorch's SDPA
                    )

                    # Warm up
                    with torch.no_grad():
                        _ = model_sdpa(**inputs)

                    # Benchmark
                    if self.device.type == "cuda":
                        torch.cuda.synchronize()
                        start_mem = torch.cuda.memory_allocated()

                    start_time = time.time()
                    with torch.no_grad():
                        for _ in range(10):
                            _ = model_sdpa(**inputs)

                    if self.device.type == "cuda":
                        torch.cuda.synchronize()

                    sdpa_time = time.time() - start_time
                    sdpa_mem = (
                        torch.cuda.memory_allocated() - start_mem
                        if self.device.type == "cuda"
                        else 0
                    )

                    results["sdpa"] = {
                        "time": f"{sdpa_time:.3f}s",
                        "memory": format_size(sdpa_mem),
                        "throughput": f"{(10 * batch_size) / sdpa_time:.1f} samples/s",
                        "speedup": f"{standard_time / sdpa_time:.2f}x",
                    }

                    del model_sdpa
                    if self.device.type == "cuda":
                        torch.cuda.empty_cache()

                # Print results
                print("\nðŸ“Š Attention Method Comparison:")
                print(
                    f"{'Method':<20} {'Time':<10} {'Memory':<15} {'Throughput':<20} {'Speedup':<10}"
                )
                print("-" * 85)
                for method, metrics in results.items():
                    speedup = metrics.get("speedup", "1.00x")
                    print(
                        f"{method:<20} {metrics['time']:<10} {metrics['memory']:<15} "
                        f"{metrics['throughput']:<20} {speedup:<10}"
                    )

                return results

            def demonstrate_memory_efficient_attention(
                self, model_name: str = "gpt2-medium", max_length: int = 1024
            ) -> dict[str, Any]:
                """
                Demonstrate memory-efficient attention for long sequences.

                Args:
                    model_name: Model to use
                    max_length: Maximum sequence length

                Returns:
                    Memory usage comparison
                """
                print("\nðŸ’¾ Memory-Efficient Attention Demo")

                tokenizer = AutoTokenizer.from_pretrained(model_name)
                tokenizer.pad_token = tokenizer.eos_token

                # Create long input
                long_text = " ".join(["This is a long text sequence."] * (max_length // 10))
                inputs = tokenizer(
                    long_text, return_tensors="pt", truncation=True, max_length=max_length
                ).to(self.device)

                results = {}

                # Test different configurations
                configs = [
                    {"name": "Standard FP32", "dtype": torch.float32, "use_cache": True},
                    {"name": "FP16", "dtype": torch.float16, "use_cache": True},
                    {"name": "FP16 No Cache", "dtype": torch.float16, "use_cache": False},
                ]

                if self.has_flash_attn and self.device.type == "cuda":
                    configs.append(
                        {
                            "name": "Flash Attention",
                            "dtype": torch.float16,
                            "use_cache": False,
                            "use_flash": True,
                        }
                    )

                for config in configs:
                    print(f"\nðŸ“Š Testing {config['name']}...")

                    if self.device.type == "cuda":
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                        start_mem = torch.cuda.memory_allocated()

                    # Load model with config
                    model_kwargs = {
                        "torch_dtype": config["dtype"],
                        "device_map": "auto",
                        "use_cache": config["use_cache"],
                    }

                    if config.get("use_flash"):
                        model_kwargs["use_flash_attention_2"] = True

                    model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)

                    # Forward pass
                    with torch.no_grad():
                        outputs = model(**inputs)

                    if self.device.type == "cuda":
                        torch.cuda.synchronize()
                        peak_mem = torch.cuda.memory_allocated() - start_mem
                    else:
                        peak_mem = 0

                    results[config["name"]] = {
                        "peak_memory": format_size(peak_mem),
                        "dtype": str(config["dtype"]).split(".")[-1],
                        "use_cache": config["use_cache"],
                    }

                    del model
                    if self.device.type == "cuda":
                        torch.cuda.empty_cache()

                # Print comparison
                print("\nðŸ“Š Memory Usage Comparison:")
                print(
                    f"{'Configuration':<20} {'Peak Memory':<15} {'Data Type':<10} {'KV Cache':<10}"
                )
                print("-" * 65)
                for name, metrics in results.items():
                    print(
                        f"{name:<20} {metrics['peak_memory']:<15} {metrics['dtype']:<10} "
                        f"{'Yes' if metrics['use_cache'] else 'No':<10}"
                    )

                return results

            def benchmark_batch_processing(
                self,
                model_name: str = "distilbert-base-uncased",
                batch_sizes: list[int] = [1, 4, 8, 16, 32],
            ) -> dict[str, Any]:
                """
                Benchmark attention performance with different batch sizes.

                Args:
                    model_name: Model to benchmark
                    batch_sizes: List of batch sizes to test

                Returns:
                    Benchmark results
                """
                print("\nðŸ“ˆ Batch Processing Benchmark with Attention Optimizations")

                model = AutoModelForSequenceClassification.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
                    device_map="auto",
                )
                tokenizer = AutoTokenizer.from_pretrained(model_name)

                results = []

                for batch_size in batch_sizes:
                    print(f"\nðŸ“Š Testing batch size: {batch_size}")

                    # Create batch
                    texts = ["This is a sample text for benchmarking."] * batch_size
                    inputs = tokenizer(
                        texts,
                        return_tensors="pt",
                        padding=True,
                        truncation=True,
                        max_length=128,
                    ).to(self.device)

                    # Warm up
                    with torch.no_grad():
                        _ = model(**inputs)

                    # Benchmark
                    if self.device.type == "cuda":
                        torch.cuda.synchronize()

                    start_time = time.time()
                    with torch.no_grad():
                        for _ in range(20):
                            _ = model(**inputs)

                    if self.device.type == "cuda":
                        torch.cuda.synchronize()

                    total_time = time.time() - start_time

                    results.append(
                        {
                            "batch_size": batch_size,
                            "total_time": f"{total_time:.3f}s",
                            "throughput": f"{(20 * batch_size) / total_time:.1f} samples/s",
                            "avg_latency": f"{(total_time / 20) * 1000:.1f}ms",
                        }
                    )

                # Print results
                print("\nðŸ“Š Batch Processing Results:")
                print(
                    f"{'Batch Size':<12} {'Total Time':<12} {'Throughput':<20} {'Avg Latency':<15}"
                )
                print("-" * 70)
                for result in results:
                    print(
                        f"{result['batch_size']:<12} {result['total_time']:<12} "
                        f"{result['throughput']:<20} {result['avg_latency']:<15}"
                    )

                return {"batch_results": results}


        def demonstrate_flash_attention():
            """Demonstrate Flash Attention and attention optimizations."""
            print("=" * 80)
            print("âš¡ FLASH ATTENTION DEMONSTRATION")
            print("=" * 80)

            demo = FlashAttentionDemo()

            # 1. Compare attention methods
            print("\n1ï¸âƒ£ Attention Method Comparison")
            attention_results = demo.compare_attention_methods(
                sequence_length=512, batch_size=4
            )

            # 2. Memory-efficient attention
            print("\n2ï¸âƒ£ Memory-Efficient Attention")
            memory_results = demo.demonstrate_memory_efficient_attention()

            # 3. Batch processing benchmark
            print("\n3ï¸âƒ£ Batch Processing Performance")
            batch_results = demo.benchmark_batch_processing()

            print("\n" + "=" * 80)
            print("ðŸ“Š ATTENTION OPTIMIZATION SUMMARY")
            print("=" * 80)

            if demo.has_flash_attn:
                print("\nâœ… Flash Attention is available and demonstrated!")
                print("   - Significantly reduced memory usage")
                print("   - Faster processing for long sequences")
                print("   - Better scaling with batch size")
            else:
                print("\nâš ï¸  Flash Attention not available, but demonstrated:")
                print("   - PyTorch SDPA optimization")
                print("   - FP16 memory savings")
                print("   - Batch processing optimization")

            print("\nðŸ’¡ Key Takeaways:")
            print("   - Attention optimization can provide 2-4x speedup")
            print("   - Memory usage can be reduced by 50-75%")
            print("   - Larger batches are more efficient per sample")
            print("   - Flash Attention excels with long sequences")


        if __name__ == "__main__":
            demonstrate_flash_attention()
      metadata:
        extension: .py
        size_bytes: 15025
        language: python
    src/main.py:
      content: |
        """Main entry point for workflow demonstrations."""

        import argparse

        from src.advanced_quantization import demonstrate_advanced_quantization
        from src.custom_pipelines import demonstrate_custom_pipelines
        from src.data_workflows import demonstrate_data_workflows
        from src.diffusion_generation import demonstrate_diffusion_generation
        from src.edge_deployment import demonstrate_edge_deployment
        from src.flash_attention import demonstrate_flash_attention
        from src.optimization import demonstrate_optimization
        from src.peft_lora import demonstrate_peft_lora
        from src.production_workflows import demonstrate_production_workflow
        from src.synthetic_data import demonstrate_synthetic_data


        def main():
            """Run all workflow demonstrations."""
            parser = argparse.ArgumentParser(
                description="Custom Pipelines and Data Workflows Demonstrations"
            )
            parser.add_argument(
                "--demo",
                choices=[
                    "all",
                    "pipelines",
                    "data",
                    "optimization",
                    "synthetic",
                    "production",
                    "edge",
                    "peft",
                    "flash",
                    "quantization",
                    "diffusion",
                ],
                default="all",
                help="Which demonstration to run",
            )
            args = parser.parse_args()

            print("=== Hugging Face Workflow Mastery ===\n")

            if args.demo in ["all", "pipelines"]:
                print("\n--- Custom Pipeline Demonstrations ---")
                demonstrate_custom_pipelines()

            if args.demo in ["all", "data"]:
                print("\n--- Data Workflow Demonstrations ---")
                demonstrate_data_workflows()

            if args.demo in ["all", "optimization"]:
                print("\n--- Optimization Demonstrations ---")
                demonstrate_optimization()

            if args.demo in ["all", "synthetic"]:
                print("\n--- Synthetic Data Demonstrations ---")
                demonstrate_synthetic_data()

            if args.demo in ["all", "production"]:
                print("\n--- Production Workflow Demonstration ---")
                demonstrate_production_workflow()

            if args.demo in ["all", "edge"]:
                print("\n--- Edge Deployment Demonstration ---")
                demonstrate_edge_deployment()

            if args.demo in ["all", "peft"]:
                print("\n--- PEFT/LoRA Demonstration ---")
                demonstrate_peft_lora()

            if args.demo in ["all", "flash"]:
                print("\n--- Flash Attention Demonstration ---")
                demonstrate_flash_attention()

            if args.demo in ["all", "quantization"]:
                print("\n--- Advanced Quantization Demonstration ---")
                demonstrate_advanced_quantization()

            if args.demo in ["all", "diffusion"]:
                print("\n--- Diffusion Generation Demonstration ---")
                demonstrate_diffusion_generation()

            print("\nAll demonstrations complete!")


        if __name__ == "__main__":
            main()
      metadata:
        extension: .py
        size_bytes: 2806
        language: python
    src/data_workflows.py:
      content: |
        """Efficient data handling with Hugging Face Datasets."""

        import time

        import numpy as np
        import psutil
        from datasets import Dataset, load_dataset

        from src.config import CACHE_DIR, NUM_WORKERS


        def measure_memory():
            """Get current memory usage in MB."""
            return psutil.Process().memory_info().rss / 1024 / 1024


        def preprocess_batch(batch):
            """Preprocess a batch of examples."""
            # Lowercase text
            batch["text"] = [text.lower() for text in batch["text"]]

            # Add text length
            batch["length"] = [len(text.split()) for text in batch["text"]]

            # Add complexity score (simple example)
            batch["complexity"] = [
                len(set(text.split())) / len(text.split()) if len(text.split()) > 0 else 0
                for text in batch["text"]
            ]

            return batch


        def demonstrate_data_workflows():
            """Demonstrate efficient data handling techniques."""

            print("1. Standard Data Loading")
            print("-" * 40)

            # Load small dataset
            start_mem = measure_memory()
            start_time = time.time()

            dataset = load_dataset("imdb", split="train[:1000]", cache_dir=CACHE_DIR)

            load_time = time.time() - start_time
            load_mem = measure_memory() - start_mem

            print(f"Loaded {len(dataset)} examples")
            print(f"Time: {load_time:.2f}s, Memory: {load_mem:.2f}MB")
            print(f"First example: {dataset[0]['text'][:100]}...")

            print("\n2. Efficient Batch Processing")
            print("-" * 40)

            # Process without batching (slow)
            start_time = time.time()
            dataset_slow = dataset.map(
                lambda x: {"text": x["text"].lower(), "length": len(x["text"].split())},
                desc="Processing (no batching)",
            )
            no_batch_time = time.time() - start_time

            # Process with batching (fast)
            start_time = time.time()
            dataset_fast = dataset.map(
                preprocess_batch,
                batched=True,
                batch_size=100,
                num_proc=NUM_WORKERS,
                desc="Processing (batched)",
            )
            batch_time = time.time() - start_time

            print(f"Without batching: {no_batch_time:.2f}s")
            print(f"With batching: {batch_time:.2f}s")
            print(f"Speedup: {no_batch_time/batch_time:.2f}x")

            print("\n3. Filtering and Selection")
            print("-" * 40)

            # Filter long reviews
            long_reviews = dataset_fast.filter(
                lambda x: x["length"] > 100, desc="Filtering long reviews"
            )

            print(f"Original dataset: {len(dataset_fast)} examples")
            print(f"After filtering: {len(long_reviews)} examples")

            # Select specific columns
            slim_dataset = long_reviews.select_columns(["text", "length"])
            print(f"Columns after selection: {slim_dataset.column_names}")

            print("\n4. Streaming Large Datasets")
            print("-" * 40)

            # Stream dataset without loading into memory
            print("Loading Wikipedia (streaming mode)...")
            start_mem = measure_memory()

            streaming_dataset = load_dataset(
                "wikipedia",
                "20220301.simple",
                split="train",
                streaming=True,
                cache_dir=CACHE_DIR,
            )

            # Process first 100 examples
            processed_count = 0
            total_length = 0

            for example in streaming_dataset:
                processed_count += 1
                total_length += len(example["text"].split())

                if processed_count >= 100:
                    break

            stream_mem = measure_memory() - start_mem

            print(f"Processed {processed_count} Wikipedia articles")
            print(f"Average article length: {total_length/processed_count:.0f} words")
            print(f"Memory used: {stream_mem:.2f}MB (vs ~6GB for full dataset)")

            print("\n5. Creating Custom Datasets")
            print("-" * 40)

            # Create dataset from Python objects
            custom_data = {
                "text": [
                    "This is a positive review.",
                    "This is a negative review.",
                    "This is a neutral comment.",
                ],
                "label": [1, 0, 2],
                "source": ["web", "app", "email"],
            }

            custom_dataset = Dataset.from_dict(custom_data)
            print(f"Custom dataset: {custom_dataset}")

            # Add computed columns
            def add_features(example):
                example["num_words"] = len(example["text"].split())
                example["has_punctuation"] = any(c in example["text"] for c in ".,!?")
                return example

            custom_dataset = custom_dataset.map(add_features)
            print(f"With features: {custom_dataset}")
            print(f"First example: {custom_dataset[0]}")

            print("\n6. Dataset Statistics")
            print("-" * 40)

            # Compute statistics efficiently
            lengths = dataset_fast["length"]
            complexities = dataset_fast["complexity"]

            print("Text length statistics:")
            print(f"  Mean: {np.mean(lengths):.1f} words")
            print(f"  Std: {np.std(lengths):.1f} words")
            print(f"  Min: {np.min(lengths)} words")
            print(f"  Max: {np.max(lengths)} words")

            print("\nComplexity statistics:")
            print(f"  Mean: {np.mean(complexities):.3f}")
            print(f"  Std: {np.std(complexities):.3f}")


        if __name__ == "__main__":
            demonstrate_data_workflows()
      metadata:
        extension: .py
        size_bytes: 4941
        language: python
