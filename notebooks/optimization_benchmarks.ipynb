{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Optimization Benchmarks\n",
    "\n",
    "This notebook demonstrates various optimization techniques and their impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from src.optimization import benchmark_inference\n",
    "from src.utils import calculate_model_size, MemoryTracker\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Model info\n",
    "model_info = calculate_model_size(model)\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Size: {model_info['total_size_mb']:.1f}MB\")\n",
    "print(f\"Parameters: {model_info['total_params']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quantization Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test texts\n",
    "test_texts = [\n",
    "    \"This is a positive review.\",\n",
    "    \"This is a negative review.\",\n",
    "    \"This is a neutral statement.\"\n",
    "] * 10\n",
    "\n",
    "# Benchmark FP32\n",
    "fp32_time, fp32_mem = benchmark_inference(\n",
    "    model, tokenizer, test_texts, \"FP32 Model\"\n",
    ")\n",
    "\n",
    "# Quantize and benchmark\n",
    "if torch.cuda.is_available():\n",
    "    # FP16\n",
    "    model_fp16 = model.half()\n",
    "    fp16_time, fp16_mem = benchmark_inference(\n",
    "        model_fp16, tokenizer, test_texts, \"FP16 Model\"\n",
    "    )\n",
    "else:\n",
    "    # INT8 for CPU\n",
    "    model_int8 = torch.quantization.quantize_dynamic(\n",
    "        model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "    )\n",
    "    int8_time, int8_mem = benchmark_inference(\n",
    "        model_int8, tokenizer, test_texts, \"INT8 Model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Batch Size Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different batch sizes\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32]\n",
    "throughputs = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    test_batch = test_texts[:batch_size]\n",
    "    time_taken, _ = benchmark_inference(\n",
    "        model, tokenizer, test_batch, f\"Batch size {batch_size}\"\n",
    "    )\n",
    "    throughput = len(test_batch) / time_taken\n",
    "    throughputs.append(throughput)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(batch_sizes, throughputs, 'b-o')\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Throughput (samples/sec)')\n",
    "plt.title('Throughput vs Batch Size')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile memory usage\n",
    "with MemoryTracker() as tracker:\n",
    "    inputs = tokenizer(\n",
    "        test_texts, \n",
    "        padding=True, \n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        model = model.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "print(f\"Peak memory usage: {tracker.get_memory_used():.2f}MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimization Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison chart\n",
    "techniques = ['FP32', 'FP16/INT8', 'Batching']\n",
    "speedups = [1.0, 2.0, 5.0]  # Example values\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(techniques, speedups, color=['blue', 'green', 'orange'])\n",
    "plt.ylabel('Speedup Factor')\n",
    "plt.title('Optimization Technique Comparison')\n",
    "plt.ylim(0, 6)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, speedup in zip(bars, speedups):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "             f'{speedup}x', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
