{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports loaded successfully!\n",
      "PyTorch version: 2.7.1\n",
      "Device available: MPS\n"
     ]
    }
   ],
   "source": [
    "# Essential imports for this notebook\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import transformers components\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "print(\"‚úÖ All imports loaded successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {torch.cuda.is_available() and 'CUDA' or ('MPS' if torch.backends.mps.is_available() else 'CPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentiment analysis pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline loaded successfully!\n",
      "Bias Detection Report:\n",
      "======================\n",
      "\n",
      "Gender:\n",
      "  Total mentions: 6\n",
      "  Positive rate: 50.0%\n",
      "  Negative rate: 50.0%\n",
      "\n",
      "Age:\n",
      "  Total mentions: 2\n",
      "  Positive rate: 50.0%\n",
      "  Negative rate: 50.0%\n",
      "\n",
      "üí° Mitigation Strategies:\n",
      "  - Use balanced training data\n",
      "  - Apply debiasing techniques during fine-tuning\n",
      "  - Implement fairness constraints\n",
      "  - Regular bias audits in production\n"
     ]
    }
   ],
   "source": [
    "# First, let's import necessary libraries and create the pipeline\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Create the sentiment analysis pipeline\n",
    "print(\"Loading sentiment analysis pipeline...\")\n",
    "clf = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "print(\"Pipeline loaded successfully!\")\n",
    "\n",
    "# Now let's implement bias detection\n",
    "def check_bias_in_predictions(texts, predictions):\n",
    "    \"\"\"Check for potential bias in model predictions.\"\"\"\n",
    "    \n",
    "    # Demographic keywords to check\n",
    "    demographic_terms = {\n",
    "        \"gender\": [\"man\", \"woman\", \"male\", \"female\", \"he\", \"she\"],\n",
    "        \"age\": [\"young\", \"old\", \"elderly\", \"teenager\"],\n",
    "        \"ethnicity\": [\"asian\", \"black\", \"white\", \"hispanic\", \"latino\"]\n",
    "    }\n",
    "    \n",
    "    # Analyze sentiment distribution by demographic mentions\n",
    "    bias_report = {category: {\"positive\": 0, \"negative\": 0, \"total\": 0} \n",
    "                   for category in demographic_terms.keys()}\n",
    "    \n",
    "    for text, pred in zip(texts, predictions):\n",
    "        text_lower = text.lower()\n",
    "        for category, terms in demographic_terms.items():\n",
    "            if any(term in text_lower for term in terms):\n",
    "                bias_report[category][\"total\"] += 1\n",
    "                if pred[\"label\"].upper() == \"POSITIVE\":\n",
    "                    bias_report[category][\"positive\"] += 1\n",
    "                else:\n",
    "                    bias_report[category][\"negative\"] += 1\n",
    "    \n",
    "    return bias_report\n",
    "\n",
    "# Test bias detection\n",
    "test_texts_bias = [\n",
    "    \"The young engineer did excellent work.\",\n",
    "    \"The old manager was difficult to work with.\",\n",
    "    \"She delivered the project on time.\",\n",
    "    \"He failed to meet expectations.\",\n",
    "    \"The woman presented innovative ideas.\",\n",
    "    \"The man's proposal was rejected.\"\n",
    "]\n",
    "\n",
    "# Get predictions\n",
    "predictions_bias = clf(test_texts_bias)\n",
    "\n",
    "# Check for bias\n",
    "bias_results = check_bias_in_predictions(test_texts_bias, predictions_bias)\n",
    "\n",
    "print(\"Bias Detection Report:\")\n",
    "print(\"======================\")\n",
    "for category, stats in bias_results.items():\n",
    "    if stats[\"total\"] > 0:\n",
    "        pos_rate = stats[\"positive\"] / stats[\"total\"]\n",
    "        print(f\"\\n{category.capitalize()}:\")\n",
    "        print(f\"  Total mentions: {stats['total']}\")\n",
    "        print(f\"  Positive rate: {pos_rate:.1%}\")\n",
    "        print(f\"  Negative rate: {1-pos_rate:.1%}\")\n",
    "        \n",
    "        # Flag potential bias\n",
    "        if pos_rate < 0.3 or pos_rate > 0.7:\n",
    "            print(\"  ‚ö†Ô∏è Potential bias detected!\")\n",
    "\n",
    "print(\"\\nüí° Mitigation Strategies:\")\n",
    "print(\"  - Use balanced training data\")\n",
    "print(\"  - Apply debiasing techniques during fine-tuning\")\n",
    "print(\"  - Implement fairness constraints\")\n",
    "print(\"  - Regular bias audits in production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical AI and Bias Detection\n",
    "\n",
    "Implementing fairness checks in production pipelines (2025 best practices)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2025 Updates\n",
    "\n",
    "This notebook has been updated to reflect the latest best practices as of 2025:\n",
    "- **Transformers**: v4.53.0+ with modern models (Phi-2, RoBERTa variants)\n",
    "- **Datasets**: v3.6.0+ with improved streaming support\n",
    "- **Quantization**: BitsAndBytesConfig for INT8/INT4 optimization\n",
    "- **QLoRA**: Quantized LoRA for memory-efficient fine-tuning\n",
    "- **Flash Attention 2**: For GPU acceleration with long sequences\n",
    "- **Ethical AI**: Built-in bias detection and fairness checks\n",
    "\n",
    "All deprecated APIs (like `register_pipeline`) have been removed or updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash Attention 2 Overview:\n",
      "==========================\n",
      "‚ö†Ô∏è Flash Attention not installed\n",
      "  Install with: pip install flash-attn\n",
      "\n",
      "‚ö†Ô∏è No CUDA GPU available - Flash Attention requires GPU\n",
      "\n",
      "üöÄ Flash Attention Benefits:\n",
      "  - 2-4x faster attention computation\n",
      "  - 10-20x memory reduction for long sequences\n",
      "  - Enables longer context windows (up to 64k)\n",
      "  - Automatic with use_flash_attention_2=True\n",
      "\n",
      "üìù Usage Example:\n",
      "\n",
      "model = AutoModelForCausalLM.from_pretrained(\n",
      "    \"microsoft/phi-2\",\n",
      "    torch_dtype=torch.float16,\n",
      "    use_flash_attention_2=True,  # Enable Flash Attention\n",
      "    device_map=\"auto\"\n",
      ")\n",
      "\n",
      "\n",
      "üìä Performance Comparison:\n",
      "| Sequence Length | Standard Attention | Flash Attention 2 | Speedup |\n",
      "|-----------------|-------------------|-------------------|---------|\n",
      "| 512 tokens      | 100ms            | 50ms              | 2x      |\n",
      "| 2048 tokens     | 800ms            | 200ms             | 4x      |\n",
      "| 8192 tokens     | OOM              | 1200ms            | ‚àû       |\n"
     ]
    }
   ],
   "source": [
    "# Flash Attention demonstration\n",
    "# First import torch if not already imported\n",
    "import torch\n",
    "\n",
    "print(\"Flash Attention 2 Overview:\")\n",
    "print(\"==========================\")\n",
    "\n",
    "# Check if Flash Attention is available\n",
    "has_flash_attn = False\n",
    "try:\n",
    "    import flash_attn\n",
    "    has_flash_attn = True\n",
    "    print(\"‚úÖ Flash Attention package found!\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Flash Attention not installed\")\n",
    "    print(\"  Install with: pip install flash-attn\")\n",
    "\n",
    "# Check GPU compatibility\n",
    "if torch.cuda.is_available():\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    print(f\"\\nGPU Compute Capability: {capability[0]}.{capability[1]}\")\n",
    "    if capability[0] >= 8:  # Ampere or newer\n",
    "        print(\"‚úÖ GPU supports Flash Attention (Ampere or newer)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è GPU may not fully support Flash Attention\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No CUDA GPU available - Flash Attention requires GPU\")\n",
    "\n",
    "print(\"\\nüöÄ Flash Attention Benefits:\")\n",
    "print(\"  - 2-4x faster attention computation\")\n",
    "print(\"  - 10-20x memory reduction for long sequences\")\n",
    "print(\"  - Enables longer context windows (up to 64k)\")\n",
    "print(\"  - Automatic with use_flash_attention_2=True\")\n",
    "\n",
    "# Configuration example\n",
    "print(\"\\nüìù Usage Example:\")\n",
    "print(\"\"\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/phi-2\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_flash_attention_2=True,  # Enable Flash Attention\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüìä Performance Comparison:\")\n",
    "print(\"| Sequence Length | Standard Attention | Flash Attention 2 | Speedup |\")\n",
    "print(\"|-----------------|-------------------|-------------------|---------|\")\n",
    "print(\"| 512 tokens      | 100ms            | 50ms              | 2x      |\")\n",
    "print(\"| 2048 tokens     | 800ms            | 200ms             | 4x      |\")\n",
    "print(\"| 8192 tokens     | OOM              | 1200ms            | ‚àû       |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flash Attention for GPU Optimization\n",
    "\n",
    "Flash Attention 2 provides significant speedups for transformer models on modern GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLoRA Configuration Example:\n",
      "================================\n",
      "\n",
      "Quantization Settings:\n",
      "  load_in_4bit: True\n",
      "  bnb_4bit_quant_type: nf4\n",
      "  bnb_4bit_compute_dtype: float16\n",
      "  bnb_4bit_use_double_quant: True\n",
      "\n",
      "LoRA Settings:\n",
      "  r: 8\n",
      "  lora_alpha: 16\n",
      "  lora_dropout: 0.1\n",
      "  target_modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj']\n",
      "\n",
      "üí° QLoRA Benefits:\n",
      "  - 75% memory reduction vs standard LoRA\n",
      "  - Enables 13B model fine-tuning on 24GB GPU\n",
      "  - Maintains 95% of full fine-tuning performance\n",
      "  - Compatible with Flash Attention 2\n",
      "\n",
      "üìä Memory Usage Comparison:\n",
      "| Method | Llama-2-7B | Llama-2-13B | Llama-2-70B |\n",
      "|--------|------------|-------------|-------------|\n",
      "| Full   | 28GB       | 52GB        | 280GB       |\n",
      "| LoRA   | 14GB       | 26GB        | 140GB       |\n",
      "| QLoRA  | 4GB        | 8GB         | 35GB        |\n"
     ]
    }
   ],
   "source": [
    "# QLoRA demonstration (conceptual - requires GPU with sufficient memory)\n",
    "print(\"QLoRA Configuration Example:\")\n",
    "print(\"================================\")\n",
    "\n",
    "# QLoRA configuration for 4-bit quantization\n",
    "qlora_config = {\n",
    "    \"quantization\": {\n",
    "        \"load_in_4bit\": True,\n",
    "        \"bnb_4bit_quant_type\": \"nf4\",\n",
    "        \"bnb_4bit_compute_dtype\": \"float16\",\n",
    "        \"bnb_4bit_use_double_quant\": True\n",
    "    },\n",
    "    \"lora\": {\n",
    "        \"r\": 8,  # Lower rank for memory efficiency\n",
    "        \"lora_alpha\": 16,\n",
    "        \"lora_dropout\": 0.1,\n",
    "        \"target_modules\": [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nQuantization Settings:\")\n",
    "for key, value in qlora_config[\"quantization\"].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nLoRA Settings:\")\n",
    "for key, value in qlora_config[\"lora\"].items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nüí° QLoRA Benefits:\")\n",
    "print(\"  - 75% memory reduction vs standard LoRA\")\n",
    "print(\"  - Enables 13B model fine-tuning on 24GB GPU\")\n",
    "print(\"  - Maintains 95% of full fine-tuning performance\")\n",
    "print(\"  - Compatible with Flash Attention 2\")\n",
    "\n",
    "# Memory comparison\n",
    "print(\"\\nüìä Memory Usage Comparison:\")\n",
    "print(\"| Method | Llama-2-7B | Llama-2-13B | Llama-2-70B |\")\n",
    "print(\"|--------|------------|-------------|-------------|\")\n",
    "print(\"| Full   | 28GB       | 52GB        | 280GB       |\")\n",
    "print(\"| LoRA   | 14GB       | 26GB        | 140GB       |\")\n",
    "print(\"| QLoRA  | 4GB        | 8GB         | 35GB        |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QLoRA: Quantized LoRA for Efficient Fine-tuning\n",
    "\n",
    "QLoRA enables fine-tuning large models on consumer GPUs by combining 4-bit quantization with LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Modern quantization with BitsAndBytesConfig\n# Import torch first\nimport torch\n\ntry:\n    from transformers import BitsAndBytesConfig\n    \n    # INT8 quantization configuration\n    quantization_config_int8 = BitsAndBytesConfig(\n        load_in_8bit=True,\n        # Remove bnb_8bit_compute_dtype as it doesn't exist in the API\n        # The compute dtype is handled automatically by the library\n    )\n    \n    # INT4 quantization configuration (for larger models)\n    quantization_config_int4 = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",  # NormalFloat4 for better accuracy\n        bnb_4bit_compute_dtype=torch.float16,\n        bnb_4bit_use_double_quant=True  # Double quantization\n    )\n    \n    print(\"‚úÖ BitsAndBytes configurations created!\")\n    print(\"\\nINT8 Config:\")\n    print(f\"  - 8-bit loading: {quantization_config_int8.load_in_8bit}\")\n    # Check available attributes\n    int8_attrs = [attr for attr in dir(quantization_config_int8) if not attr.startswith('_')]\n    print(f\"  - Available INT8 attributes: {', '.join(int8_attrs[:5])}...\")\n    \n    print(\"\\nINT4 Config:\")\n    print(f\"  - 4-bit loading: {quantization_config_int4.load_in_4bit}\")\n    print(f\"  - Quant type: {quantization_config_int4.bnb_4bit_quant_type}\")\n    print(f\"  - Compute dtype: {quantization_config_int4.bnb_4bit_compute_dtype}\")\n    print(f\"  - Double quant: {quantization_config_int4.bnb_4bit_use_double_quant}\")\n    \n    print(\"\\nüí° Usage Example:\")\n    print(\"\"\"\n# Load model with quantization\nfrom transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/phi-2\",\n    quantization_config=quantization_config_int4,\n    device_map=\"auto\"\n)\n\"\"\")\n    \n    print(\"\\nüìä Memory Savings:\")\n    print(\"| Quantization | Memory Reduction | Performance |\")\n    print(\"|--------------|------------------|-------------|\")\n    print(\"| INT8         | 50-75%          | ~95-98%     |\")\n    print(\"| INT4         | 75-87.5%        | ~90-95%     |\")\n    \nexcept ImportError:\n    print(\"‚ö†Ô∏è bitsandbytes not installed. Install with: pip install bitsandbytes\")\n    print(\"Continuing with standard models...\")\n    print(\"\\nNote: Quantization requires compatible hardware (CUDA GPUs)\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Error with BitsAndBytesConfig: {e}\")\n    print(\"This may be due to API changes or platform limitations.\")\n    print(\"Quantization is primarily supported on CUDA GPUs.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing Pipelines and Data Workflows: Advanced Models and Efficient Processing\n",
    "\n",
    "This notebook contains all examples from Chapter 8 with step-by-step explanations.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Environment Setup](#environment-setup)\n",
    "2. [Pipeline Basics and Customization](#pipeline-basics)\n",
    "3. [Efficient Data Handling](#data-handling)\n",
    "4. [Optimization Techniques](#optimization)\n",
    "5. [Synthetic Data Generation](#synthetic-data)\n",
    "6. [Production Workflows](#production)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup <a id='environment-setup'></a>\n",
    "\n",
    "First, let's set up our environment with all necessary imports and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "Environment ready!\n",
      "PyTorch version: 2.7.1\n",
      "CUDA available: False\n",
      "Device: CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardhightower/src/art_hug_08/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Add src to path for local imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    Pipeline,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "# Note: register_pipeline has been removed in newer versions\n",
    "from transformers.utils import logging\n",
    "\n",
    "# Datasets\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# For optimization examples\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# For synthetic data generation\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "print(\"Environment ready!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Configuration\n",
    "\n",
    "Let's implement the cross-platform device detection from the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "def get_optimal_device() -> torch.device:\n",
    "    \"\"\"Automatically detect best available device.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():  # Apple Silicon\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_optimal_device()\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Basics and Customization <a id='pipeline-basics'></a>\n",
    "\n",
    "### Quick Start: Modern Pipeline Usage\n",
    "\n",
    "Let's start with the basic pipeline example from the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.9708475470542908}]\n",
      "Text: 'I love this product!'\n",
      "  Sentiment: positive (confidence: 0.985)\n",
      "\n",
      "Text: 'This is terrible.'\n",
      "  Sentiment: negative (confidence: 0.915)\n",
      "\n",
      "Text: 'Not bad, but could be better.'\n",
      "  Sentiment: positive (confidence: 0.465)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Modern quick-start with explicit model and device (2025 best practice)\n",
    "clf = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='cardiffnlp/twitter-roberta-base-sentiment-latest',  # Updated modern model\n",
    "    device=0 if DEVICE.type == \"cuda\" else -1  # 0 for CUDA GPU, -1 for CPU\n",
    ")\n",
    "\n",
    "# Run prediction on text\n",
    "result = clf('I love Hugging Face!')\n",
    "print(result)\n",
    "# Expected output: [{'label': 'POSITIVE', 'score': 0.9998}]\n",
    "\n",
    "# Let's try multiple examples\n",
    "texts = [\n",
    "    \"I love this product!\",\n",
    "    \"This is terrible.\",\n",
    "    \"Not bad, but could be better.\"\n",
    "]\n",
    "results = clf(texts)\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"  Sentiment: {result['label']} (confidence: {result['score']:.3f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Preprocessing\n",
    "\n",
    "Now let's add custom preprocessing to normalize text before inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original texts: ['Wow! Amazing product!!!', \"I don't like this...\"]\n",
      "Cleaned texts: ['wow amazing product', 'i dont like this']\n",
      "\n",
      "Results after preprocessing:\n",
      "Original: 'Wow! Amazing product!!!'\n",
      "Cleaned: 'wow amazing product'\n",
      "Result: {'label': 'positive', 'score': 0.9600003957748413}\n",
      "\n",
      "Original: 'I don't like this...'\n",
      "Cleaned: 'i dont like this'\n",
      "Result: {'label': 'negative', 'score': 0.820702850818634}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def custom_preprocess(text):\n",
    "    \"\"\"Normalize text for consistent predictions.\"\"\"\n",
    "    import string\n",
    "    text = text.lower()\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Test preprocessing\n",
    "texts = [\"Wow! Amazing product!!!\", \"I don't like this...\"]\n",
    "print(\"Original texts:\", texts)\n",
    "\n",
    "# Clean then predict\n",
    "cleaned = [custom_preprocess(t) for t in texts]\n",
    "print(\"Cleaned texts:\", cleaned)\n",
    "\n",
    "# Batch processing for speed\n",
    "results = clf(cleaned, batch_size=16)\n",
    "print(\"\\nResults after preprocessing:\")\n",
    "for original, clean, result in zip(texts, cleaned, results):\n",
    "    print(f\"Original: '{original}'\")\n",
    "    print(f\"Cleaned: '{clean}'\")\n",
    "    print(f\"Result: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced: Pipeline Subclassing\n",
    "\n",
    "Create a reusable pipeline with built-in preprocessing and postprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom pipeline concept demonstrated!\n"
     ]
    }
   ],
   "source": [
    "class CustomSentimentPipeline(Pipeline):\n",
    "    def preprocess(self, inputs):\n",
    "        \"\"\"Strip HTML, normalize text.\"\"\"\n",
    "        if isinstance(inputs, str):\n",
    "            text = inputs.lower()\n",
    "            import string\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "            return super().preprocess(text)\n",
    "        return super().preprocess(inputs)\n",
    "    \n",
    "    def postprocess(self, outputs):\n",
    "        \"\"\"Add confidence thresholds.\"\"\"\n",
    "        results = super().postprocess(outputs)\n",
    "        for r in results:\n",
    "            r['confident'] = r['score'] > 0.95\n",
    "        return results\n",
    "\n",
    "# Note: In practice, you would register and use this custom pipeline\n",
    "# For now, let's demonstrate the concept with the standard pipeline\n",
    "print(\"Custom pipeline concept demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Pipeline Components\n",
    "\n",
    "Let's peek under the hood to understand pipeline anatomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming dataset...\n",
      "Dataset not loaded. Run the previous cell to load the IMDB dataset.\n"
     ]
    }
   ],
   "source": [
    "# Import if needed and check dataset status\n",
    "try:\n",
    "    # Define preprocessing function\n",
    "    def preprocess_batch(batch):\n",
    "        \"\"\"Process entire batches at once.\"\"\"\n",
    "        batch['text'] = [text.lower() for text in batch['text']]\n",
    "        batch['length'] = [len(text.split()) for text in batch['text']]\n",
    "        return batch\n",
    "\n",
    "    # Transform with parallel processing\n",
    "    print(\"Transforming dataset...\")\n",
    "    start_time = time.time()\n",
    "    dataset = dataset.map(preprocess_batch, batched=True, num_proc=4)\n",
    "    print(f\"Transformation completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    # Filter short reviews\n",
    "    print(f\"\\nDataset before filtering: {len(dataset)} examples\")\n",
    "    dataset = dataset.filter(lambda x: x['length'] > 20)\n",
    "    print(f\"Dataset after filtering: {len(dataset)} examples\")\n",
    "\n",
    "    # Check the new features\n",
    "    print(f\"\\nUpdated features: {dataset.features}\")\n",
    "    print(f\"Example with new features: {dataset[0]}\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"Dataset not loaded. Run the previous cell to load the IMDB dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composing Multiple Pipelines\n",
    "\n",
    "Let's create a combined sentiment + NER pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing streaming dataset...\n",
      "Processed 32 examples...\n",
      "Processed 64 examples...\n",
      "Processed 96 examples...\n",
      "\n",
      "Total processed: 96 examples\n"
     ]
    }
   ],
   "source": [
    "# Create a sample CSV for demonstration\n",
    "import csv\n",
    "\n",
    "sample_data = [\n",
    "    {\"text\": \"This product is amazing!\", \"label\": \"positive\"},\n",
    "    {\"text\": \"Terrible experience.\", \"label\": \"negative\"},\n",
    "    {\"text\": \"Good value for money.\", \"label\": \"positive\"},\n",
    "    {\"text\": \"Not worth the price.\", \"label\": \"negative\"},\n",
    "    {\"text\": \"Excellent quality!\", \"label\": \"positive\"}\n",
    "] * 20  # Repeat for larger dataset\n",
    "\n",
    "csv_path = \"sample_reviews.csv\"\n",
    "with open(csv_path, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['text', 'label'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(sample_data)\n",
    "\n",
    "# Stream the dataset\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "    streaming_dataset = load_dataset('csv', data_files=csv_path, split='train', streaming=True)\n",
    "    \n",
    "    # Process in batches\n",
    "    batch_size = 32\n",
    "    batch = []\n",
    "    processed_count = 0\n",
    "    \n",
    "    print(\"Processing streaming dataset...\")\n",
    "    for example in streaming_dataset:\n",
    "        batch.append(custom_preprocess(example['text']))\n",
    "        \n",
    "        if len(batch) == batch_size:\n",
    "            # Process batch\n",
    "            results = clf(batch, batch_size=batch_size)\n",
    "            processed_count += len(batch)\n",
    "            print(f\"Processed {processed_count} examples...\")\n",
    "            batch = []\n",
    "        \n",
    "        # Stop after processing 100 examples for demo\n",
    "        if processed_count >= 96:\n",
    "            break\n",
    "    \n",
    "    # Process remaining batch\n",
    "    if batch:\n",
    "        results = clf(batch)\n",
    "        processed_count += len(batch)\n",
    "    \n",
    "    print(f\"\\nTotal processed: {processed_count} examples\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error with streaming: {e}\")\n",
    "    print(\"Continuing without streaming...\")\n",
    "\n",
    "# Clean up\n",
    "if os.path.exists(csv_path):\n",
    "    os.remove(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging Pipelines\n",
    "\n",
    "Enable verbose logging to debug pipeline issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom dataset: Dataset({\n",
      "    features: ['text', 'category'],\n",
      "    num_rows: 5\n",
      "})\n",
      "\n",
      "First example: {'text': 'The future of AI is bright and full of possibilities.', 'category': 'future'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f95187822f34be08bcd136e34d930cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After transformation: {'text': 'The future of AI is bright and full of possibilities.', 'category': 'future', 'word_count': 10, 'char_count': 53}\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset if not already imported\n",
    "from datasets import Dataset\n",
    "\n",
    "# Create a custom dataset from dictionaries\n",
    "custom_data = {\n",
    "    \"text\": [\n",
    "        \"The future of AI is bright and full of possibilities.\",\n",
    "        \"Machine learning transforms how we solve complex problems.\",\n",
    "        \"Deep learning models continue to improve rapidly.\",\n",
    "        \"Natural language processing enables better human-computer interaction.\",\n",
    "        \"Computer vision applications are becoming more sophisticated.\"\n",
    "    ],\n",
    "    \"category\": [\"future\", \"ml\", \"dl\", \"nlp\", \"cv\"]\n",
    "}\n",
    "\n",
    "custom_dataset = Dataset.from_dict(custom_data)\n",
    "print(f\"Custom dataset: {custom_dataset}\")\n",
    "print(f\"\\nFirst example: {custom_dataset[0]}\")\n",
    "\n",
    "# Apply transformations\n",
    "def add_metadata(example):\n",
    "    example['word_count'] = len(example['text'].split())\n",
    "    example['char_count'] = len(example['text'])\n",
    "    return example\n",
    "\n",
    "custom_dataset = custom_dataset.map(add_metadata)\n",
    "print(f\"\\nAfter transformation: {custom_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Data Handling with ü§ó Datasets <a id='data-handling'></a>\n",
    "\n",
    "### Loading and Transforming Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b198c31cc8804afa936030d66cb20923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11be19dd2d654d5da894556ee10df810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231ad65897724c899a513378811cad9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1000\n",
      "First example: {'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n",
      "\n",
      "Features: {'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "# Import the required modules\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load a small dataset for demonstration\n",
    "dataset = load_dataset('imdb', split='train[:1000]')  # Load only first 1000 examples\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"First example: {dataset[0]}\")\n",
    "print(f\"\\nFeatures: {dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6881eae918493d887ae7a5b8bf5dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation completed in 0.23 seconds\n",
      "\n",
      "Dataset before filtering: 1000 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254b0a91b5ed4f11a414f23a55efae47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after filtering: 998 examples\n",
      "\n",
      "Updated features: {'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None), 'length': Value(dtype='int64', id=None)}\n",
      "Example with new features: {'text': 'i rented i am curious-yellow from my video store because of all the controversy that surrounded it when it was first released in 1967. i also heard that at first it was seized by u.s. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" i really had to see this for myself.<br /><br />the plot is centered around a young swedish drama student named lena who wants to learn everything she can about life. in particular she wants to focus her attentions to making some sort of documentary on what the average swede thought about certain political issues such as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />what kills me about i am curious-yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman, arguably their answer to good old boy john ford, had sex scenes in his films.<br /><br />i do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in america. i am curious-yellow is a good film for anyone wanting to study the meat and potatoes (no pun intended) of swedish cinema. but really, this film doesn\\'t have much of a plot.', 'label': 0, 'length': 288}\n"
     ]
    }
   ],
   "source": [
    "# Define preprocessing function\n",
    "def preprocess_batch(batch):\n",
    "    \"\"\"Process entire batches at once.\"\"\"\n",
    "    batch['text'] = [text.lower() for text in batch['text']]\n",
    "    batch['length'] = [len(text.split()) for text in batch['text']]\n",
    "    return batch\n",
    "\n",
    "# Transform with parallel processing\n",
    "print(\"Transforming dataset...\")\n",
    "start_time = time.time()\n",
    "dataset = dataset.map(preprocess_batch, batched=True, num_proc=4)\n",
    "print(f\"Transformation completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Filter short reviews\n",
    "print(f\"\\nDataset before filtering: {len(dataset)} examples\")\n",
    "dataset = dataset.filter(lambda x: x['length'] > 20)\n",
    "print(f\"Dataset after filtering: {len(dataset)} examples\")\n",
    "\n",
    "# Check the new features\n",
    "print(f\"\\nUpdated features: {dataset.features}\")\n",
    "print(f\"Example with new features: {dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Large Datasets\n",
    "\n",
    "For massive datasets, use streaming to avoid memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing streaming dataset...\n",
      "Processed 32 examples...\n",
      "Processed 64 examples...\n",
      "Processed 96 examples...\n",
      "\n",
      "Total processed: 96 examples\n"
     ]
    }
   ],
   "source": [
    "# Create a sample CSV for demonstration\n",
    "import csv\n",
    "\n",
    "sample_data = [\n",
    "    {\"text\": \"This product is amazing!\", \"label\": \"positive\"},\n",
    "    {\"text\": \"Terrible experience.\", \"label\": \"negative\"},\n",
    "    {\"text\": \"Good value for money.\", \"label\": \"positive\"},\n",
    "    {\"text\": \"Not worth the price.\", \"label\": \"negative\"},\n",
    "    {\"text\": \"Excellent quality!\", \"label\": \"positive\"}\n",
    "] * 20  # Repeat for larger dataset\n",
    "\n",
    "csv_path = \"sample_reviews.csv\"\n",
    "with open(csv_path, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['text', 'label'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(sample_data)\n",
    "\n",
    "# Stream the dataset\n",
    "streaming_dataset = load_dataset('csv', data_files=csv_path, split='train', streaming=True)\n",
    "\n",
    "# Process in batches\n",
    "batch_size = 32\n",
    "batch = []\n",
    "processed_count = 0\n",
    "\n",
    "print(\"Processing streaming dataset...\")\n",
    "for example in streaming_dataset:\n",
    "    batch.append(custom_preprocess(example['text']))\n",
    "    \n",
    "    if len(batch) == batch_size:\n",
    "        # Process batch\n",
    "        results = clf(batch, batch_size=batch_size)\n",
    "        processed_count += len(batch)\n",
    "        print(f\"Processed {processed_count} examples...\")\n",
    "        batch = []\n",
    "    \n",
    "    # Stop after processing 100 examples for demo\n",
    "    if processed_count >= 96:\n",
    "        break\n",
    "\n",
    "# Process remaining batch\n",
    "if batch:\n",
    "    results = clf(batch)\n",
    "    processed_count += len(batch)\n",
    "\n",
    "print(f\"\\nTotal processed: {processed_count} examples\")\n",
    "\n",
    "# Clean up\n",
    "os.remove(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Custom Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom dataset: Dataset({\n",
      "    features: ['text', 'category'],\n",
      "    num_rows: 5\n",
      "})\n",
      "\n",
      "First example: {'text': 'The future of AI is bright and full of possibilities.', 'category': 'future'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917779d223ec47c6883b411407335669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After transformation: {'text': 'The future of AI is bright and full of possibilities.', 'category': 'future', 'word_count': 10, 'char_count': 53}\n"
     ]
    }
   ],
   "source": [
    "# Create a custom dataset from dictionaries\n",
    "custom_data = {\n",
    "    \"text\": [\n",
    "        \"The future of AI is bright and full of possibilities.\",\n",
    "        \"Machine learning transforms how we solve complex problems.\",\n",
    "        \"Deep learning models continue to improve rapidly.\",\n",
    "        \"Natural language processing enables better human-computer interaction.\",\n",
    "        \"Computer vision applications are becoming more sophisticated.\"\n",
    "    ],\n",
    "    \"category\": [\"future\", \"ml\", \"dl\", \"nlp\", \"cv\"]\n",
    "}\n",
    "\n",
    "custom_dataset = Dataset.from_dict(custom_data)\n",
    "print(f\"Custom dataset: {custom_dataset}\")\n",
    "print(f\"\\nFirst example: {custom_dataset[0]}\")\n",
    "\n",
    "# Apply transformations\n",
    "def add_metadata(example):\n",
    "    example['word_count'] = len(example['text'].split())\n",
    "    example['char_count'] = len(example['text'])\n",
    "    return example\n",
    "\n",
    "custom_dataset = custom_dataset.map(add_metadata)\n",
    "print(f\"\\nAfter transformation: {custom_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Techniques <a id='optimization'></a>\n",
    "\n",
    "### Batching for 10x Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: Processing one by one...\n",
      "Time taken: 0.347 seconds\n",
      "Average per text: 43.4 ms\n",
      "\n",
      "Method 2: Batch processing...\n",
      "Time taken: 1.279 seconds\n",
      "Average per text: 40.0 ms\n",
      "\n",
      "Speedup: 1.1x faster with batching!\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data\n",
    "test_texts = [\n",
    "    \"Review 1: This product exceeded my expectations.\",\n",
    "    \"Review 2: Not satisfied with the quality.\",\n",
    "    \"Review 3: Average product, nothing special.\",\n",
    "    \"Review 4: Absolutely love it!\",\n",
    "    \"Review 5: Waste of money.\",\n",
    "    \"Review 6: Good value for the price.\",\n",
    "    \"Review 7: Would recommend to friends.\",\n",
    "    \"Review 8: Poor customer service.\"\n",
    "] * 4  # Repeat for more examples\n",
    "\n",
    "# Method 1: One by one (slow)\n",
    "print(\"Method 1: Processing one by one...\")\n",
    "start_time = time.time()\n",
    "results_single = []\n",
    "for text in test_texts[:8]:  # Process only first 8 for demo\n",
    "    result = clf(text)\n",
    "    results_single.append(result)\n",
    "single_time = time.time() - start_time\n",
    "print(f\"Time taken: {single_time:.3f} seconds\")\n",
    "print(f\"Average per text: {single_time/8*1000:.1f} ms\")\n",
    "\n",
    "# Method 2: Batch processing (fast)\n",
    "print(\"\\nMethod 2: Batch processing...\")\n",
    "start_time = time.time()\n",
    "results_batch = clf(test_texts, \n",
    "                   padding=True,\n",
    "                   truncation=True,\n",
    "                   max_length=128)\n",
    "batch_time = time.time() - start_time\n",
    "print(f\"Time taken: {batch_time:.3f} seconds\")\n",
    "print(f\"Average per text: {batch_time/len(test_texts)*1000:.1f} ms\")\n",
    "\n",
    "# Calculate speedup\n",
    "speedup = (single_time/8*len(test_texts)) / batch_time\n",
    "print(f\"\\nSpeedup: {speedup:.1f}x faster with batching!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a modern text generation model (2025 update)\n",
    "print(\"Loading text generation model...\")\n",
    "gen = pipeline(\n",
    "    'text-generation',\n",
    "    model='microsoft/phi-2',  # Modern efficient model\n",
    "    device=0 if DEVICE.type == \"cuda\" else -1\n",
    ")\n",
    "\n",
    "# Generate product reviews\n",
    "prompts = [\n",
    "    \"This smartphone is\",\n",
    "    \"The laptop performance is\",\n",
    "    \"Customer service was\"\n",
    "]\n",
    "\n",
    "print(\"Generating synthetic reviews...\\n\")\n",
    "for prompt in prompts:\n",
    "    generated = gen(\n",
    "        prompt,\n",
    "        max_new_tokens=30,\n",
    "        num_return_sequences=2,\n",
    "        temperature=0.8,\n",
    "        pad_token_id=gen.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    for i, g in enumerate(generated):\n",
    "        print(f\"  Generated {i+1}: {g['generated_text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading standard model...\n",
      "Standard model size: 255.4 MB\n",
      "Total parameters: 66,955,010\n",
      "\n",
      "Quantization options:\n",
      "- INT8: ~75% size reduction, minimal accuracy loss\n",
      "- INT4: ~87.5% size reduction, may require fine-tuning\n",
      "- Dynamic quantization: Adapts to input ranges\n"
     ]
    }
   ],
   "source": [
    "# Load a small model for demonstration\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "print(\"Loading standard model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Calculate model size\n",
    "param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "model_size = param_size + buffer_size\n",
    "print(f\"Standard model size: {model_size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Note: Actual quantization requires bitsandbytes library and compatible GPU\n",
    "# This is a conceptual demonstration\n",
    "print(\"\\nQuantization options:\")\n",
    "print(\"- INT8: ~75% size reduction, minimal accuracy loss\")\n",
    "print(\"- INT4: ~87.5% size reduction, may require fine-tuning\")\n",
    "print(\"- Dynamic quantization: Adapts to input ranges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Tracking Utility\n",
    "\n",
    "Implement the memory tracking context manager from the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing memory tracking...\n",
      "Memory tracking only available for CUDA devices\n"
     ]
    }
   ],
   "source": [
    "@contextmanager\n",
    "def track_memory(device: str = \"cuda\"):\n",
    "    \"\"\"Context manager for GPU memory profiling.\"\"\"\n",
    "    if device == \"cuda\" and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        start_memory = torch.cuda.memory_allocated()\n",
    "        yield\n",
    "        torch.cuda.synchronize()\n",
    "        end_memory = torch.cuda.memory_allocated()\n",
    "        memory_used = end_memory - start_memory\n",
    "        print(f\"Memory used: {memory_used / 1024 / 1024:.2f} MB\")\n",
    "    else:\n",
    "        yield\n",
    "        print(\"Memory tracking only available for CUDA devices\")\n",
    "\n",
    "# Example usage\n",
    "print(\"Testing memory tracking...\")\n",
    "with track_memory(device=DEVICE.type):\n",
    "    # Run some inference\n",
    "    _ = clf(\"Test text for memory tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class SimpleRetailWorkflow:\n",
    "    \"\"\"Simplified production workflow for retail review analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize pipelines with modern models\n",
    "        self.sentiment_pipeline = pipeline(\n",
    "            'sentiment-analysis',\n",
    "            model='cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "        )\n",
    "        \n",
    "        # Priority keywords for urgency detection\n",
    "        self.priority_keywords = {\n",
    "            \"urgent\": [\"broken\", \"damaged\", \"fraud\", \"stolen\", \"urgent\"],\n",
    "            \"high\": [\"terrible\", \"awful\", \"worst\", \"refund\", \"complaint\"],\n",
    "            \"medium\": [\"disappointed\", \"issue\", \"problem\", \"concern\"],\n",
    "            \"low\": [\"suggestion\", \"feedback\", \"minor\"]\n",
    "        }\n",
    "    \n",
    "    def analyze_priority(self, text: str) -> str:\n",
    "        \"\"\"Determine review priority based on keywords.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for priority, keywords in self.priority_keywords.items():\n",
    "            if any(keyword in text_lower for keyword in keywords):\n",
    "                return priority\n",
    "        \n",
    "        return \"normal\"\n",
    "    \n",
    "    def process_review(self, review: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a single review.\"\"\"\n",
    "        # Sentiment analysis\n",
    "        sentiment = self.sentiment_pipeline(review)[0]\n",
    "        \n",
    "        # Priority detection\n",
    "        priority = self.analyze_priority(review)\n",
    "        \n",
    "        # Adjust for model output format\n",
    "        label = sentiment.get(\"label\", \"LABEL_0\")\n",
    "        if label == \"LABEL_2\":\n",
    "            label = \"POSITIVE\"\n",
    "        elif label == \"LABEL_0\":\n",
    "            label = \"NEGATIVE\"\n",
    "        elif label == \"LABEL_1\":\n",
    "            label = \"NEUTRAL\"\n",
    "        \n",
    "        return {\n",
    "            \"text\": review,\n",
    "            \"sentiment\": label,\n",
    "            \"sentiment_score\": sentiment[\"score\"],\n",
    "            \"priority\": priority,\n",
    "            \"needs_attention\": priority in [\"urgent\", \"high\"]\n",
    "        }\n",
    "    \n",
    "    def process_batch(self, reviews: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Process multiple reviews and generate insights.\"\"\"\n",
    "        results = [self.process_review(review) for review in reviews]\n",
    "        \n",
    "        # Generate insights\n",
    "        total = len(results)\n",
    "        urgent_count = sum(1 for r in results if r[\"priority\"] in [\"urgent\", \"high\"])\n",
    "        positive_count = sum(1 for r in results if r[\"sentiment\"] == \"POSITIVE\")\n",
    "        \n",
    "        insights = {\n",
    "            \"total_reviews\": total,\n",
    "            \"urgent_reviews\": urgent_count,\n",
    "            \"sentiment_distribution\": {\n",
    "                \"positive\": positive_count,\n",
    "                \"negative\": total - positive_count,\n",
    "                \"positive_rate\": positive_count / total if total > 0 else 0\n",
    "            },\n",
    "            \"results\": results\n",
    "        }\n",
    "        \n",
    "        return insights\n",
    "\n",
    "# Test the workflow\n",
    "workflow = SimpleRetailWorkflow()\n",
    "\n",
    "sample_reviews = [\n",
    "    \"This product is absolutely amazing! Fast shipping and great quality.\",\n",
    "    \"Terrible experience. The item arrived broken and customer service was unhelpful.\",\n",
    "    \"Good value for money, but packaging could be better.\",\n",
    "    \"URGENT: Received wrong item. Need immediate refund!\",\n",
    "    \"The product works as described. Delivery was on time.\"\n",
    "]\n",
    "\n",
    "# Process reviews\n",
    "print(\"Processing reviews...\\n\")\n",
    "start_time = time.time()\n",
    "insights = workflow.process_batch(sample_reviews)\n",
    "process_time = time.time() - start_time\n",
    "\n",
    "# Display results\n",
    "print(\"=== WORKFLOW RESULTS ===\")\n",
    "print(f\"Total reviews processed: {insights['total_reviews']}\")\n",
    "print(f\"Processing time: {process_time:.3f} seconds\")\n",
    "print(f\"\\nUrgent reviews requiring attention: {insights['urgent_reviews']}\")\n",
    "print(f\"Positive sentiment rate: {insights['sentiment_distribution']['positive_rate']:.1%}\")\n",
    "\n",
    "print(\"\\n=== DETAILED RESULTS ===\")\n",
    "for i, result in enumerate(insights['results']):\n",
    "    if result['needs_attention']:\n",
    "        print(f\"\\n‚ö†Ô∏è  Review {i+1} (NEEDS ATTENTION):\")\n",
    "    else:\n",
    "        print(f\"\\nReview {i+1}:\")\n",
    "    print(f\"  Text: '{result['text'][:50]}...'\")\n",
    "    print(f\"  Sentiment: {result['sentiment']} ({result['sentiment_score']:.3f})\")\n",
    "    print(f\"  Priority: {result['priority'].upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Configuration:\n",
      "  Rank (r): 16\n",
      "  Alpha: 32\n",
      "  Dropout: 0.1\n",
      "  Target modules: {'value', 'query'}\n",
      "\n",
      "Parameter efficiency:\n",
      "  Original BERT-base: ~110M parameters\n",
      "  LoRA trainable: ~0.3M parameters\n",
      "  Reduction: 99.7% fewer trainable parameters!\n"
     ]
    }
   ],
   "source": [
    "# PEFT configuration example (conceptual)\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "# Define LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Sequence classification\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"]  # Target attention layers\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(f\"  Rank (r): {peft_config.r}\")\n",
    "print(f\"  Alpha: {peft_config.lora_alpha}\")\n",
    "print(f\"  Dropout: {peft_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {peft_config.target_modules}\")\n",
    "\n",
    "# Calculate approximate trainable parameters\n",
    "# For BERT-base with r=16:\n",
    "# Original: ~110M parameters\n",
    "# LoRA trainable: ~0.3M parameters (0.3% of original)\n",
    "print(\"\\nParameter efficiency:\")\n",
    "print(\"  Original BERT-base: ~110M parameters\")\n",
    "print(\"  LoRA trainable: ~0.3M parameters\")\n",
    "print(\"  Reduction: 99.7% fewer trainable parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class Config:\n",
    "    \"\"\"Centralized configuration with environment fallbacks (2025 best practices).\"\"\"\n",
    "    \n",
    "    # Device configuration with automatic detection\n",
    "    DEVICE = get_optimal_device()\n",
    "    \n",
    "    # Model configurations with env overrides - using modern models\n",
    "    DEFAULT_SENTIMENT_MODEL = os.getenv(\n",
    "        \"SENTIMENT_MODEL\", \n",
    "        \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    )\n",
    "    \n",
    "    DEFAULT_GENERATION_MODEL = os.getenv(\n",
    "        \"GENERATION_MODEL\",\n",
    "        \"microsoft/phi-2\"  # Efficient modern model\n",
    "    )\n",
    "    \n",
    "    DEFAULT_NER_MODEL = os.getenv(\n",
    "        \"NER_MODEL\",\n",
    "        \"dslim/bert-base-NER\"  # Efficient NER model\n",
    "    )\n",
    "    \n",
    "    # Performance settings\n",
    "    BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"32\"))\n",
    "    MAX_LENGTH = int(os.getenv(\"MAX_LENGTH\", \"512\"))\n",
    "    ENABLE_FLASH_ATTENTION = os.getenv(\"ENABLE_FLASH_ATTENTION\", \"true\").lower() == \"true\"\n",
    "    \n",
    "    # Quantization settings\n",
    "    ENABLE_INT8 = os.getenv(\"ENABLE_INT8\", \"false\").lower() == \"true\"\n",
    "    ENABLE_INT4 = os.getenv(\"ENABLE_INT4\", \"false\").lower() == \"true\"\n",
    "    \n",
    "    # Directory management with auto-creation\n",
    "    DATA_PATH = Path(os.getenv(\"DATA_PATH\", \"./data\"))\n",
    "    CACHE_DIR = Path(os.getenv(\"CACHE_DIR\", \"./cache\"))\n",
    "    \n",
    "    @classmethod\n",
    "    def display(cls):\n",
    "        \"\"\"Display current configuration.\"\"\"\n",
    "        print(\"Current Configuration (2025):\")\n",
    "        print(f\"  Device: {cls.DEVICE}\")\n",
    "        print(f\"  Sentiment Model: {cls.DEFAULT_SENTIMENT_MODEL}\")\n",
    "        print(f\"  Generation Model: {cls.DEFAULT_GENERATION_MODEL}\")\n",
    "        print(f\"  NER Model: {cls.DEFAULT_NER_MODEL}\")\n",
    "        print(f\"  Batch Size: {cls.BATCH_SIZE}\")\n",
    "        print(f\"  Max Length: {cls.MAX_LENGTH}\")\n",
    "        print(f\"  Flash Attention: {cls.ENABLE_FLASH_ATTENTION}\")\n",
    "        print(f\"  INT8 Quantization: {cls.ENABLE_INT8}\")\n",
    "        print(f\"  INT4 Quantization: {cls.ENABLE_INT4}\")\n",
    "        print(f\"  Data Path: {cls.DATA_PATH}\")\n",
    "        print(f\"  Cache Dir: {cls.CACHE_DIR}\")\n",
    "\n",
    "# Display configuration\n",
    "Config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text generation model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic reviews...\n",
      "\n",
      "Prompt: 'This smartphone is'\n",
      "  Generated 1: This smartphone is meant for smart-phones and tablets that are designed to be used by people from all walks of life. The company will also develop its own smart-\n",
      "  Generated 2: This smartphone is based on an ARM processor, running on an ARM Cortex-A9 processor.\n",
      "\n",
      "The Moto X is powered by the Qualcomm Snapdragon 810 chipset\n",
      "\n",
      "Prompt: 'The laptop performance is'\n",
      "  Generated 1: The laptop performance is slightly better on a regular laptop. While the new Razer Blade Stealth is slightly more advanced, the device can handle more demanding actions with less than average CPU\n",
      "  Generated 2: The laptop performance is slightly better compared to the other devices. It is not as hard as a Surface Pro 3, but it will still be a bit slower than the Surface\n",
      "\n",
      "Prompt: 'Customer service was'\n",
      "  Generated 1: Customer service was very good as they were very quick to answer questions. They were very helpful during my initial attempts to get a refund after I spent more than $200\n",
      "  Generated 2: Customer service was very good. Unfortunately, the customer service was not there to be a good one. I have no idea why it was not on my list.. I\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a small text generation model\n",
    "print(\"Loading text generation model...\")\n",
    "gen = pipeline(\n",
    "    'text-generation',\n",
    "    model='gpt2',  # Using smaller model for demo\n",
    "    device=0 if DEVICE.type == \"cuda\" else -1\n",
    ")\n",
    "\n",
    "# Generate product reviews\n",
    "prompts = [\n",
    "    \"This smartphone is\",\n",
    "    \"The laptop performance is\",\n",
    "    \"Customer service was\"\n",
    "]\n",
    "\n",
    "print(\"Generating synthetic reviews...\\n\")\n",
    "for prompt in prompts:\n",
    "    generated = gen(\n",
    "        prompt,\n",
    "        max_new_tokens=30,\n",
    "        num_return_sequences=2,\n",
    "        temperature=0.8,\n",
    "        pad_token_id=gen.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    for i, g in enumerate(generated):\n",
    "        print(f\"  Generated {i+1}: {g['generated_text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data Validation\n",
    "\n",
    "Implement quality checks for synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Summary and Key Takeaways\n\nIn this tutorial notebook, we've covered the 2025 best practices for Hugging Face workflows:\n\n1. **Pipeline Customization**: From basic usage to custom preprocessing and component composition\n   - Using modern models like `cardiffnlp/twitter-roberta-base-sentiment-latest`\n   - Custom pipeline classes with `_sanitize_parameters` method\n   - Debugging with transformers logging\n\n2. **Efficient Data Handling**: Using ü§ó Datasets for scalable data processing\n   - Streaming large datasets to handle data larger than memory\n   - Parallel processing with `map` and `filter`\n   - Creating custom datasets from various sources\n\n3. **Modern Optimization Techniques**: \n   - **Batching**: 5-10x throughput improvement\n   - **BitsAndBytesConfig**: INT8 and INT4 quantization for 75-87.5% model size reduction\n   - **Flash Attention 2**: 2-4x speedup on compatible GPUs\n   - **Memory tracking**: Monitoring GPU usage\n\n4. **QLoRA (Quantized LoRA)**: \n   - Combine 4-bit quantization with LoRA for memory-efficient fine-tuning\n   - Enable large model training on consumer GPUs\n   - Maintain 95% performance at 25% memory usage\n\n5. **Synthetic Data Generation**: Creating and validating synthetic training data\n   - Text generation with temperature control\n   - Quality validation and filtering\n   - Addressing data scarcity\n\n6. **Production Workflows**: Building robust, scalable systems\n   - Priority detection and routing\n   - Configuration management with environment variables\n   - Performance benchmarking utilities\n\n7. **Ethical AI**: \n   - Bias detection in model predictions\n   - Fairness monitoring across demographic groups\n   - Mitigation strategies for production deployment\n\n### Key Performance Gains:\n- **Batching**: 5-10x throughput improvement\n- **INT8 Quantization**: 75% model size reduction\n- **INT4 Quantization**: 87.5% model size reduction  \n- **QLoRA**: Train 13B models on 24GB GPUs\n- **Flash Attention 2**: Handle 8k+ token sequences\n- **Streaming**: Process datasets larger than memory\n\n### Next Steps:\n1. Experiment with QLoRA on your own models\n2. Implement BitsAndBytesConfig quantization\n3. Build custom pipelines with bias detection\n4. Try Flash Attention 2 on long sequences\n5. Deploy optimized models to production\n\n**Remember**: In 2025, responsible AI isn't optional‚Äîit's essential. Always include bias detection and fairness checks in production pipelines!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Workflows <a id='production'></a>\n",
    "\n",
    "### Complete Production Pipeline Example\n",
    "\n",
    "Let's implement a simplified version of the RetailReviewWorkflow from the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing reviews...\n",
      "\n",
      "=== WORKFLOW RESULTS ===\n",
      "Total reviews processed: 5\n",
      "Processing time: 0.149 seconds\n",
      "\n",
      "Urgent reviews requiring attention: 2\n",
      "Positive sentiment rate: 40.0%\n",
      "\n",
      "=== DETAILED RESULTS ===\n",
      "\n",
      "Review 1:\n",
      "  Text: 'This product is absolutely amazing! Fast shipping ...'\n",
      "  Sentiment: POSITIVE (1.000)\n",
      "  Priority: NORMAL\n",
      "\n",
      "‚ö†Ô∏è  Review 2 (NEEDS ATTENTION):\n",
      "  Text: 'Terrible experience. The item arrived broken and c...'\n",
      "  Sentiment: NEGATIVE (1.000)\n",
      "  Priority: URGENT\n",
      "\n",
      "Review 3:\n",
      "  Text: 'Good value for money, but packaging could be bette...'\n",
      "  Sentiment: NEGATIVE (0.992)\n",
      "  Priority: NORMAL\n",
      "\n",
      "‚ö†Ô∏è  Review 4 (NEEDS ATTENTION):\n",
      "  Text: 'URGENT: Received wrong item. Need immediate refund...'\n",
      "  Sentiment: NEGATIVE (0.999)\n",
      "  Priority: URGENT\n",
      "\n",
      "Review 5:\n",
      "  Text: 'The product works as described. Delivery was on ti...'\n",
      "  Sentiment: POSITIVE (0.992)\n",
      "  Priority: NORMAL\n"
     ]
    }
   ],
   "source": [
    "class SimpleRetailWorkflow:\n",
    "    \"\"\"Simplified production workflow for retail review analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize pipelines\n",
    "        self.sentiment_pipeline = pipeline(\n",
    "            'sentiment-analysis',\n",
    "            model='distilbert-base-uncased-finetuned-sst-2-english'\n",
    "        )\n",
    "        \n",
    "        # Priority keywords for urgency detection\n",
    "        self.priority_keywords = {\n",
    "            \"urgent\": [\"broken\", \"damaged\", \"fraud\", \"stolen\", \"urgent\"],\n",
    "            \"high\": [\"terrible\", \"awful\", \"worst\", \"refund\", \"complaint\"],\n",
    "            \"medium\": [\"disappointed\", \"issue\", \"problem\", \"concern\"],\n",
    "            \"low\": [\"suggestion\", \"feedback\", \"minor\"]\n",
    "        }\n",
    "    \n",
    "    def analyze_priority(self, text: str) -> str:\n",
    "        \"\"\"Determine review priority based on keywords.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for priority, keywords in self.priority_keywords.items():\n",
    "            if any(keyword in text_lower for keyword in keywords):\n",
    "                return priority\n",
    "        \n",
    "        return \"normal\"\n",
    "    \n",
    "    def process_review(self, review: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a single review.\"\"\"\n",
    "        # Sentiment analysis\n",
    "        sentiment = self.sentiment_pipeline(review)[0]\n",
    "        \n",
    "        # Priority detection\n",
    "        priority = self.analyze_priority(review)\n",
    "        \n",
    "        return {\n",
    "            \"text\": review,\n",
    "            \"sentiment\": sentiment[\"label\"],\n",
    "            \"sentiment_score\": sentiment[\"score\"],\n",
    "            \"priority\": priority,\n",
    "            \"needs_attention\": priority in [\"urgent\", \"high\"]\n",
    "        }\n",
    "    \n",
    "    def process_batch(self, reviews: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Process multiple reviews and generate insights.\"\"\"\n",
    "        results = [self.process_review(review) for review in reviews]\n",
    "        \n",
    "        # Generate insights\n",
    "        total = len(results)\n",
    "        urgent_count = sum(1 for r in results if r[\"priority\"] in [\"urgent\", \"high\"])\n",
    "        positive_count = sum(1 for r in results if r[\"sentiment\"] == \"POSITIVE\")\n",
    "        \n",
    "        insights = {\n",
    "            \"total_reviews\": total,\n",
    "            \"urgent_reviews\": urgent_count,\n",
    "            \"sentiment_distribution\": {\n",
    "                \"positive\": positive_count,\n",
    "                \"negative\": total - positive_count,\n",
    "                \"positive_rate\": positive_count / total if total > 0 else 0\n",
    "            },\n",
    "            \"results\": results\n",
    "        }\n",
    "        \n",
    "        return insights\n",
    "\n",
    "# Test the workflow\n",
    "workflow = SimpleRetailWorkflow()\n",
    "\n",
    "sample_reviews = [\n",
    "    \"This product is absolutely amazing! Fast shipping and great quality.\",\n",
    "    \"Terrible experience. The item arrived broken and customer service was unhelpful.\",\n",
    "    \"Good value for money, but packaging could be better.\",\n",
    "    \"URGENT: Received wrong item. Need immediate refund!\",\n",
    "    \"The product works as described. Delivery was on time.\"\n",
    "]\n",
    "\n",
    "# Process reviews\n",
    "print(\"Processing reviews...\\n\")\n",
    "start_time = time.time()\n",
    "insights = workflow.process_batch(sample_reviews)\n",
    "process_time = time.time() - start_time\n",
    "\n",
    "# Display results\n",
    "print(\"=== WORKFLOW RESULTS ===\")\n",
    "print(f\"Total reviews processed: {insights['total_reviews']}\")\n",
    "print(f\"Processing time: {process_time:.3f} seconds\")\n",
    "print(f\"\\nUrgent reviews requiring attention: {insights['urgent_reviews']}\")\n",
    "print(f\"Positive sentiment rate: {insights['sentiment_distribution']['positive_rate']:.1%}\")\n",
    "\n",
    "print(\"\\n=== DETAILED RESULTS ===\")\n",
    "for i, result in enumerate(insights['results']):\n",
    "    if result['needs_attention']:\n",
    "        print(f\"\\n‚ö†Ô∏è  Review {i+1} (NEEDS ATTENTION):\")\n",
    "    else:\n",
    "        print(f\"\\nReview {i+1}:\")\n",
    "    print(f\"  Text: '{result['text'][:50]}...'\")\n",
    "    print(f\"  Sentiment: {result['sentiment']} ({result['sentiment_score']:.3f})\")\n",
    "    print(f\"  Priority: {result['priority'].upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Management\n",
    "\n",
    "Implement a configuration system with environment variable support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Configuration:\n",
      "  Device: mps\n",
      "  Default Model: distilbert-base-uncased-finetuned-sst-2-english\n",
      "  Batch Size: 32\n",
      "  Max Length: 512\n",
      "  Flash Attention: True\n",
      "  Data Path: data\n",
      "  Cache Dir: cache\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    \"\"\"Centralized configuration with environment fallbacks.\"\"\"\n",
    "    \n",
    "    # Device configuration with automatic detection\n",
    "    DEVICE = get_optimal_device()\n",
    "    \n",
    "    # Model configurations with env overrides\n",
    "    DEFAULT_SENTIMENT_MODEL = os.getenv(\n",
    "        \"SENTIMENT_MODEL\", \n",
    "        \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    )\n",
    "    \n",
    "    # Performance settings\n",
    "    BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"32\"))\n",
    "    MAX_LENGTH = int(os.getenv(\"MAX_LENGTH\", \"512\"))\n",
    "    ENABLE_FLASH_ATTENTION = os.getenv(\"ENABLE_FLASH_ATTENTION\", \"true\").lower() == \"true\"\n",
    "    \n",
    "    # Directory management with auto-creation\n",
    "    DATA_PATH = Path(os.getenv(\"DATA_PATH\", \"./data\"))\n",
    "    CACHE_DIR = Path(os.getenv(\"CACHE_DIR\", \"./cache\"))\n",
    "    \n",
    "    @classmethod\n",
    "    def display(cls):\n",
    "        \"\"\"Display current configuration.\"\"\"\n",
    "        print(\"Current Configuration:\")\n",
    "        print(f\"  Device: {cls.DEVICE}\")\n",
    "        print(f\"  Default Model: {cls.DEFAULT_SENTIMENT_MODEL}\")\n",
    "        print(f\"  Batch Size: {cls.BATCH_SIZE}\")\n",
    "        print(f\"  Max Length: {cls.MAX_LENGTH}\")\n",
    "        print(f\"  Flash Attention: {cls.ENABLE_FLASH_ATTENTION}\")\n",
    "        print(f\"  Data Path: {cls.DATA_PATH}\")\n",
    "        print(f\"  Cache Dir: {cls.CACHE_DIR}\")\n",
    "\n",
    "# Display configuration\n",
    "Config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Benchmarking\n",
    "\n",
    "Create a simple benchmarking utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modern Quantization with BitsAndBytes\n",
    "\n",
    "Demonstrating INT8 and INT4 quantization for memory reduction (2025 best practices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking Sentiment Analysis...\n",
      "  Single inference: 37.4 ms/sample\n",
      "  Batch inference: 36.8 ms/sample\n",
      "  Throughput (single): 26.7 samples/sec\n",
      "  Throughput (batch): 27.2 samples/sec\n",
      "  Batch speedup: 1.0x\n"
     ]
    }
   ],
   "source": [
    "def benchmark_pipeline(pipeline_func, inputs: List[str], name: str = \"Pipeline\") -> Dict[str, float]:\n",
    "    \"\"\"Benchmark a pipeline with various metrics.\"\"\"\n",
    "    print(f\"\\nBenchmarking {name}...\")\n",
    "    \n",
    "    # Warmup\n",
    "    _ = pipeline_func(inputs[0])\n",
    "    \n",
    "    # Single inference\n",
    "    start = time.time()\n",
    "    for inp in inputs[:10]:\n",
    "        _ = pipeline_func(inp)\n",
    "    single_time = time.time() - start\n",
    "    \n",
    "    # Batch inference\n",
    "    start = time.time()\n",
    "    _ = pipeline_func(inputs)\n",
    "    batch_time = time.time() - start\n",
    "    \n",
    "    metrics = {\n",
    "        \"single_latency_ms\": (single_time / 10) * 1000,\n",
    "        \"batch_latency_ms\": (batch_time / len(inputs)) * 1000,\n",
    "        \"throughput_single\": 10 / single_time,\n",
    "        \"throughput_batch\": len(inputs) / batch_time,\n",
    "        \"speedup\": (single_time / 10 * len(inputs)) / batch_time\n",
    "    }\n",
    "    \n",
    "    print(f\"  Single inference: {metrics['single_latency_ms']:.1f} ms/sample\")\n",
    "    print(f\"  Batch inference: {metrics['batch_latency_ms']:.1f} ms/sample\")\n",
    "    print(f\"  Throughput (single): {metrics['throughput_single']:.1f} samples/sec\")\n",
    "    print(f\"  Throughput (batch): {metrics['throughput_batch']:.1f} samples/sec\")\n",
    "    print(f\"  Batch speedup: {metrics['speedup']:.1f}x\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Benchmark our sentiment pipeline\n",
    "test_inputs = [f\"Test review number {i}. This is a sample text.\" for i in range(50)]\n",
    "metrics = benchmark_pipeline(clf, test_inputs, \"Sentiment Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "In this tutorial notebook, we've covered:\n",
    "\n",
    "1. **Pipeline Customization**: From basic usage to custom preprocessing and component composition\n",
    "2. **Efficient Data Handling**: Using ü§ó Datasets for scalable data processing\n",
    "3. **Optimization Techniques**: Batching, quantization, and memory tracking\n",
    "4. **Synthetic Data Generation**: Creating and validating synthetic training data\n",
    "5. **Production Workflows**: Building robust, scalable systems for real-world deployment\n",
    "\n",
    "### Key Performance Gains:\n",
    "- **Batching**: 5-10x throughput improvement\n",
    "- **Quantization**: 75% model size reduction\n",
    "- **Streaming**: Handle datasets larger than memory\n",
    "- **PEFT/LoRA**: Train with 0.1% of parameters\n",
    "\n",
    "### Next Steps:\n",
    "1. Experiment with different models and batch sizes\n",
    "2. Implement quantization on compatible hardware\n",
    "3. Build custom pipelines for your specific use case\n",
    "4. Explore synthetic data generation for your domain\n",
    "\n",
    "**Remember**: Great AI isn't about using the fanciest models. It's about building robust, efficient workflows that solve real problems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}