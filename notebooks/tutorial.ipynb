{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing Pipelines and Data Workflows: Advanced Models and Efficient Processing\n",
    "\n",
    "This notebook contains all examples from Chapter 8 with step-by-step explanations.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Environment Setup](#environment-setup)\n",
    "2. [Pipeline Basics and Customization](#pipeline-basics)\n",
    "3. [Efficient Data Handling](#data-handling)\n",
    "4. [Optimization Techniques](#optimization)\n",
    "5. [Synthetic Data Generation](#synthetic-data)\n",
    "6. [Production Workflows](#production)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup <a id='environment-setup'></a>\n",
    "\n",
    "First, let's set up our environment with all necessary imports and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'register_pipeline' from 'transformers.pipelines' (/Users/richardhightower/src/art_hug_08/.venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Transformers imports\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     pipeline,\n\u001b[32m     17\u001b[39m     Pipeline,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     AutoModelForCausalLM\n\u001b[32m     22\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipelines\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_pipeline\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Datasets\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'register_pipeline' from 'transformers.pipelines' (/Users/richardhightower/src/art_hug_08/.venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Add src to path for local imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    Pipeline,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "from transformers.pipelines import register_pipeline\n",
    "from transformers.utils import logging\n",
    "\n",
    "# Datasets\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# For optimization examples\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# For synthetic data generation\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "print(\"Environment ready!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Configuration\n",
    "\n",
    "Let's implement the cross-platform device detection from the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "def get_optimal_device() -> torch.device:\n",
    "    \"\"\"Automatically detect best available device.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():  # Apple Silicon\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_optimal_device()\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Basics and Customization <a id='pipeline-basics'></a>\n",
    "\n",
    "### Quick Start: Modern Pipeline Usage\n",
    "\n",
    "Let's start with the basic pipeline example from the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998641014099121}]\n",
      "Text: 'I love this product!'\n",
      "  Sentiment: POSITIVE (confidence: 1.000)\n",
      "\n",
      "Text: 'This is terrible.'\n",
      "  Sentiment: NEGATIVE (confidence: 1.000)\n",
      "\n",
      "Text: 'Not bad, but could be better.'\n",
      "  Sentiment: NEGATIVE (confidence: 0.802)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Modern quick-start with explicit model and device\n",
    "clf = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    device=0 if DEVICE.type == \"cuda\" else -1  # 0 for CUDA GPU, -1 for CPU\n",
    ")\n",
    "\n",
    "# Run prediction on text\n",
    "result = clf('I love Hugging Face!')\n",
    "print(result)\n",
    "# Expected output: [{'label': 'POSITIVE', 'score': 0.9998}]\n",
    "\n",
    "# Let's try multiple examples\n",
    "texts = [\n",
    "    \"I love this product!\",\n",
    "    \"This is terrible.\",\n",
    "    \"Not bad, but could be better.\"\n",
    "]\n",
    "results = clf(texts)\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"  Sentiment: {result['label']} (confidence: {result['score']:.3f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Preprocessing\n",
    "\n",
    "Now let's add custom preprocessing to normalize text before inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original texts: ['Wow! Amazing product!!!', \"I don't like this...\"]\n",
      "Cleaned texts: ['wow amazing product', 'i dont like this']\n",
      "\n",
      "Results after preprocessing:\n",
      "Original: 'Wow! Amazing product!!!'\n",
      "Cleaned: 'wow amazing product'\n",
      "Result: {'label': 'POSITIVE', 'score': 0.9998600482940674}\n",
      "\n",
      "Original: 'I don't like this...'\n",
      "Cleaned: 'i dont like this'\n",
      "Result: {'label': 'NEGATIVE', 'score': 0.8758226037025452}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def custom_preprocess(text):\n",
    "    \"\"\"Normalize text for consistent predictions.\"\"\"\n",
    "    import string\n",
    "    text = text.lower()\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Test preprocessing\n",
    "texts = [\"Wow! Amazing product!!!\", \"I don't like this...\"]\n",
    "print(\"Original texts:\", texts)\n",
    "\n",
    "# Clean then predict\n",
    "cleaned = [custom_preprocess(t) for t in texts]\n",
    "print(\"Cleaned texts:\", cleaned)\n",
    "\n",
    "# Batch processing for speed\n",
    "results = clf(cleaned, batch_size=16)\n",
    "print(\"\\nResults after preprocessing:\")\n",
    "for original, clean, result in zip(texts, cleaned, results):\n",
    "    print(f\"Original: '{original}'\")\n",
    "    print(f\"Cleaned: '{clean}'\")\n",
    "    print(f\"Result: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced: Pipeline Subclassing\n",
    "\n",
    "Create a reusable pipeline with built-in preprocessing and postprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom pipeline concept demonstrated!\n"
     ]
    }
   ],
   "source": [
    "class CustomSentimentPipeline(Pipeline):\n",
    "    def preprocess(self, inputs):\n",
    "        \"\"\"Strip HTML, normalize text.\"\"\"\n",
    "        if isinstance(inputs, str):\n",
    "            text = inputs.lower()\n",
    "            import string\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "            return super().preprocess(text)\n",
    "        return super().preprocess(inputs)\n",
    "    \n",
    "    def postprocess(self, outputs):\n",
    "        \"\"\"Add confidence thresholds.\"\"\"\n",
    "        results = super().postprocess(outputs)\n",
    "        for r in results:\n",
    "            r['confident'] = r['score'] > 0.95\n",
    "        return results\n",
    "\n",
    "# Note: In practice, you would register and use this custom pipeline\n",
    "# For now, let's demonstrate the concept with the standard pipeline\n",
    "print(\"Custom pipeline concept demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Pipeline Components\n",
    "\n",
    "Let's peek under the hood to understand pipeline anatomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DistilBertForSequenceClassification\n",
      "Tokenizer: DistilBertTokenizerFast\n",
      "Processor: None\n",
      "Framework: pt\n",
      "Device: cpu\n",
      "\n",
      "Model architecture: distilbert\n",
      "Hidden size: 768\n",
      "Number of labels: 2\n",
      "\n",
      "Tokenizer vocab size: 30522\n",
      "Max length: 512\n"
     ]
    }
   ],
   "source": [
    "# Inspect pipeline components\n",
    "print('Model:', type(clf.model).__name__)\n",
    "print('Tokenizer:', type(clf.tokenizer).__name__)  \n",
    "print('Processor:', getattr(clf, 'processor', None))\n",
    "print('Framework:', clf.framework)\n",
    "print('Device:', clf.device)\n",
    "\n",
    "# Let's look at model details\n",
    "print(f\"\\nModel architecture: {clf.model.config.model_type}\")\n",
    "print(f\"Hidden size: {clf.model.config.hidden_size}\")\n",
    "print(f\"Number of labels: {clf.model.config.num_labels}\")\n",
    "\n",
    "# Tokenizer details\n",
    "print(f\"\\nTokenizer vocab size: {clf.tokenizer.vocab_size}\")\n",
    "print(f\"Max length: {clf.tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composing Multiple Pipelines\n",
    "\n",
    "Let's create a combined sentiment + NER pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n",
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Apple Inc. makes amazing products! I love my iPhone.\n",
      "\n",
      "Sentiment: POSITIVE (1.000)\n",
      "\n",
      "Entities found:\n",
      "  - Apple: I-ORG\n",
      "  - Inc: I-ORG\n",
      "  - iPhone: I-MISC\n"
     ]
    }
   ],
   "source": [
    "# Load individual pipelines\n",
    "sentiment_pipe = pipeline('sentiment-analysis')\n",
    "ner_pipe = pipeline('ner')\n",
    "\n",
    "def combined_analysis(text):\n",
    "    \"\"\"Combine sentiment and NER analysis.\"\"\"\n",
    "    sentiment = sentiment_pipe(text)\n",
    "    entities = ner_pipe(text)\n",
    "    \n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"sentiment\": sentiment[0],\n",
    "        \"entities\": entities\n",
    "    }\n",
    "\n",
    "# Test combined analysis\n",
    "test_text = \"Apple Inc. makes amazing products! I love my iPhone.\"\n",
    "result = combined_analysis(test_text)\n",
    "\n",
    "print(f\"Text: {result['text']}\")\n",
    "print(f\"\\nSentiment: {result['sentiment']['label']} ({result['sentiment']['score']:.3f})\")\n",
    "print(\"\\nEntities found:\")\n",
    "for entity in result['entities']:\n",
    "    print(f\"  - {entity['word']}: {entity['entity_group'] if 'entity_group' in entity else entity['entity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging Pipelines\n",
    "\n",
    "Enable verbose logging to debug pipeline issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logging' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Enable debug logging\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mlogging\u001b[49m.set_verbosity_debug()\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Now pipeline operations will show detailed logs\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDebug mode enabled. Running pipeline...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'logging' is not defined"
     ]
    }
   ],
   "source": [
    "# Enable debug logging\n",
    "logging.set_verbosity_debug()\n",
    "\n",
    "# Now pipeline operations will show detailed logs\n",
    "print(\"Debug mode enabled. Running pipeline...\")\n",
    "result = clf(\"Debug me!\")\n",
    "print(f\"\\nResult: {result}\")\n",
    "\n",
    "# Reset logging to normal\n",
    "logging.set_verbosity_warning()\n",
    "print(\"\\nLogging reset to normal level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Data Handling with ðŸ¤— Datasets <a id='data-handling'></a>\n",
    "\n",
    "### Loading and Transforming Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load a small dataset for demonstration\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mimdb\u001b[39m\u001b[33m'\u001b[39m, split=\u001b[33m'\u001b[39m\u001b[33mtrain[:1000]\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Load only first 1000 examples\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFirst example: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Load a small dataset for demonstration\n",
    "dataset = load_dataset('imdb', split='train[:1000]')  # Load only first 1000 examples\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"First example: {dataset[0]}\")\n",
    "print(f\"\\nFeatures: {dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming dataset...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTransforming dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m dataset = \u001b[43mdataset\u001b[49m.map(preprocess_batch, batched=\u001b[38;5;28;01mTrue\u001b[39;00m, num_proc=\u001b[32m4\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTransformation completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Filter short reviews\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Define preprocessing function\n",
    "def preprocess_batch(batch):\n",
    "    \"\"\"Process entire batches at once.\"\"\"\n",
    "    batch['text'] = [text.lower() for text in batch['text']]\n",
    "    batch['length'] = [len(text.split()) for text in batch['text']]\n",
    "    return batch\n",
    "\n",
    "# Transform with parallel processing\n",
    "print(\"Transforming dataset...\")\n",
    "start_time = time.time()\n",
    "dataset = dataset.map(preprocess_batch, batched=True, num_proc=4)\n",
    "print(f\"Transformation completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Filter short reviews\n",
    "print(f\"\\nDataset before filtering: {len(dataset)} examples\")\n",
    "dataset = dataset.filter(lambda x: x['length'] > 20)\n",
    "print(f\"Dataset after filtering: {len(dataset)} examples\")\n",
    "\n",
    "# Check the new features\n",
    "print(f\"\\nUpdated features: {dataset.features}\")\n",
    "print(f\"Example with new features: {dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Large Datasets\n",
    "\n",
    "For massive datasets, use streaming to avoid memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m     writer.writerows(sample_data)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Stream the dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m streaming_dataset = \u001b[43mload_dataset\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mcsv\u001b[39m\u001b[33m'\u001b[39m, data_files=csv_path, split=\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m, streaming=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Process in batches\u001b[39;00m\n\u001b[32m     22\u001b[39m batch_size = \u001b[32m32\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a sample CSV for demonstration\n",
    "import csv\n",
    "\n",
    "sample_data = [\n",
    "    {\"text\": \"This product is amazing!\", \"label\": \"positive\"},\n",
    "    {\"text\": \"Terrible experience.\", \"label\": \"negative\"},\n",
    "    {\"text\": \"Good value for money.\", \"label\": \"positive\"},\n",
    "    {\"text\": \"Not worth the price.\", \"label\": \"negative\"},\n",
    "    {\"text\": \"Excellent quality!\", \"label\": \"positive\"}\n",
    "] * 20  # Repeat for larger dataset\n",
    "\n",
    "csv_path = \"sample_reviews.csv\"\n",
    "with open(csv_path, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['text', 'label'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(sample_data)\n",
    "\n",
    "# Stream the dataset\n",
    "streaming_dataset = load_dataset('csv', data_files=csv_path, split='train', streaming=True)\n",
    "\n",
    "# Process in batches\n",
    "batch_size = 32\n",
    "batch = []\n",
    "processed_count = 0\n",
    "\n",
    "print(\"Processing streaming dataset...\")\n",
    "for example in streaming_dataset:\n",
    "    batch.append(custom_preprocess(example['text']))\n",
    "    \n",
    "    if len(batch) == batch_size:\n",
    "        # Process batch\n",
    "        results = clf(batch, batch_size=batch_size)\n",
    "        processed_count += len(batch)\n",
    "        print(f\"Processed {processed_count} examples...\")\n",
    "        batch = []\n",
    "    \n",
    "    # Stop after processing 100 examples for demo\n",
    "    if processed_count >= 96:\n",
    "        break\n",
    "\n",
    "# Process remaining batch\n",
    "if batch:\n",
    "    results = clf(batch)\n",
    "    processed_count += len(batch)\n",
    "\n",
    "print(f\"\\nTotal processed: {processed_count} examples\")\n",
    "\n",
    "# Clean up\n",
    "os.remove(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Custom Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create a custom dataset from dictionaries\u001b[39;00m\n\u001b[32m      2\u001b[39m custom_data = {\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m      4\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe future of AI is bright and full of possibilities.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mfuture\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mml\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdl\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mnlp\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcv\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     11\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m custom_dataset = \u001b[43mDataset\u001b[49m.from_dict(custom_data)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCustom dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_dataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFirst example: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcustom_dataset[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a custom dataset from dictionaries\n",
    "custom_data = {\n",
    "    \"text\": [\n",
    "        \"The future of AI is bright and full of possibilities.\",\n",
    "        \"Machine learning transforms how we solve complex problems.\",\n",
    "        \"Deep learning models continue to improve rapidly.\",\n",
    "        \"Natural language processing enables better human-computer interaction.\",\n",
    "        \"Computer vision applications are becoming more sophisticated.\"\n",
    "    ],\n",
    "    \"category\": [\"future\", \"ml\", \"dl\", \"nlp\", \"cv\"]\n",
    "}\n",
    "\n",
    "custom_dataset = Dataset.from_dict(custom_data)\n",
    "print(f\"Custom dataset: {custom_dataset}\")\n",
    "print(f\"\\nFirst example: {custom_dataset[0]}\")\n",
    "\n",
    "# Apply transformations\n",
    "def add_metadata(example):\n",
    "    example['word_count'] = len(example['text'].split())\n",
    "    example['char_count'] = len(example['text'])\n",
    "    return example\n",
    "\n",
    "custom_dataset = custom_dataset.map(add_metadata)\n",
    "print(f\"\\nAfter transformation: {custom_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Techniques <a id='optimization'></a>\n",
    "\n",
    "### Batching for 10x Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: Processing one by one...\n",
      "Time taken: 0.176 seconds\n",
      "Average per text: 22.0 ms\n",
      "\n",
      "Method 2: Batch processing...\n",
      "Time taken: 0.607 seconds\n",
      "Average per text: 19.0 ms\n",
      "\n",
      "Speedup: 1.2x faster with batching!\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data\n",
    "test_texts = [\n",
    "    \"Review 1: This product exceeded my expectations.\",\n",
    "    \"Review 2: Not satisfied with the quality.\",\n",
    "    \"Review 3: Average product, nothing special.\",\n",
    "    \"Review 4: Absolutely love it!\",\n",
    "    \"Review 5: Waste of money.\",\n",
    "    \"Review 6: Good value for the price.\",\n",
    "    \"Review 7: Would recommend to friends.\",\n",
    "    \"Review 8: Poor customer service.\"\n",
    "] * 4  # Repeat for more examples\n",
    "\n",
    "# Method 1: One by one (slow)\n",
    "print(\"Method 1: Processing one by one...\")\n",
    "start_time = time.time()\n",
    "results_single = []\n",
    "for text in test_texts[:8]:  # Process only first 8 for demo\n",
    "    result = clf(text)\n",
    "    results_single.append(result)\n",
    "single_time = time.time() - start_time\n",
    "print(f\"Time taken: {single_time:.3f} seconds\")\n",
    "print(f\"Average per text: {single_time/8*1000:.1f} ms\")\n",
    "\n",
    "# Method 2: Batch processing (fast)\n",
    "print(\"\\nMethod 2: Batch processing...\")\n",
    "start_time = time.time()\n",
    "results_batch = clf(test_texts, \n",
    "                   padding=True,\n",
    "                   truncation=True,\n",
    "                   max_length=128)\n",
    "batch_time = time.time() - start_time\n",
    "print(f\"Time taken: {batch_time:.3f} seconds\")\n",
    "print(f\"Average per text: {batch_time/len(test_texts)*1000:.1f} ms\")\n",
    "\n",
    "# Calculate speedup\n",
    "speedup = (single_time/8*len(test_texts)) / batch_time\n",
    "print(f\"\\nSpeedup: {speedup:.1f}x faster with batching!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modern Quantization\n",
    "\n",
    "Demonstrate quantization for cost reduction (requires appropriate hardware)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading standard model...\n",
      "Standard model size: 255.4 MB\n",
      "Total parameters: 66,955,010\n",
      "\n",
      "Quantization options:\n",
      "- INT8: ~75% size reduction, minimal accuracy loss\n",
      "- INT4: ~87.5% size reduction, may require fine-tuning\n",
      "- Dynamic quantization: Adapts to input ranges\n"
     ]
    }
   ],
   "source": [
    "# Load a small model for demonstration\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "print(\"Loading standard model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Calculate model size\n",
    "param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "model_size = param_size + buffer_size\n",
    "print(f\"Standard model size: {model_size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Note: Actual quantization requires bitsandbytes library and compatible GPU\n",
    "# This is a conceptual demonstration\n",
    "print(\"\\nQuantization options:\")\n",
    "print(\"- INT8: ~75% size reduction, minimal accuracy loss\")\n",
    "print(\"- INT4: ~87.5% size reduction, may require fine-tuning\")\n",
    "print(\"- Dynamic quantization: Adapts to input ranges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Tracking Utility\n",
    "\n",
    "Implement the memory tracking context manager from the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing memory tracking...\n",
      "Memory tracking only available for CUDA devices\n"
     ]
    }
   ],
   "source": [
    "@contextmanager\n",
    "def track_memory(device: str = \"cuda\"):\n",
    "    \"\"\"Context manager for GPU memory profiling.\"\"\"\n",
    "    if device == \"cuda\" and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        start_memory = torch.cuda.memory_allocated()\n",
    "        yield\n",
    "        torch.cuda.synchronize()\n",
    "        end_memory = torch.cuda.memory_allocated()\n",
    "        memory_used = end_memory - start_memory\n",
    "        print(f\"Memory used: {memory_used / 1024 / 1024:.2f} MB\")\n",
    "    else:\n",
    "        yield\n",
    "        print(\"Memory tracking only available for CUDA devices\")\n",
    "\n",
    "# Example usage\n",
    "print(\"Testing memory tracking...\")\n",
    "with track_memory(device=DEVICE.type):\n",
    "    # Run some inference\n",
    "    _ = clf(\"Test text for memory tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT/LoRA Concept\n",
    "\n",
    "Demonstrate Parameter-Efficient Fine-Tuning concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Configuration:\n",
      "  Rank (r): 16\n",
      "  Alpha: 32\n",
      "  Dropout: 0.1\n",
      "  Target modules: {'query', 'value'}\n",
      "\n",
      "Parameter efficiency:\n",
      "  Original BERT-base: ~110M parameters\n",
      "  LoRA trainable: ~0.3M parameters\n",
      "  Reduction: 99.7% fewer trainable parameters!\n"
     ]
    }
   ],
   "source": [
    "# PEFT configuration example (conceptual)\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "# Define LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Sequence classification\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"]  # Target attention layers\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(f\"  Rank (r): {peft_config.r}\")\n",
    "print(f\"  Alpha: {peft_config.lora_alpha}\")\n",
    "print(f\"  Dropout: {peft_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {peft_config.target_modules}\")\n",
    "\n",
    "# Calculate approximate trainable parameters\n",
    "# For BERT-base with r=16:\n",
    "# Original: ~110M parameters\n",
    "# LoRA trainable: ~0.3M parameters (0.3% of original)\n",
    "print(\"\\nParameter efficiency:\")\n",
    "print(\"  Original BERT-base: ~110M parameters\")\n",
    "print(\"  LoRA trainable: ~0.3M parameters\")\n",
    "print(\"  Reduction: 99.7% fewer trainable parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data Generation <a id='synthetic-data'></a>\n",
    "\n",
    "### Text Generation with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text generation model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating synthetic reviews...\n",
      "\n",
      "Prompt: 'This smartphone is'\n",
      "  Generated 1: This smartphone is still working, but it's going to be a challenge to figure out how to improve this new feature.\n",
      "\n",
      "Samsung's Galaxy Note 8 is the\n",
      "  Generated 2: This smartphone is an innovative system combining the unique features of the previous generation with a modern platform and a premium design,\" said J.P. Morgan analyst John F.\n",
      "\n",
      "Prompt: 'The laptop performance is'\n",
      "  Generated 1: The laptop performance is superb. I have an older version of the Macbook Air running OS X 10.5.5 (9.8.10).\n",
      "\n",
      "The\n",
      "  Generated 2: The laptop performance is very good and the only thing that I think about is the SSD capacity. For a laptop that is only 2TB, the performance is not as good\n",
      "\n",
      "Prompt: 'Customer service was'\n",
      "  Generated 1: Customer service was one of the most frustrating. I received my order from a local Walmart a day early. When I got home I was given my order at 8:\n",
      "  Generated 2: Customer service was not forthcoming.\n",
      "\n",
      "He also said that the company was unable to discuss the situation or what was happening on the ground.\n",
      "\n",
      "\"We need\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load a small text generation model\n",
    "print(\"Loading text generation model...\")\n",
    "gen = pipeline(\n",
    "    'text-generation',\n",
    "    model='gpt2',  # Using smaller model for demo\n",
    "    device=0 if DEVICE.type == \"cuda\" else -1\n",
    ")\n",
    "\n",
    "# Generate product reviews\n",
    "prompts = [\n",
    "    \"This smartphone is\",\n",
    "    \"The laptop performance is\",\n",
    "    \"Customer service was\"\n",
    "]\n",
    "\n",
    "print(\"Generating synthetic reviews...\\n\")\n",
    "for prompt in prompts:\n",
    "    generated = gen(\n",
    "        prompt,\n",
    "        max_new_tokens=30,\n",
    "        num_return_sequences=2,\n",
    "        temperature=0.8,\n",
    "        pad_token_id=gen.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    for i, g in enumerate(generated):\n",
    "        print(f\"  Generated {i+1}: {g['generated_text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data Validation\n",
    "\n",
    "Implement quality checks for synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Results:\n",
      "  Total samples: 5\n",
      "  Valid samples: 2\n",
      "  Validity rate: 40.0%\n",
      "  Issues found: {'truncated', 'repetitive', 'too_short'}\n"
     ]
    }
   ],
   "source": [
    "def validate_synthetic_text(texts: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Basic validation for synthetic text data.\"\"\"\n",
    "    results = {\n",
    "        \"total\": len(texts),\n",
    "        \"valid\": 0,\n",
    "        \"issues\": []\n",
    "    }\n",
    "    \n",
    "    for text in texts:\n",
    "        issues = []\n",
    "        \n",
    "        # Check length\n",
    "        if len(text.split()) < 5:\n",
    "            issues.append(\"too_short\")\n",
    "        elif len(text.split()) > 200:\n",
    "            issues.append(\"too_long\")\n",
    "        \n",
    "        # Check for repetition\n",
    "        words = text.lower().split()\n",
    "        if len(words) > 0 and len(set(words)) / len(words) < 0.5:\n",
    "            issues.append(\"repetitive\")\n",
    "        \n",
    "        # Check for truncation\n",
    "        if text.endswith(\"...\") or not text.endswith(('.', '!', '?')):\n",
    "            issues.append(\"truncated\")\n",
    "        \n",
    "        if not issues:\n",
    "            results[\"valid\"] += 1\n",
    "        else:\n",
    "            results[\"issues\"].extend(issues)\n",
    "    \n",
    "    results[\"validity_rate\"] = results[\"valid\"] / results[\"total\"]\n",
    "    return results\n",
    "\n",
    "# Test validation\n",
    "synthetic_samples = [\n",
    "    \"This product is amazing and works perfectly!\",\n",
    "    \"Good good good good good.\",\n",
    "    \"The laptop\",\n",
    "    \"Excellent quality and fast shipping. Would buy again.\",\n",
    "    \"This is a test that ends abruptly and\"\n",
    "]\n",
    "\n",
    "validation_results = validate_synthetic_text(synthetic_samples)\n",
    "print(\"Validation Results:\")\n",
    "print(f\"  Total samples: {validation_results['total']}\")\n",
    "print(f\"  Valid samples: {validation_results['valid']}\")\n",
    "print(f\"  Validity rate: {validation_results['validity_rate']:.1%}\")\n",
    "print(f\"  Issues found: {set(validation_results['issues'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Workflows <a id='production'></a>\n",
    "\n",
    "### Complete Production Pipeline Example\n",
    "\n",
    "Let's implement a simplified version of the RetailReviewWorkflow from the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRetailWorkflow:\n",
    "    \"\"\"Simplified production workflow for retail review analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize pipelines\n",
    "        self.sentiment_pipeline = pipeline(\n",
    "            'sentiment-analysis',\n",
    "            model='distilbert-base-uncased-finetuned-sst-2-english'\n",
    "        )\n",
    "        \n",
    "        # Priority keywords for urgency detection\n",
    "        self.priority_keywords = {\n",
    "            \"urgent\": [\"broken\", \"damaged\", \"fraud\", \"stolen\", \"urgent\"],\n",
    "            \"high\": [\"terrible\", \"awful\", \"worst\", \"refund\", \"complaint\"],\n",
    "            \"medium\": [\"disappointed\", \"issue\", \"problem\", \"concern\"],\n",
    "            \"low\": [\"suggestion\", \"feedback\", \"minor\"]\n",
    "        }\n",
    "    \n",
    "    def analyze_priority(self, text: str) -> str:\n",
    "        \"\"\"Determine review priority based on keywords.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for priority, keywords in self.priority_keywords.items():\n",
    "            if any(keyword in text_lower for keyword in keywords):\n",
    "                return priority\n",
    "        \n",
    "        return \"normal\"\n",
    "    \n",
    "    def process_review(self, review: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a single review.\"\"\"\n",
    "        # Sentiment analysis\n",
    "        sentiment = self.sentiment_pipeline(review)[0]\n",
    "        \n",
    "        # Priority detection\n",
    "        priority = self.analyze_priority(review)\n",
    "        \n",
    "        return {\n",
    "            \"text\": review,\n",
    "            \"sentiment\": sentiment[\"label\"],\n",
    "            \"sentiment_score\": sentiment[\"score\"],\n",
    "            \"priority\": priority,\n",
    "            \"needs_attention\": priority in [\"urgent\", \"high\"]\n",
    "        }\n",
    "    \n",
    "    def process_batch(self, reviews: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Process multiple reviews and generate insights.\"\"\"\n",
    "        results = [self.process_review(review) for review in reviews]\n",
    "        \n",
    "        # Generate insights\n",
    "        total = len(results)\n",
    "        urgent_count = sum(1 for r in results if r[\"priority\"] in [\"urgent\", \"high\"])\n",
    "        positive_count = sum(1 for r in results if r[\"sentiment\"] == \"POSITIVE\")\n",
    "        \n",
    "        insights = {\n",
    "            \"total_reviews\": total,\n",
    "            \"urgent_reviews\": urgent_count,\n",
    "            \"sentiment_distribution\": {\n",
    "                \"positive\": positive_count,\n",
    "                \"negative\": total - positive_count,\n",
    "                \"positive_rate\": positive_count / total if total > 0 else 0\n",
    "            },\n",
    "            \"results\": results\n",
    "        }\n",
    "        \n",
    "        return insights\n",
    "\n",
    "# Test the workflow\n",
    "workflow = SimpleRetailWorkflow()\n",
    "\n",
    "sample_reviews = [\n",
    "    \"This product is absolutely amazing! Fast shipping and great quality.\",\n",
    "    \"Terrible experience. The item arrived broken and customer service was unhelpful.\",\n",
    "    \"Good value for money, but packaging could be better.\",\n",
    "    \"URGENT: Received wrong item. Need immediate refund!\",\n",
    "    \"The product works as described. Delivery was on time.\"\n",
    "]\n",
    "\n",
    "# Process reviews\n",
    "print(\"Processing reviews...\\n\")\n",
    "start_time = time.time()\n",
    "insights = workflow.process_batch(sample_reviews)\n",
    "process_time = time.time() - start_time\n",
    "\n",
    "# Display results\n",
    "print(\"=== WORKFLOW RESULTS ===\")\n",
    "print(f\"Total reviews processed: {insights['total_reviews']}\")\n",
    "print(f\"Processing time: {process_time:.3f} seconds\")\n",
    "print(f\"\\nUrgent reviews requiring attention: {insights['urgent_reviews']}\")\n",
    "print(f\"Positive sentiment rate: {insights['sentiment_distribution']['positive_rate']:.1%}\")\n",
    "\n",
    "print(\"\\n=== DETAILED RESULTS ===\")\n",
    "for i, result in enumerate(insights['results']):\n",
    "    if result['needs_attention']:\n",
    "        print(f\"\\nâš ï¸  Review {i+1} (NEEDS ATTENTION):\")\n",
    "    else:\n",
    "        print(f\"\\nReview {i+1}:\")\n",
    "    print(f\"  Text: '{result['text'][:50]}...'\")\n",
    "    print(f\"  Sentiment: {result['sentiment']} ({result['sentiment_score']:.3f})\")\n",
    "    print(f\"  Priority: {result['priority'].upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Management\n",
    "\n",
    "Implement a configuration system with environment variable support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Centralized configuration with environment fallbacks.\"\"\"\n",
    "    \n",
    "    # Device configuration with automatic detection\n",
    "    DEVICE = get_optimal_device()\n",
    "    \n",
    "    # Model configurations with env overrides\n",
    "    DEFAULT_SENTIMENT_MODEL = os.getenv(\n",
    "        \"SENTIMENT_MODEL\", \n",
    "        \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    )\n",
    "    \n",
    "    # Performance settings\n",
    "    BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"32\"))\n",
    "    MAX_LENGTH = int(os.getenv(\"MAX_LENGTH\", \"512\"))\n",
    "    ENABLE_FLASH_ATTENTION = os.getenv(\"ENABLE_FLASH_ATTENTION\", \"true\").lower() == \"true\"\n",
    "    \n",
    "    # Directory management with auto-creation\n",
    "    DATA_PATH = Path(os.getenv(\"DATA_PATH\", \"./data\"))\n",
    "    CACHE_DIR = Path(os.getenv(\"CACHE_DIR\", \"./cache\"))\n",
    "    \n",
    "    @classmethod\n",
    "    def display(cls):\n",
    "        \"\"\"Display current configuration.\"\"\"\n",
    "        print(\"Current Configuration:\")\n",
    "        print(f\"  Device: {cls.DEVICE}\")\n",
    "        print(f\"  Default Model: {cls.DEFAULT_SENTIMENT_MODEL}\")\n",
    "        print(f\"  Batch Size: {cls.BATCH_SIZE}\")\n",
    "        print(f\"  Max Length: {cls.MAX_LENGTH}\")\n",
    "        print(f\"  Flash Attention: {cls.ENABLE_FLASH_ATTENTION}\")\n",
    "        print(f\"  Data Path: {cls.DATA_PATH}\")\n",
    "        print(f\"  Cache Dir: {cls.CACHE_DIR}\")\n",
    "\n",
    "# Display configuration\n",
    "Config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Benchmarking\n",
    "\n",
    "Create a simple benchmarking utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_pipeline(pipeline_func, inputs: List[str], name: str = \"Pipeline\") -> Dict[str, float]:\n",
    "    \"\"\"Benchmark a pipeline with various metrics.\"\"\"\n",
    "    print(f\"\\nBenchmarking {name}...\")\n",
    "    \n",
    "    # Warmup\n",
    "    _ = pipeline_func(inputs[0])\n",
    "    \n",
    "    # Single inference\n",
    "    start = time.time()\n",
    "    for inp in inputs[:10]:\n",
    "        _ = pipeline_func(inp)\n",
    "    single_time = time.time() - start\n",
    "    \n",
    "    # Batch inference\n",
    "    start = time.time()\n",
    "    _ = pipeline_func(inputs)\n",
    "    batch_time = time.time() - start\n",
    "    \n",
    "    metrics = {\n",
    "        \"single_latency_ms\": (single_time / 10) * 1000,\n",
    "        \"batch_latency_ms\": (batch_time / len(inputs)) * 1000,\n",
    "        \"throughput_single\": 10 / single_time,\n",
    "        \"throughput_batch\": len(inputs) / batch_time,\n",
    "        \"speedup\": (single_time / 10 * len(inputs)) / batch_time\n",
    "    }\n",
    "    \n",
    "    print(f\"  Single inference: {metrics['single_latency_ms']:.1f} ms/sample\")\n",
    "    print(f\"  Batch inference: {metrics['batch_latency_ms']:.1f} ms/sample\")\n",
    "    print(f\"  Throughput (single): {metrics['throughput_single']:.1f} samples/sec\")\n",
    "    print(f\"  Throughput (batch): {metrics['throughput_batch']:.1f} samples/sec\")\n",
    "    print(f\"  Batch speedup: {metrics['speedup']:.1f}x\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Benchmark our sentiment pipeline\n",
    "test_inputs = [f\"Test review number {i}. This is a sample text.\" for i in range(50)]\n",
    "metrics = benchmark_pipeline(clf, test_inputs, \"Sentiment Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "In this tutorial notebook, we've covered:\n",
    "\n",
    "1. **Pipeline Customization**: From basic usage to custom preprocessing and component composition\n",
    "2. **Efficient Data Handling**: Using ðŸ¤— Datasets for scalable data processing\n",
    "3. **Optimization Techniques**: Batching, quantization, and memory tracking\n",
    "4. **Synthetic Data Generation**: Creating and validating synthetic training data\n",
    "5. **Production Workflows**: Building robust, scalable systems for real-world deployment\n",
    "\n",
    "### Key Performance Gains:\n",
    "- **Batching**: 5-10x throughput improvement\n",
    "- **Quantization**: 75% model size reduction\n",
    "- **Streaming**: Handle datasets larger than memory\n",
    "- **PEFT/LoRA**: Train with 0.1% of parameters\n",
    "\n",
    "### Next Steps:\n",
    "1. Experiment with different models and batch sizes\n",
    "2. Implement quantization on compatible hardware\n",
    "3. Build custom pipelines for your specific use case\n",
    "4. Explore synthetic data generation for your domain\n",
    "\n",
    "**Remember**: Great AI isn't about using the fanciest models. It's about building robust, efficient workflows that solve real problems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
