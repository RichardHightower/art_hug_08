{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing Pipelines and Data Workflows: Advanced Models and Efficient Processing\n",
    "\n",
    "This notebook contains all examples from Chapter 8 with step-by-step explanations.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Environment Setup](#environment-setup)\n",
    "2. [Pipeline Basics and Customization](#pipeline-basics)\n",
    "3. [Efficient Data Handling](#data-handling)\n",
    "4. [Optimization Techniques](#optimization)\n",
    "5. [Synthetic Data Generation](#synthetic-data)\n",
    "6. [Production Workflows](#production)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup <a id='environment-setup'></a>\n",
    "\n",
    "First, let's set up our environment with all necessary imports and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Add src to path for local imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    Pipeline,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "from transformers.pipelines import register_pipeline\n",
    "from transformers.utils import logging\n",
    "\n",
    "# Datasets\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# For optimization examples\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# For synthetic data generation\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "print(\"Environment ready!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Configuration\n",
    "\n",
    "Let's implement the cross-platform device detection from the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_device() -> torch.device:\n",
    "    \"\"\"Automatically detect best available device.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():  # Apple Silicon\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_optimal_device()\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Basics and Customization <a id='pipeline-basics'></a>\n",
    "\n",
    "### Quick Start: Modern Pipeline Usage\n",
    "\n",
    "Let's start with the basic pipeline example from the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modern quick-start with explicit model and device\n",
    "clf = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    device=0 if DEVICE.type == \"cuda\" else -1  # 0 for CUDA GPU, -1 for CPU\n",
    ")\n",
    "\n",
    "# Run prediction on text\n",
    "result = clf('I love Hugging Face!')\n",
    "print(result)\n",
    "# Expected output: [{'label': 'POSITIVE', 'score': 0.9998}]\n",
    "\n",
    "# Let's try multiple examples\n",
    "texts = [\n",
    "    \"I love this product!\",\n",
    "    \"This is terrible.\",\n",
    "    \"Not bad, but could be better.\"\n",
    "]\n",
    "results = clf(texts)\n",
    "for text, result in zip(texts, results):\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"  Sentiment: {result['label']} (confidence: {result['score']:.3f})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Preprocessing\n",
    "\n",
    "Now let's add custom preprocessing to normalize text before inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_preprocess(text):\n",
    "    \"\"\"Normalize text for consistent predictions.\"\"\"\n",
    "    import string\n",
    "    text = text.lower()\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# Test preprocessing\n",
    "texts = [\"Wow! Amazing product!!!\", \"I don't like this...\"]\n",
    "print(\"Original texts:\", texts)\n",
    "\n",
    "# Clean then predict\n",
    "cleaned = [custom_preprocess(t) for t in texts]\n",
    "print(\"Cleaned texts:\", cleaned)\n",
    "\n",
    "# Batch processing for speed\n",
    "results = clf(cleaned, batch_size=16)\n",
    "print(\"\\nResults after preprocessing:\")\n",
    "for original, clean, result in zip(texts, cleaned, results):\n",
    "    print(f\"Original: '{original}'\")\n",
    "    print(f\"Cleaned: '{clean}'\")\n",
    "    print(f\"Result: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced: Pipeline Subclassing\n",
    "\n",
    "Create a reusable pipeline with built-in preprocessing and postprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSentimentPipeline(Pipeline):\n",
    "    def preprocess(self, inputs):\n",
    "        \"\"\"Strip HTML, normalize text.\"\"\"\n",
    "        if isinstance(inputs, str):\n",
    "            text = inputs.lower()\n",
    "            import string\n",
    "            text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "            return super().preprocess(text)\n",
    "        return super().preprocess(inputs)\n",
    "    \n",
    "    def postprocess(self, outputs):\n",
    "        \"\"\"Add confidence thresholds.\"\"\"\n",
    "        results = super().postprocess(outputs)\n",
    "        for r in results:\n",
    "            r['confident'] = r['score'] > 0.95\n",
    "        return results\n",
    "\n",
    "# Note: In practice, you would register and use this custom pipeline\n",
    "# For now, let's demonstrate the concept with the standard pipeline\n",
    "print(\"Custom pipeline concept demonstrated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting Pipeline Components\n",
    "\n",
    "Let's peek under the hood to understand pipeline anatomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect pipeline components\n",
    "print('Model:', type(clf.model).__name__)\n",
    "print('Tokenizer:', type(clf.tokenizer).__name__)  \n",
    "print('Processor:', getattr(clf, 'processor', None))\n",
    "print('Framework:', clf.framework)\n",
    "print('Device:', clf.device)\n",
    "\n",
    "# Let's look at model details\n",
    "print(f\"\\nModel architecture: {clf.model.config.model_type}\")\n",
    "print(f\"Hidden size: {clf.model.config.hidden_size}\")\n",
    "print(f\"Number of labels: {clf.model.config.num_labels}\")\n",
    "\n",
    "# Tokenizer details\n",
    "print(f\"\\nTokenizer vocab size: {clf.tokenizer.vocab_size}\")\n",
    "print(f\"Max length: {clf.tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composing Multiple Pipelines\n",
    "\n",
    "Let's create a combined sentiment + NER pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load individual pipelines\n",
    "sentiment_pipe = pipeline('sentiment-analysis')\n",
    "ner_pipe = pipeline('ner')\n",
    "\n",
    "def combined_analysis(text):\n",
    "    \"\"\"Combine sentiment and NER analysis.\"\"\"\n",
    "    sentiment = sentiment_pipe(text)\n",
    "    entities = ner_pipe(text)\n",
    "    \n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"sentiment\": sentiment[0],\n",
    "        \"entities\": entities\n",
    "    }\n",
    "\n",
    "# Test combined analysis\n",
    "test_text = \"Apple Inc. makes amazing products! I love my iPhone.\"\n",
    "result = combined_analysis(test_text)\n",
    "\n",
    "print(f\"Text: {result['text']}\")\n",
    "print(f\"\\nSentiment: {result['sentiment']['label']} ({result['sentiment']['score']:.3f})\")\n",
    "print(\"\\nEntities found:\")\n",
    "for entity in result['entities']:\n",
    "    print(f\"  - {entity['word']}: {entity['entity_group'] if 'entity_group' in entity else entity['entity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging Pipelines\n",
    "\n",
    "Enable verbose logging to debug pipeline issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable debug logging\n",
    "logging.set_verbosity_debug()\n",
    "\n",
    "# Now pipeline operations will show detailed logs\n",
    "print(\"Debug mode enabled. Running pipeline...\")\n",
    "result = clf(\"Debug me!\")\n",
    "print(f\"\\nResult: {result}\")\n",
    "\n",
    "# Reset logging to normal\n",
    "logging.set_verbosity_warning()\n",
    "print(\"\\nLogging reset to normal level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficient Data Handling with 🤗 Datasets <a id='data-handling'></a>\n",
    "\n",
    "### Loading and Transforming Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small dataset for demonstration\n",
    "dataset = load_dataset('imdb', split='train[:1000]')  # Load only first 1000 examples\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"First example: {dataset[0]}\")\n",
    "print(f\"\\nFeatures: {dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "def preprocess_batch(batch):\n",
    "    \"\"\"Process entire batches at once.\"\"\"\n",
    "    batch['text'] = [text.lower() for text in batch['text']]\n",
    "    batch['length'] = [len(text.split()) for text in batch['text']]\n",
    "    return batch\n",
    "\n",
    "# Transform with parallel processing\n",
    "print(\"Transforming dataset...\")\n",
    "start_time = time.time()\n",
    "dataset = dataset.map(preprocess_batch, batched=True, num_proc=4)\n",
    "print(f\"Transformation completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Filter short reviews\n",
    "print(f\"\\nDataset before filtering: {len(dataset)} examples\")\n",
    "dataset = dataset.filter(lambda x: x['length'] > 20)\n",
    "print(f\"Dataset after filtering: {len(dataset)} examples\")\n",
    "\n",
    "# Check the new features\n",
    "print(f\"\\nUpdated features: {dataset.features}\")\n",
    "print(f\"Example with new features: {dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Large Datasets\n",
    "\n",
    "For massive datasets, use streaming to avoid memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample CSV for demonstration\n",
    "import csv\n",
    "\n",
    "sample_data = [\n",
    "    {\"text\": \"This product is amazing!\", \"label\": \"positive\"},\n",
    "    {\"text\": \"Terrible experience.\", \"label\": \"negative\"},\n",
    "    {\"text\": \"Good value for money.\", \"label\": \"positive\"},\n",
    "    {\"text\": \"Not worth the price.\", \"label\": \"negative\"},\n",
    "    {\"text\": \"Excellent quality!\", \"label\": \"positive\"}\n",
    "] * 20  # Repeat for larger dataset\n",
    "\n",
    "csv_path = \"sample_reviews.csv\"\n",
    "with open(csv_path, 'w', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=['text', 'label'])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(sample_data)\n",
    "\n",
    "# Stream the dataset\n",
    "streaming_dataset = load_dataset('csv', data_files=csv_path, split='train', streaming=True)\n",
    "\n",
    "# Process in batches\n",
    "batch_size = 32\n",
    "batch = []\n",
    "processed_count = 0\n",
    "\n",
    "print(\"Processing streaming dataset...\")\n",
    "for example in streaming_dataset:\n",
    "    batch.append(custom_preprocess(example['text']))\n",
    "    \n",
    "    if len(batch) == batch_size:\n",
    "        # Process batch\n",
    "        results = clf(batch, batch_size=batch_size)\n",
    "        processed_count += len(batch)\n",
    "        print(f\"Processed {processed_count} examples...\")\n",
    "        batch = []\n",
    "    \n",
    "    # Stop after processing 100 examples for demo\n",
    "    if processed_count >= 96:\n",
    "        break\n",
    "\n",
    "# Process remaining batch\n",
    "if batch:\n",
    "    results = clf(batch)\n",
    "    processed_count += len(batch)\n",
    "\n",
    "print(f\"\\nTotal processed: {processed_count} examples\")\n",
    "\n",
    "# Clean up\n",
    "os.remove(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Custom Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom dataset from dictionaries\n",
    "custom_data = {\n",
    "    \"text\": [\n",
    "        \"The future of AI is bright and full of possibilities.\",\n",
    "        \"Machine learning transforms how we solve complex problems.\",\n",
    "        \"Deep learning models continue to improve rapidly.\",\n",
    "        \"Natural language processing enables better human-computer interaction.\",\n",
    "        \"Computer vision applications are becoming more sophisticated.\"\n",
    "    ],\n",
    "    \"category\": [\"future\", \"ml\", \"dl\", \"nlp\", \"cv\"]\n",
    "}\n",
    "\n",
    "custom_dataset = Dataset.from_dict(custom_data)\n",
    "print(f\"Custom dataset: {custom_dataset}\")\n",
    "print(f\"\\nFirst example: {custom_dataset[0]}\")\n",
    "\n",
    "# Apply transformations\n",
    "def add_metadata(example):\n",
    "    example['word_count'] = len(example['text'].split())\n",
    "    example['char_count'] = len(example['text'])\n",
    "    return example\n",
    "\n",
    "custom_dataset = custom_dataset.map(add_metadata)\n",
    "print(f\"\\nAfter transformation: {custom_dataset[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Techniques <a id='optimization'></a>\n",
    "\n",
    "### Batching for 10x Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "test_texts = [\n",
    "    \"Review 1: This product exceeded my expectations.\",\n",
    "    \"Review 2: Not satisfied with the quality.\",\n",
    "    \"Review 3: Average product, nothing special.\",\n",
    "    \"Review 4: Absolutely love it!\",\n",
    "    \"Review 5: Waste of money.\",\n",
    "    \"Review 6: Good value for the price.\",\n",
    "    \"Review 7: Would recommend to friends.\",\n",
    "    \"Review 8: Poor customer service.\"\n",
    "] * 4  # Repeat for more examples\n",
    "\n",
    "# Method 1: One by one (slow)\n",
    "print(\"Method 1: Processing one by one...\")\n",
    "start_time = time.time()\n",
    "results_single = []\n",
    "for text in test_texts[:8]:  # Process only first 8 for demo\n",
    "    result = clf(text)\n",
    "    results_single.append(result)\n",
    "single_time = time.time() - start_time\n",
    "print(f\"Time taken: {single_time:.3f} seconds\")\n",
    "print(f\"Average per text: {single_time/8*1000:.1f} ms\")\n",
    "\n",
    "# Method 2: Batch processing (fast)\n",
    "print(\"\\nMethod 2: Batch processing...\")\n",
    "start_time = time.time()\n",
    "results_batch = clf(test_texts, \n",
    "                   padding=True,\n",
    "                   truncation=True,\n",
    "                   max_length=128)\n",
    "batch_time = time.time() - start_time\n",
    "print(f\"Time taken: {batch_time:.3f} seconds\")\n",
    "print(f\"Average per text: {batch_time/len(test_texts)*1000:.1f} ms\")\n",
    "\n",
    "# Calculate speedup\n",
    "speedup = (single_time/8*len(test_texts)) / batch_time\n",
    "print(f\"\\nSpeedup: {speedup:.1f}x faster with batching!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modern Quantization\n",
    "\n",
    "Demonstrate quantization for cost reduction (requires appropriate hardware)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small model for demonstration\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "print(\"Loading standard model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Calculate model size\n",
    "param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "model_size = param_size + buffer_size\n",
    "print(f\"Standard model size: {model_size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Note: Actual quantization requires bitsandbytes library and compatible GPU\n",
    "# This is a conceptual demonstration\n",
    "print(\"\\nQuantization options:\")\n",
    "print(\"- INT8: ~75% size reduction, minimal accuracy loss\")\n",
    "print(\"- INT4: ~87.5% size reduction, may require fine-tuning\")\n",
    "print(\"- Dynamic quantization: Adapts to input ranges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Tracking Utility\n",
    "\n",
    "Implement the memory tracking context manager from the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def track_memory(device: str = \"cuda\"):\n",
    "    \"\"\"Context manager for GPU memory profiling.\"\"\"\n",
    "    if device == \"cuda\" and torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "        start_memory = torch.cuda.memory_allocated()\n",
    "        yield\n",
    "        torch.cuda.synchronize()\n",
    "        end_memory = torch.cuda.memory_allocated()\n",
    "        memory_used = end_memory - start_memory\n",
    "        print(f\"Memory used: {memory_used / 1024 / 1024:.2f} MB\")\n",
    "    else:\n",
    "        yield\n",
    "        print(\"Memory tracking only available for CUDA devices\")\n",
    "\n",
    "# Example usage\n",
    "print(\"Testing memory tracking...\")\n",
    "with track_memory(device=DEVICE.type):\n",
    "    # Run some inference\n",
    "    _ = clf(\"Test text for memory tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEFT/LoRA Concept\n",
    "\n",
    "Demonstrate Parameter-Efficient Fine-Tuning concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT configuration example (conceptual)\n",
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "# Define LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Sequence classification\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"]  # Target attention layers\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(f\"  Rank (r): {peft_config.r}\")\n",
    "print(f\"  Alpha: {peft_config.lora_alpha}\")\n",
    "print(f\"  Dropout: {peft_config.lora_dropout}\")\n",
    "print(f\"  Target modules: {peft_config.target_modules}\")\n",
    "\n",
    "# Calculate approximate trainable parameters\n",
    "# For BERT-base with r=16:\n",
    "# Original: ~110M parameters\n",
    "# LoRA trainable: ~0.3M parameters (0.3% of original)\n",
    "print(\"\\nParameter efficiency:\")\n",
    "print(\"  Original BERT-base: ~110M parameters\")\n",
    "print(\"  LoRA trainable: ~0.3M parameters\")\n",
    "print(\"  Reduction: 99.7% fewer trainable parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data Generation <a id='synthetic-data'></a>\n",
    "\n",
    "### Text Generation with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small text generation model\n",
    "print(\"Loading text generation model...\")\n",
    "gen = pipeline(\n",
    "    'text-generation',\n",
    "    model='gpt2',  # Using smaller model for demo\n",
    "    device=0 if DEVICE.type == \"cuda\" else -1\n",
    ")\n",
    "\n",
    "# Generate product reviews\n",
    "prompts = [\n",
    "    \"This smartphone is\",\n",
    "    \"The laptop performance is\",\n",
    "    \"Customer service was\"\n",
    "]\n",
    "\n",
    "print(\"Generating synthetic reviews...\\n\")\n",
    "for prompt in prompts:\n",
    "    generated = gen(\n",
    "        prompt,\n",
    "        max_new_tokens=30,\n",
    "        num_return_sequences=2,\n",
    "        temperature=0.8,\n",
    "        pad_token_id=gen.tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    for i, g in enumerate(generated):\n",
    "        print(f\"  Generated {i+1}: {g['generated_text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic Data Validation\n",
    "\n",
    "Implement quality checks for synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_synthetic_text(texts: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"Basic validation for synthetic text data.\"\"\"\n",
    "    results = {\n",
    "        \"total\": len(texts),\n",
    "        \"valid\": 0,\n",
    "        \"issues\": []\n",
    "    }\n",
    "    \n",
    "    for text in texts:\n",
    "        issues = []\n",
    "        \n",
    "        # Check length\n",
    "        if len(text.split()) < 5:\n",
    "            issues.append(\"too_short\")\n",
    "        elif len(text.split()) > 200:\n",
    "            issues.append(\"too_long\")\n",
    "        \n",
    "        # Check for repetition\n",
    "        words = text.lower().split()\n",
    "        if len(words) > 0 and len(set(words)) / len(words) < 0.5:\n",
    "            issues.append(\"repetitive\")\n",
    "        \n",
    "        # Check for truncation\n",
    "        if text.endswith(\"...\") or not text.endswith(('.', '!', '?')):\n",
    "            issues.append(\"truncated\")\n",
    "        \n",
    "        if not issues:\n",
    "            results[\"valid\"] += 1\n",
    "        else:\n",
    "            results[\"issues\"].extend(issues)\n",
    "    \n",
    "    results[\"validity_rate\"] = results[\"valid\"] / results[\"total\"]\n",
    "    return results\n",
    "\n",
    "# Test validation\n",
    "synthetic_samples = [\n",
    "    \"This product is amazing and works perfectly!\",\n",
    "    \"Good good good good good.\",\n",
    "    \"The laptop\",\n",
    "    \"Excellent quality and fast shipping. Would buy again.\",\n",
    "    \"This is a test that ends abruptly and\"\n",
    "]\n",
    "\n",
    "validation_results = validate_synthetic_text(synthetic_samples)\n",
    "print(\"Validation Results:\")\n",
    "print(f\"  Total samples: {validation_results['total']}\")\n",
    "print(f\"  Valid samples: {validation_results['valid']}\")\n",
    "print(f\"  Validity rate: {validation_results['validity_rate']:.1%}\")\n",
    "print(f\"  Issues found: {set(validation_results['issues'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Workflows <a id='production'></a>\n",
    "\n",
    "### Complete Production Pipeline Example\n",
    "\n",
    "Let's implement a simplified version of the RetailReviewWorkflow from the chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRetailWorkflow:\n",
    "    \"\"\"Simplified production workflow for retail review analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Initialize pipelines\n",
    "        self.sentiment_pipeline = pipeline(\n",
    "            'sentiment-analysis',\n",
    "            model='distilbert-base-uncased-finetuned-sst-2-english'\n",
    "        )\n",
    "        \n",
    "        # Priority keywords for urgency detection\n",
    "        self.priority_keywords = {\n",
    "            \"urgent\": [\"broken\", \"damaged\", \"fraud\", \"stolen\", \"urgent\"],\n",
    "            \"high\": [\"terrible\", \"awful\", \"worst\", \"refund\", \"complaint\"],\n",
    "            \"medium\": [\"disappointed\", \"issue\", \"problem\", \"concern\"],\n",
    "            \"low\": [\"suggestion\", \"feedback\", \"minor\"]\n",
    "        }\n",
    "    \n",
    "    def analyze_priority(self, text: str) -> str:\n",
    "        \"\"\"Determine review priority based on keywords.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for priority, keywords in self.priority_keywords.items():\n",
    "            if any(keyword in text_lower for keyword in keywords):\n",
    "                return priority\n",
    "        \n",
    "        return \"normal\"\n",
    "    \n",
    "    def process_review(self, review: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a single review.\"\"\"\n",
    "        # Sentiment analysis\n",
    "        sentiment = self.sentiment_pipeline(review)[0]\n",
    "        \n",
    "        # Priority detection\n",
    "        priority = self.analyze_priority(review)\n",
    "        \n",
    "        return {\n",
    "            \"text\": review,\n",
    "            \"sentiment\": sentiment[\"label\"],\n",
    "            \"sentiment_score\": sentiment[\"score\"],\n",
    "            \"priority\": priority,\n",
    "            \"needs_attention\": priority in [\"urgent\", \"high\"]\n",
    "        }\n",
    "    \n",
    "    def process_batch(self, reviews: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Process multiple reviews and generate insights.\"\"\"\n",
    "        results = [self.process_review(review) for review in reviews]\n",
    "        \n",
    "        # Generate insights\n",
    "        total = len(results)\n",
    "        urgent_count = sum(1 for r in results if r[\"priority\"] in [\"urgent\", \"high\"])\n",
    "        positive_count = sum(1 for r in results if r[\"sentiment\"] == \"POSITIVE\")\n",
    "        \n",
    "        insights = {\n",
    "            \"total_reviews\": total,\n",
    "            \"urgent_reviews\": urgent_count,\n",
    "            \"sentiment_distribution\": {\n",
    "                \"positive\": positive_count,\n",
    "                \"negative\": total - positive_count,\n",
    "                \"positive_rate\": positive_count / total if total > 0 else 0\n",
    "            },\n",
    "            \"results\": results\n",
    "        }\n",
    "        \n",
    "        return insights\n",
    "\n",
    "# Test the workflow\n",
    "workflow = SimpleRetailWorkflow()\n",
    "\n",
    "sample_reviews = [\n",
    "    \"This product is absolutely amazing! Fast shipping and great quality.\",\n",
    "    \"Terrible experience. The item arrived broken and customer service was unhelpful.\",\n",
    "    \"Good value for money, but packaging could be better.\",\n",
    "    \"URGENT: Received wrong item. Need immediate refund!\",\n",
    "    \"The product works as described. Delivery was on time.\"\n",
    "]\n",
    "\n",
    "# Process reviews\n",
    "print(\"Processing reviews...\\n\")\n",
    "start_time = time.time()\n",
    "insights = workflow.process_batch(sample_reviews)\n",
    "process_time = time.time() - start_time\n",
    "\n",
    "# Display results\n",
    "print(\"=== WORKFLOW RESULTS ===\")\n",
    "print(f\"Total reviews processed: {insights['total_reviews']}\")\n",
    "print(f\"Processing time: {process_time:.3f} seconds\")\n",
    "print(f\"\\nUrgent reviews requiring attention: {insights['urgent_reviews']}\")\n",
    "print(f\"Positive sentiment rate: {insights['sentiment_distribution']['positive_rate']:.1%}\")\n",
    "\n",
    "print(\"\\n=== DETAILED RESULTS ===\")\n",
    "for i, result in enumerate(insights['results']):\n",
    "    if result['needs_attention']:\n",
    "        print(f\"\\n⚠️  Review {i+1} (NEEDS ATTENTION):\")\n",
    "    else:\n",
    "        print(f\"\\nReview {i+1}:\")\n",
    "    print(f\"  Text: '{result['text'][:50]}...'\")\n",
    "    print(f\"  Sentiment: {result['sentiment']} ({result['sentiment_score']:.3f})\")\n",
    "    print(f\"  Priority: {result['priority'].upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Management\n",
    "\n",
    "Implement a configuration system with environment variable support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"Centralized configuration with environment fallbacks.\"\"\"\n",
    "    \n",
    "    # Device configuration with automatic detection\n",
    "    DEVICE = get_optimal_device()\n",
    "    \n",
    "    # Model configurations with env overrides\n",
    "    DEFAULT_SENTIMENT_MODEL = os.getenv(\n",
    "        \"SENTIMENT_MODEL\", \n",
    "        \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "    )\n",
    "    \n",
    "    # Performance settings\n",
    "    BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", \"32\"))\n",
    "    MAX_LENGTH = int(os.getenv(\"MAX_LENGTH\", \"512\"))\n",
    "    ENABLE_FLASH_ATTENTION = os.getenv(\"ENABLE_FLASH_ATTENTION\", \"true\").lower() == \"true\"\n",
    "    \n",
    "    # Directory management with auto-creation\n",
    "    DATA_PATH = Path(os.getenv(\"DATA_PATH\", \"./data\"))\n",
    "    CACHE_DIR = Path(os.getenv(\"CACHE_DIR\", \"./cache\"))\n",
    "    \n",
    "    @classmethod\n",
    "    def display(cls):\n",
    "        \"\"\"Display current configuration.\"\"\"\n",
    "        print(\"Current Configuration:\")\n",
    "        print(f\"  Device: {cls.DEVICE}\")\n",
    "        print(f\"  Default Model: {cls.DEFAULT_SENTIMENT_MODEL}\")\n",
    "        print(f\"  Batch Size: {cls.BATCH_SIZE}\")\n",
    "        print(f\"  Max Length: {cls.MAX_LENGTH}\")\n",
    "        print(f\"  Flash Attention: {cls.ENABLE_FLASH_ATTENTION}\")\n",
    "        print(f\"  Data Path: {cls.DATA_PATH}\")\n",
    "        print(f\"  Cache Dir: {cls.CACHE_DIR}\")\n",
    "\n",
    "# Display configuration\n",
    "Config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Benchmarking\n",
    "\n",
    "Create a simple benchmarking utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_pipeline(pipeline_func, inputs: List[str], name: str = \"Pipeline\") -> Dict[str, float]:\n",
    "    \"\"\"Benchmark a pipeline with various metrics.\"\"\"\n",
    "    print(f\"\\nBenchmarking {name}...\")\n",
    "    \n",
    "    # Warmup\n",
    "    _ = pipeline_func(inputs[0])\n",
    "    \n",
    "    # Single inference\n",
    "    start = time.time()\n",
    "    for inp in inputs[:10]:\n",
    "        _ = pipeline_func(inp)\n",
    "    single_time = time.time() - start\n",
    "    \n",
    "    # Batch inference\n",
    "    start = time.time()\n",
    "    _ = pipeline_func(inputs)\n",
    "    batch_time = time.time() - start\n",
    "    \n",
    "    metrics = {\n",
    "        \"single_latency_ms\": (single_time / 10) * 1000,\n",
    "        \"batch_latency_ms\": (batch_time / len(inputs)) * 1000,\n",
    "        \"throughput_single\": 10 / single_time,\n",
    "        \"throughput_batch\": len(inputs) / batch_time,\n",
    "        \"speedup\": (single_time / 10 * len(inputs)) / batch_time\n",
    "    }\n",
    "    \n",
    "    print(f\"  Single inference: {metrics['single_latency_ms']:.1f} ms/sample\")\n",
    "    print(f\"  Batch inference: {metrics['batch_latency_ms']:.1f} ms/sample\")\n",
    "    print(f\"  Throughput (single): {metrics['throughput_single']:.1f} samples/sec\")\n",
    "    print(f\"  Throughput (batch): {metrics['throughput_batch']:.1f} samples/sec\")\n",
    "    print(f\"  Batch speedup: {metrics['speedup']:.1f}x\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Benchmark our sentiment pipeline\n",
    "test_inputs = [f\"Test review number {i}. This is a sample text.\" for i in range(50)]\n",
    "metrics = benchmark_pipeline(clf, test_inputs, \"Sentiment Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "In this tutorial notebook, we've covered:\n",
    "\n",
    "1. **Pipeline Customization**: From basic usage to custom preprocessing and component composition\n",
    "2. **Efficient Data Handling**: Using 🤗 Datasets for scalable data processing\n",
    "3. **Optimization Techniques**: Batching, quantization, and memory tracking\n",
    "4. **Synthetic Data Generation**: Creating and validating synthetic training data\n",
    "5. **Production Workflows**: Building robust, scalable systems for real-world deployment\n",
    "\n",
    "### Key Performance Gains:\n",
    "- **Batching**: 5-10x throughput improvement\n",
    "- **Quantization**: 75% model size reduction\n",
    "- **Streaming**: Handle datasets larger than memory\n",
    "- **PEFT/LoRA**: Train with 0.1% of parameters\n",
    "\n",
    "### Next Steps:\n",
    "1. Experiment with different models and batch sizes\n",
    "2. Implement quantization on compatible hardware\n",
    "3. Build custom pipelines for your specific use case\n",
    "4. Explore synthetic data generation for your domain\n",
    "\n",
    "**Remember**: Great AI isn't about using the fanciest models. It's about building robust, efficient workflows that solve real problems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}